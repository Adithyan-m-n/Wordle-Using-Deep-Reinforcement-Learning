{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eCFPWul_bH8",
        "outputId": "d3931a3c-aa3b-4a93-fdda-c923398cb1a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: gym 0.25.2\n",
            "Uninstalling gym-0.25.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/gym-0.25.2.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/gym/*\n",
            "Proceed (Y/n)? n\n",
            "\u001b[33mWARNING: Skipping stable-baselines3 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.1.0-py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium<0.30,>=0.28.1 (from stable-baselines3[extra])\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.1.0+cu118)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.8.0.76)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.2)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.14.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.6.0)\n",
            "Collecting shimmy[atari]~=1.1.0 (from stable-baselines3[extra])\n",
            "  Downloading Shimmy-1.1.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (9.4.0)\n",
            "Collecting autorom[accept-rom-license]~=0.6.1 (from stable-baselines3[extra])\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.31.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra])\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.1.0->stable-baselines3[extra])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.59.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.5)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.12.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2023.3.post1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.1.0->stable-baselines3[extra]) (6.1.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=39fccca84e733e30ae7288c2cd7e2c4e2f0d9f700228ffca3c41e94af440f9cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, gymnasium, ale-py, shimmy, AutoROM.accept-rom-license, autorom, stable-baselines3\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 farama-notifications-0.0.4 gymnasium-0.29.1 shimmy-1.1.0 stable-baselines3-2.1.0\n",
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.10/dist-packages (2.1.0)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.1.0+cu118)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.12.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-rl2) (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.59.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (3.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (3.0.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (3.2.2)\n",
            "Installing collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall gym stable-baselines3\n",
        "!pip install gym\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install stable-baselines3\n",
        "!pip install keras-rl2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n97TJUG1_GkT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "\n",
        "# Word list preprocessing\n",
        "words_list_path = r'/content/word_list.txt'\n",
        "f = open(words_list_path, 'r')\n",
        "words_list = list(f.readlines())\n",
        "words_list = [word.strip('1234567890,. \\n .\\' -') for word in words_list]\n",
        "wordle_word_list = [word for word in words_list if len(word) == 5]\n",
        "wordle_word_list = [word for word in wordle_word_list if not any(ele.isupper() for ele in word)]\n",
        "\n",
        "# Game functions\n",
        "\n",
        "def wordle_obs_from_words(board, word, guess_word, turn_num):\n",
        "    obs = np.zeros(5)\n",
        "    for ind, (i, j) in enumerate(zip(word, guess_word)):\n",
        "        if i == j:\n",
        "            obs[ind] = 1\n",
        "        else:\n",
        "            if j in word:\n",
        "                obs[ind] = -1\n",
        "    board[turn_num] = obs\n",
        "    return board\n",
        "\n",
        "def wordle_score_guess(word, guess_word):\n",
        "    if word == guess_word:\n",
        "        return 100  # Correctly guessed the word\n",
        "    score = sum([10 if i == j else (5 if j in word else -10) for i, j in zip(word, guess_word)])\n",
        "    return max(score, 0)\n",
        "\n",
        "# Environment\n",
        "class WordleEnv(gym.Env):\n",
        "\n",
        "    def __init__(self, wordle_word_list):\n",
        "        self.board = np.zeros((6, 5))\n",
        "        self.last_guess = None\n",
        "        self.wordle_word_list = wordle_word_list\n",
        "        self.action_space = spaces.Discrete(len(self.wordle_word_list))\n",
        "        self.observation_space = spaces.Box(low=-1, high=1, shape=self.board.shape)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.word = random.choice(self.wordle_word_list)\n",
        "        self.board *= 0\n",
        "        self.turn = 0\n",
        "        self.last_guess = None\n",
        "        return self.board\n",
        "\n",
        "    def step(self, action):\n",
        "        guess_word = self.wordle_word_list[action]\n",
        "        self.board = wordle_obs_from_words(self.board, self.word, guess_word, self.turn)\n",
        "        reward = wordle_score_guess(self.word, guess_word)\n",
        "        if guess_word == self.last_guess:\n",
        "            reward -= 5  # Penalize repeating the same guess\n",
        "        self.last_guess = guess_word\n",
        "        self.turn += 1\n",
        "        done = self.turn >= 6 or reward == 100\n",
        "        return self.board, reward, done, {}\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print(self.last_guess,self.word)\n",
        "        print(self.board)\n",
        "\n",
        "env = WordleEnv(wordle_word_list)\n",
        "\n",
        "# Deep Q Network\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(env.action_space.n))\n",
        "\n",
        "# Memory\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "\n",
        "policy = BoltzmannQPolicy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDHkQi8zVHI3"
      },
      "outputs": [],
      "source": [
        "dqn = DQNAgent(model=model, nb_actions=env.action_space.n, memory=memory, policy=policy)\n",
        "dqn.compile(Adam(learning_rate=0.001), metrics=['mae'])\n",
        "# Training\n",
        "dqn.fit(env, nb_steps=30000, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzNDUHsqEX3b",
        "outputId": "91a0be07-776b-43ab-aed6-509a116c80fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "    18/30000: episode: 3, duration: 0.022s, episode steps:   6, steps per second: 275, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2685.333 [852.000, 4833.000],  loss: --, mae: --, mean_q: --\n",
            "    24/30000: episode: 4, duration: 0.015s, episode steps:   6, steps per second: 398, episode reward: 20.000, mean reward:  3.333 [-5.000, 25.000], mean action: 3348.333 [249.000, 5608.000],  loss: --, mae: --, mean_q: --\n",
            "    30/30000: episode: 5, duration: 0.017s, episode steps:   6, steps per second: 354, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3180.167 [243.000, 5331.000],  loss: --, mae: --, mean_q: --\n",
            "    36/30000: episode: 6, duration: 0.015s, episode steps:   6, steps per second: 397, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2113.333 [451.000, 4332.000],  loss: --, mae: --, mean_q: --\n",
            "    42/30000: episode: 7, duration: 0.015s, episode steps:   6, steps per second: 389, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3274.667 [1573.000, 5120.000],  loss: --, mae: --, mean_q: --\n",
            "    48/30000: episode: 8, duration: 0.015s, episode steps:   6, steps per second: 396, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 2830.000 [1202.000, 5075.000],  loss: --, mae: --, mean_q: --\n",
            "    54/30000: episode: 9, duration: 0.015s, episode steps:   6, steps per second: 397, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2881.167 [1573.000, 4762.000],  loss: --, mae: --, mean_q: --\n",
            "    60/30000: episode: 10, duration: 0.016s, episode steps:   6, steps per second: 380, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 4658.167 [3768.000, 5742.000],  loss: --, mae: --, mean_q: --\n",
            "    66/30000: episode: 11, duration: 0.015s, episode steps:   6, steps per second: 413, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2478.667 [1141.000, 5742.000],  loss: --, mae: --, mean_q: --\n",
            "    72/30000: episode: 12, duration: 0.018s, episode steps:   6, steps per second: 334, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2948.667 [409.000, 4630.000],  loss: --, mae: --, mean_q: --\n",
            "    78/30000: episode: 13, duration: 0.026s, episode steps:   6, steps per second: 228, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3100.500 [636.000, 4974.000],  loss: --, mae: --, mean_q: --\n",
            "    84/30000: episode: 14, duration: 0.032s, episode steps:   6, steps per second: 185, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 3921.167 [2277.000, 5602.000],  loss: --, mae: --, mean_q: --\n",
            "    90/30000: episode: 15, duration: 0.020s, episode steps:   6, steps per second: 295, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3704.333 [597.000, 5742.000],  loss: --, mae: --, mean_q: --\n",
            "    96/30000: episode: 16, duration: 0.016s, episode steps:   6, steps per second: 373, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3006.667 [1763.000, 4685.000],  loss: --, mae: --, mean_q: --\n",
            "   102/30000: episode: 17, duration: 0.015s, episode steps:   6, steps per second: 389, episode reward: 75.000, mean reward: 12.500 [-5.000, 35.000], mean action: 3769.000 [1305.000, 5367.000],  loss: --, mae: --, mean_q: --\n",
            "   108/30000: episode: 18, duration: 0.016s, episode steps:   6, steps per second: 372, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4111.000 [996.000, 5753.000],  loss: --, mae: --, mean_q: --\n",
            "   114/30000: episode: 19, duration: 0.017s, episode steps:   6, steps per second: 358, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2794.833 [160.000, 5473.000],  loss: --, mae: --, mean_q: --\n",
            "   120/30000: episode: 20, duration: 0.016s, episode steps:   6, steps per second: 371, episode reward: 10.000, mean reward:  1.667 [-5.000, 10.000], mean action: 3016.833 [2205.000, 3985.000],  loss: --, mae: --, mean_q: --\n",
            "   126/30000: episode: 21, duration: 0.016s, episode steps:   6, steps per second: 367, episode reward: 60.000, mean reward: 10.000 [ 0.000, 40.000], mean action: 2267.000 [238.000, 5105.000],  loss: --, mae: --, mean_q: --\n",
            "   132/30000: episode: 22, duration: 0.016s, episode steps:   6, steps per second: 374, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2720.500 [161.000, 5326.000],  loss: --, mae: --, mean_q: --\n",
            "   138/30000: episode: 23, duration: 0.026s, episode steps:   6, steps per second: 232, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3141.833 [363.000, 5575.000],  loss: --, mae: --, mean_q: --\n",
            "   144/30000: episode: 24, duration: 0.019s, episode steps:   6, steps per second: 320, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2264.667 [733.000, 4460.000],  loss: --, mae: --, mean_q: --\n",
            "   150/30000: episode: 25, duration: 0.017s, episode steps:   6, steps per second: 349, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3719.167 [1599.000, 5523.000],  loss: --, mae: --, mean_q: --\n",
            "   156/30000: episode: 26, duration: 0.017s, episode steps:   6, steps per second: 359, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3173.333 [2335.000, 5017.000],  loss: --, mae: --, mean_q: --\n",
            "   162/30000: episode: 27, duration: 0.016s, episode steps:   6, steps per second: 381, episode reward: 35.000, mean reward:  5.833 [ 0.000, 15.000], mean action: 3154.333 [1202.000, 4974.000],  loss: --, mae: --, mean_q: --\n",
            "   168/30000: episode: 28, duration: 0.017s, episode steps:   6, steps per second: 356, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2614.333 [107.000, 5658.000],  loss: --, mae: --, mean_q: --\n",
            "   174/30000: episode: 29, duration: 0.017s, episode steps:   6, steps per second: 357, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1324.167 [195.000, 3221.000],  loss: --, mae: --, mean_q: --\n",
            "   180/30000: episode: 30, duration: 0.016s, episode steps:   6, steps per second: 368, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2844.167 [1035.000, 3869.000],  loss: --, mae: --, mean_q: --\n",
            "   186/30000: episode: 31, duration: 0.017s, episode steps:   6, steps per second: 359, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3089.667 [1002.000, 5017.000],  loss: --, mae: --, mean_q: --\n",
            "   192/30000: episode: 32, duration: 0.017s, episode steps:   6, steps per second: 353, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2224.500 [27.000, 5523.000],  loss: --, mae: --, mean_q: --\n",
            "   198/30000: episode: 33, duration: 0.020s, episode steps:   6, steps per second: 307, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3742.667 [2205.000, 4974.000],  loss: --, mae: --, mean_q: --\n",
            "   204/30000: episode: 34, duration: 0.018s, episode steps:   6, steps per second: 335, episode reward: 40.000, mean reward:  6.667 [ 0.000, 20.000], mean action: 2429.000 [1202.000, 4616.000],  loss: --, mae: --, mean_q: --\n",
            "   210/30000: episode: 35, duration: 0.026s, episode steps:   6, steps per second: 229, episode reward: 40.000, mean reward:  6.667 [ 0.000, 30.000], mean action: 1788.000 [869.000, 3502.000],  loss: --, mae: --, mean_q: --\n",
            "   216/30000: episode: 36, duration: 0.020s, episode steps:   6, steps per second: 306, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3211.667 [719.000, 5523.000],  loss: --, mae: --, mean_q: --\n",
            "   222/30000: episode: 37, duration: 0.019s, episode steps:   6, steps per second: 324, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3235.500 [1573.000, 4185.000],  loss: --, mae: --, mean_q: --\n",
            "   228/30000: episode: 38, duration: 0.019s, episode steps:   6, steps per second: 311, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2475.667 [852.000, 3183.000],  loss: --, mae: --, mean_q: --\n",
            "   234/30000: episode: 39, duration: 0.019s, episode steps:   6, steps per second: 324, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3210.833 [682.000, 5742.000],  loss: --, mae: --, mean_q: --\n",
            "   240/30000: episode: 40, duration: 0.019s, episode steps:   6, steps per second: 319, episode reward: 50.000, mean reward:  8.333 [ 0.000, 20.000], mean action: 3003.000 [1714.000, 4974.000],  loss: --, mae: --, mean_q: --\n",
            "   246/30000: episode: 41, duration: 0.017s, episode steps:   6, steps per second: 360, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2788.333 [238.000, 5747.000],  loss: --, mae: --, mean_q: --\n",
            "   252/30000: episode: 42, duration: 0.017s, episode steps:   6, steps per second: 345, episode reward: 50.000, mean reward:  8.333 [ 0.000, 30.000], mean action: 1758.500 [409.000, 2954.000],  loss: --, mae: --, mean_q: --\n",
            "   258/30000: episode: 43, duration: 0.017s, episode steps:   6, steps per second: 363, episode reward: 30.000, mean reward:  5.000 [ 0.000, 20.000], mean action: 4050.167 [2277.000, 5471.000],  loss: --, mae: --, mean_q: --\n",
            "   264/30000: episode: 44, duration: 0.016s, episode steps:   6, steps per second: 374, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 3402.167 [2858.000, 3779.000],  loss: --, mae: --, mean_q: --\n",
            "   270/30000: episode: 45, duration: 0.019s, episode steps:   6, steps per second: 319, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3267.000 [1109.000, 4977.000],  loss: --, mae: --, mean_q: --\n",
            "   276/30000: episode: 46, duration: 0.024s, episode steps:   6, steps per second: 253, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 5161.000 [2378.000, 5742.000],  loss: --, mae: --, mean_q: --\n",
            "   282/30000: episode: 47, duration: 0.018s, episode steps:   6, steps per second: 341, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3378.833 [1078.000, 5488.000],  loss: --, mae: --, mean_q: --\n",
            "   288/30000: episode: 48, duration: 0.017s, episode steps:   6, steps per second: 344, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2693.833 [591.000, 5396.000],  loss: --, mae: --, mean_q: --\n",
            "   294/30000: episode: 49, duration: 0.019s, episode steps:   6, steps per second: 316, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2089.500 [310.000, 4840.000],  loss: --, mae: --, mean_q: --\n",
            "   300/30000: episode: 50, duration: 0.017s, episode steps:   6, steps per second: 356, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1989.167 [352.000, 3222.000],  loss: --, mae: --, mean_q: --\n",
            "   306/30000: episode: 51, duration: 0.018s, episode steps:   6, steps per second: 343, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2612.167 [1132.000, 5697.000],  loss: --, mae: --, mean_q: --\n",
            "   312/30000: episode: 52, duration: 0.017s, episode steps:   6, steps per second: 347, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3472.833 [581.000, 5675.000],  loss: --, mae: --, mean_q: --\n",
            "   318/30000: episode: 53, duration: 0.016s, episode steps:   6, steps per second: 370, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 3750.667 [2127.000, 4940.000],  loss: --, mae: --, mean_q: --\n",
            "   324/30000: episode: 54, duration: 0.015s, episode steps:   6, steps per second: 390, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2836.333 [1128.000, 3960.000],  loss: --, mae: --, mean_q: --\n",
            "   330/30000: episode: 55, duration: 0.016s, episode steps:   6, steps per second: 366, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2063.333 [1002.000, 2669.000],  loss: --, mae: --, mean_q: --\n",
            "   336/30000: episode: 56, duration: 0.016s, episode steps:   6, steps per second: 384, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 2417.500 [442.000, 5523.000],  loss: --, mae: --, mean_q: --\n",
            "   342/30000: episode: 57, duration: 0.016s, episode steps:   6, steps per second: 384, episode reward:  5.000, mean reward:  0.833 [-5.000,  5.000], mean action: 3982.000 [1599.000, 5387.000],  loss: --, mae: --, mean_q: --\n",
            "   348/30000: episode: 58, duration: 0.024s, episode steps:   6, steps per second: 252, episode reward: 40.000, mean reward:  6.667 [ 0.000, 20.000], mean action: 3485.000 [238.000, 5523.000],  loss: --, mae: --, mean_q: --\n",
            "   354/30000: episode: 59, duration: 0.016s, episode steps:   6, steps per second: 369, episode reward: 15.000, mean reward:  2.500 [-5.000, 15.000], mean action: 4115.500 [163.000, 5742.000],  loss: --, mae: --, mean_q: --\n",
            "   360/30000: episode: 60, duration: 0.017s, episode steps:   6, steps per second: 358, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4353.833 [2588.000, 5163.000],  loss: --, mae: --, mean_q: --\n",
            "   366/30000: episode: 61, duration: 0.017s, episode steps:   6, steps per second: 350, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2483.167 [1350.000, 3863.000],  loss: --, mae: --, mean_q: --\n",
            "   372/30000: episode: 62, duration: 0.015s, episode steps:   6, steps per second: 393, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1766.667 [195.000, 3712.000],  loss: --, mae: --, mean_q: --\n",
            "   378/30000: episode: 63, duration: 0.016s, episode steps:   6, steps per second: 380, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2545.000 [670.000, 5742.000],  loss: --, mae: --, mean_q: --\n",
            "   384/30000: episode: 64, duration: 0.017s, episode steps:   6, steps per second: 357, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2871.833 [1355.000, 4616.000],  loss: --, mae: --, mean_q: --\n",
            "   390/30000: episode: 65, duration: 0.018s, episode steps:   6, steps per second: 340, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2187.000 [195.000, 3539.000],  loss: --, mae: --, mean_q: --\n",
            "   396/30000: episode: 66, duration: 0.017s, episode steps:   6, steps per second: 357, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3164.333 [1559.000, 5523.000],  loss: --, mae: --, mean_q: --\n",
            "   402/30000: episode: 67, duration: 0.016s, episode steps:   6, steps per second: 385, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3473.833 [1748.000, 5212.000],  loss: --, mae: --, mean_q: --\n",
            "   408/30000: episode: 68, duration: 0.016s, episode steps:   6, steps per second: 381, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2840.833 [1305.000, 4185.000],  loss: --, mae: --, mean_q: --\n",
            "   414/30000: episode: 69, duration: 0.016s, episode steps:   6, steps per second: 374, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 3486.333 [1074.000, 5742.000],  loss: --, mae: --, mean_q: --\n",
            "   420/30000: episode: 70, duration: 0.030s, episode steps:   6, steps per second: 201, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3514.833 [730.000, 5658.000],  loss: --, mae: --, mean_q: --\n",
            "   426/30000: episode: 71, duration: 0.018s, episode steps:   6, steps per second: 329, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3117.333 [852.000, 5523.000],  loss: --, mae: --, mean_q: --\n",
            "   432/30000: episode: 72, duration: 0.017s, episode steps:   6, steps per second: 351, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3605.667 [1057.000, 5608.000],  loss: --, mae: --, mean_q: --\n",
            "   438/30000: episode: 73, duration: 0.015s, episode steps:   6, steps per second: 388, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1746.833 [163.000, 4316.000],  loss: --, mae: --, mean_q: --\n",
            "   444/30000: episode: 74, duration: 0.015s, episode steps:   6, steps per second: 394, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2907.333 [163.000, 4210.000],  loss: --, mae: --, mean_q: --\n",
            "   450/30000: episode: 75, duration: 0.015s, episode steps:   6, steps per second: 392, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2876.667 [1530.000, 4974.000],  loss: --, mae: --, mean_q: --\n",
            "   456/30000: episode: 76, duration: 0.018s, episode steps:   6, steps per second: 330, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3382.333 [1748.000, 4908.000],  loss: --, mae: --, mean_q: --\n",
            "   462/30000: episode: 77, duration: 0.017s, episode steps:   6, steps per second: 357, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2850.167 [1202.000, 4763.000],  loss: --, mae: --, mean_q: --\n",
            "   468/30000: episode: 78, duration: 0.017s, episode steps:   6, steps per second: 347, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3092.833 [1961.000, 5250.000],  loss: --, mae: --, mean_q: --\n",
            "   474/30000: episode: 79, duration: 0.017s, episode steps:   6, steps per second: 349, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3667.167 [1575.000, 5348.000],  loss: --, mae: --, mean_q: --\n",
            "   480/30000: episode: 80, duration: 0.017s, episode steps:   6, steps per second: 356, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2790.333 [1573.000, 4900.000],  loss: --, mae: --, mean_q: --\n",
            "   486/30000: episode: 81, duration: 0.017s, episode steps:   6, steps per second: 353, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2848.167 [1260.000, 5749.000],  loss: --, mae: --, mean_q: --\n",
            "   492/30000: episode: 82, duration: 0.023s, episode steps:   6, steps per second: 264, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3297.500 [1014.000, 4613.000],  loss: --, mae: --, mean_q: --\n",
            "   498/30000: episode: 83, duration: 0.022s, episode steps:   6, steps per second: 272, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3969.333 [2205.000, 5656.000],  loss: --, mae: --, mean_q: --\n",
            "   504/30000: episode: 84, duration: 0.017s, episode steps:   6, steps per second: 351, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4389.000 [3788.000, 5212.000],  loss: --, mae: --, mean_q: --\n",
            "   510/30000: episode: 85, duration: 0.020s, episode steps:   6, steps per second: 308, episode reward:  5.000, mean reward:  0.833 [-5.000,  5.000], mean action: 2561.333 [195.000, 4646.000],  loss: --, mae: --, mean_q: --\n",
            "   516/30000: episode: 86, duration: 0.018s, episode steps:   6, steps per second: 340, episode reward: 10.000, mean reward:  1.667 [-5.000, 20.000], mean action: 2517.500 [163.000, 4464.000],  loss: --, mae: --, mean_q: --\n",
            "   522/30000: episode: 87, duration: 0.016s, episode steps:   6, steps per second: 365, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2866.333 [369.000, 4677.000],  loss: --, mae: --, mean_q: --\n",
            "   528/30000: episode: 88, duration: 0.017s, episode steps:   6, steps per second: 362, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3204.833 [2588.000, 5523.000],  loss: --, mae: --, mean_q: --\n",
            "   534/30000: episode: 89, duration: 0.016s, episode steps:   6, steps per second: 378, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4390.500 [2489.000, 5557.000],  loss: --, mae: --, mean_q: --\n",
            "   540/30000: episode: 90, duration: 0.019s, episode steps:   6, steps per second: 318, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3327.333 [355.000, 4945.000],  loss: --, mae: --, mean_q: --\n",
            "   546/30000: episode: 91, duration: 0.017s, episode steps:   6, steps per second: 348, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1395.833 [163.000, 3148.000],  loss: --, mae: --, mean_q: --\n",
            "   552/30000: episode: 92, duration: 0.016s, episode steps:   6, steps per second: 366, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 2228.167 [163.000, 3904.000],  loss: --, mae: --, mean_q: --\n",
            "   558/30000: episode: 93, duration: 0.024s, episode steps:   6, steps per second: 253, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3402.500 [2209.000, 4852.000],  loss: --, mae: --, mean_q: --\n",
            "   564/30000: episode: 94, duration: 0.018s, episode steps:   6, steps per second: 341, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3283.833 [100.000, 5523.000],  loss: --, mae: --, mean_q: --\n",
            "   570/30000: episode: 95, duration: 0.019s, episode steps:   6, steps per second: 319, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2994.500 [1531.000, 5370.000],  loss: --, mae: --, mean_q: --\n",
            "   576/30000: episode: 96, duration: 0.016s, episode steps:   6, steps per second: 385, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4169.167 [1905.000, 5740.000],  loss: --, mae: --, mean_q: --\n",
            "   582/30000: episode: 97, duration: 0.017s, episode steps:   6, steps per second: 353, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 1572.500 [534.000, 2455.000],  loss: --, mae: --, mean_q: --\n",
            "   588/30000: episode: 98, duration: 0.018s, episode steps:   6, steps per second: 333, episode reward: 30.000, mean reward:  5.000 [-5.000, 25.000], mean action: 1392.333 [27.000, 2985.000],  loss: --, mae: --, mean_q: --\n",
            "   594/30000: episode: 99, duration: 0.016s, episode steps:   6, steps per second: 365, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2454.000 [915.000, 5522.000],  loss: --, mae: --, mean_q: --\n",
            "   600/30000: episode: 100, duration: 0.016s, episode steps:   6, steps per second: 384, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 2921.833 [1202.000, 5523.000],  loss: --, mae: --, mean_q: --\n",
            "   606/30000: episode: 101, duration: 0.019s, episode steps:   6, steps per second: 309, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2694.667 [198.000, 5212.000],  loss: --, mae: --, mean_q: --\n",
            "   612/30000: episode: 102, duration: 0.016s, episode steps:   6, steps per second: 368, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 5705.500 [5523.000, 5742.000],  loss: --, mae: --, mean_q: --\n",
            "   618/30000: episode: 103, duration: 0.017s, episode steps:   6, steps per second: 350, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1899.333 [522.000, 2967.000],  loss: --, mae: --, mean_q: --\n",
            "   624/30000: episode: 104, duration: 0.015s, episode steps:   6, steps per second: 388, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2924.167 [534.000, 5608.000],  loss: --, mae: --, mean_q: --\n",
            "   630/30000: episode: 105, duration: 0.020s, episode steps:   6, steps per second: 301, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 1695.333 [318.000, 3252.000],  loss: --, mae: --, mean_q: --\n",
            "   636/30000: episode: 106, duration: 0.016s, episode steps:   6, steps per second: 378, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 4815.667 [1857.000, 5579.000],  loss: --, mae: --, mean_q: --\n",
            "   642/30000: episode: 107, duration: 0.016s, episode steps:   6, steps per second: 386, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 2320.333 [1202.000, 4316.000],  loss: --, mae: --, mean_q: --\n",
            "   648/30000: episode: 108, duration: 0.016s, episode steps:   6, steps per second: 372, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3722.500 [1090.000, 5557.000],  loss: --, mae: --, mean_q: --\n",
            "   654/30000: episode: 109, duration: 0.015s, episode steps:   6, steps per second: 394, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2627.000 [1257.000, 4909.000],  loss: --, mae: --, mean_q: --\n",
            "   660/30000: episode: 110, duration: 0.016s, episode steps:   6, steps per second: 368, episode reward: 30.000, mean reward:  5.000 [ 0.000, 20.000], mean action: 2087.833 [136.000, 3975.000],  loss: --, mae: --, mean_q: --\n",
            "   666/30000: episode: 111, duration: 0.016s, episode steps:   6, steps per second: 375, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 4023.333 [1141.000, 5742.000],  loss: --, mae: --, mean_q: --\n",
            "   672/30000: episode: 112, duration: 0.017s, episode steps:   6, steps per second: 353, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2483.833 [498.000, 4974.000],  loss: --, mae: --, mean_q: --\n",
            "   678/30000: episode: 113, duration: 0.016s, episode steps:   6, steps per second: 372, episode reward: 45.000, mean reward:  7.500 [-5.000, 35.000], mean action: 4047.000 [2455.000, 5212.000],  loss: --, mae: --, mean_q: --\n",
            "   684/30000: episode: 114, duration: 0.015s, episode steps:   6, steps per second: 388, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3564.000 [363.000, 5718.000],  loss: --, mae: --, mean_q: --\n",
            "   690/30000: episode: 115, duration: 0.015s, episode steps:   6, steps per second: 404, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2939.500 [1439.000, 4073.000],  loss: --, mae: --, mean_q: --\n",
            "   696/30000: episode: 116, duration: 0.015s, episode steps:   6, steps per second: 395, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2047.500 [163.000, 5367.000],  loss: --, mae: --, mean_q: --\n",
            "   702/30000: episode: 117, duration: 0.017s, episode steps:   6, steps per second: 358, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4523.000 [1078.000, 5212.000],  loss: --, mae: --, mean_q: --\n",
            "   708/30000: episode: 118, duration: 0.019s, episode steps:   6, steps per second: 312, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 2447.833 [106.000, 5718.000],  loss: --, mae: --, mean_q: --\n",
            "   714/30000: episode: 119, duration: 0.016s, episode steps:   6, steps per second: 385, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2674.333 [1632.000, 4220.000],  loss: --, mae: --, mean_q: --\n",
            "   720/30000: episode: 120, duration: 0.015s, episode steps:   6, steps per second: 394, episode reward: 55.000, mean reward:  9.167 [ 0.000, 20.000], mean action: 2055.833 [1202.000, 5718.000],  loss: --, mae: --, mean_q: --\n",
            "   726/30000: episode: 121, duration: 0.018s, episode steps:   6, steps per second: 339, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2828.000 [852.000, 5496.000],  loss: --, mae: --, mean_q: --\n",
            "   732/30000: episode: 122, duration: 0.016s, episode steps:   6, steps per second: 380, episode reward: 25.000, mean reward:  4.167 [-5.000, 30.000], mean action: 2125.833 [27.000, 4613.000],  loss: --, mae: --, mean_q: --\n",
            "   738/30000: episode: 123, duration: 0.016s, episode steps:   6, steps per second: 378, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 1902.667 [363.000, 2688.000],  loss: --, mae: --, mean_q: --\n",
            "   744/30000: episode: 124, duration: 0.015s, episode steps:   6, steps per second: 392, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3562.500 [2021.000, 5523.000],  loss: --, mae: --, mean_q: --\n",
            "   750/30000: episode: 125, duration: 0.015s, episode steps:   6, steps per second: 389, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2709.167 [852.000, 5117.000],  loss: --, mae: --, mean_q: --\n",
            "   756/30000: episode: 126, duration: 0.019s, episode steps:   6, steps per second: 318, episode reward: 35.000, mean reward:  5.833 [ 0.000, 35.000], mean action: 3347.833 [1765.000, 5656.000],  loss: --, mae: --, mean_q: --\n",
            "   762/30000: episode: 127, duration: 0.021s, episode steps:   6, steps per second: 291, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2738.000 [852.000, 4573.000],  loss: --, mae: --, mean_q: --\n",
            "   768/30000: episode: 128, duration: 0.016s, episode steps:   6, steps per second: 384, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2962.667 [1959.000, 5127.000],  loss: --, mae: --, mean_q: --\n",
            "   774/30000: episode: 129, duration: 0.016s, episode steps:   6, steps per second: 384, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3046.000 [1269.000, 5742.000],  loss: --, mae: --, mean_q: --\n",
            "   780/30000: episode: 130, duration: 0.026s, episode steps:   6, steps per second: 232, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3543.333 [356.000, 5656.000],  loss: --, mae: --, mean_q: --\n",
            "   786/30000: episode: 131, duration: 0.015s, episode steps:   6, steps per second: 394, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4533.667 [861.000, 5753.000],  loss: --, mae: --, mean_q: --\n",
            "   792/30000: episode: 132, duration: 0.015s, episode steps:   6, steps per second: 390, episode reward:  0.000, mean reward:  0.000 [-5.000, 10.000], mean action: 2200.833 [485.000, 5742.000],  loss: --, mae: --, mean_q: --\n",
            "   798/30000: episode: 133, duration: 0.015s, episode steps:   6, steps per second: 396, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3652.167 [1358.000, 5742.000],  loss: --, mae: --, mean_q: --\n",
            "   804/30000: episode: 134, duration: 0.015s, episode steps:   6, steps per second: 391, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3819.500 [1112.000, 5557.000],  loss: --, mae: --, mean_q: --\n",
            "   810/30000: episode: 135, duration: 0.016s, episode steps:   6, steps per second: 368, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3171.500 [238.000, 5523.000],  loss: --, mae: --, mean_q: --\n",
            "   816/30000: episode: 136, duration: 0.015s, episode steps:   6, steps per second: 400, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 1666.500 [163.000, 3770.000],  loss: --, mae: --, mean_q: --\n",
            "   822/30000: episode: 137, duration: 0.015s, episode steps:   6, steps per second: 404, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3387.000 [1620.000, 5523.000],  loss: --, mae: --, mean_q: --\n",
            "   828/30000: episode: 138, duration: 0.015s, episode steps:   6, steps per second: 399, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2478.833 [2205.000, 3598.000],  loss: --, mae: --, mean_q: --\n",
            "   834/30000: episode: 139, duration: 0.015s, episode steps:   6, steps per second: 400, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3535.833 [1596.000, 5742.000],  loss: --, mae: --, mean_q: --\n",
            "   840/30000: episode: 140, duration: 0.016s, episode steps:   6, steps per second: 373, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3316.000 [1090.000, 5212.000],  loss: --, mae: --, mean_q: --\n",
            "   846/30000: episode: 141, duration: 0.017s, episode steps:   6, steps per second: 352, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2888.333 [1641.000, 4258.000],  loss: --, mae: --, mean_q: --\n",
            "   852/30000: episode: 142, duration: 0.015s, episode steps:   6, steps per second: 394, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4165.500 [2205.000, 4977.000],  loss: --, mae: --, mean_q: --\n",
            "   858/30000: episode: 143, duration: 0.019s, episode steps:   6, steps per second: 308, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3728.833 [2205.000, 5517.000],  loss: --, mae: --, mean_q: --\n",
            "   864/30000: episode: 144, duration: 0.018s, episode steps:   6, steps per second: 335, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2953.167 [382.000, 4762.000],  loss: --, mae: --, mean_q: --\n",
            "   870/30000: episode: 145, duration: 0.016s, episode steps:   6, steps per second: 371, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3029.167 [1821.000, 5471.000],  loss: --, mae: --, mean_q: --\n",
            "   876/30000: episode: 146, duration: 0.016s, episode steps:   6, steps per second: 369, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3029.500 [852.000, 5300.000],  loss: --, mae: --, mean_q: --\n",
            "   882/30000: episode: 147, duration: 0.016s, episode steps:   6, steps per second: 370, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3037.333 [1109.000, 4305.000],  loss: --, mae: --, mean_q: --\n",
            "   888/30000: episode: 148, duration: 0.016s, episode steps:   6, steps per second: 374, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4111.833 [2021.000, 5523.000],  loss: --, mae: --, mean_q: --\n",
            "   894/30000: episode: 149, duration: 0.017s, episode steps:   6, steps per second: 352, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4552.333 [3819.000, 5289.000],  loss: --, mae: --, mean_q: --\n",
            "   900/30000: episode: 150, duration: 0.018s, episode steps:   6, steps per second: 339, episode reward: 50.000, mean reward:  8.333 [ 0.000, 35.000], mean action: 2593.500 [263.000, 5212.000],  loss: --, mae: --, mean_q: --\n",
            "   906/30000: episode: 151, duration: 0.019s, episode steps:   6, steps per second: 324, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2257.000 [479.000, 5326.000],  loss: --, mae: --, mean_q: --\n",
            "   912/30000: episode: 152, duration: 0.017s, episode steps:   6, steps per second: 356, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3602.167 [527.000, 5484.000],  loss: --, mae: --, mean_q: --\n",
            "   913/30000: episode: 153, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1338.000 [1338.000, 1338.000],  loss: --, mae: --, mean_q: --\n",
            "   919/30000: episode: 154, duration: 0.019s, episode steps:   6, steps per second: 314, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2533.167 [1558.000, 4185.000],  loss: --, mae: --, mean_q: --\n",
            "   925/30000: episode: 155, duration: 0.024s, episode steps:   6, steps per second: 254, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3280.500 [1550.000, 5656.000],  loss: --, mae: --, mean_q: --\n",
            "   931/30000: episode: 156, duration: 0.017s, episode steps:   6, steps per second: 348, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 3604.167 [2205.000, 4971.000],  loss: --, mae: --, mean_q: --\n",
            "   937/30000: episode: 157, duration: 0.016s, episode steps:   6, steps per second: 376, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2613.667 [235.000, 5523.000],  loss: --, mae: --, mean_q: --\n",
            "   943/30000: episode: 158, duration: 0.015s, episode steps:   6, steps per second: 388, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2818.833 [1355.000, 3598.000],  loss: --, mae: --, mean_q: --\n",
            "   949/30000: episode: 159, duration: 0.015s, episode steps:   6, steps per second: 412, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 2996.000 [290.000, 4248.000],  loss: --, mae: --, mean_q: --\n",
            "   955/30000: episode: 160, duration: 0.016s, episode steps:   6, steps per second: 375, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1905.000 [852.000, 3469.000],  loss: --, mae: --, mean_q: --\n",
            "   961/30000: episode: 161, duration: 0.016s, episode steps:   6, steps per second: 386, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 1916.667 [163.000, 5742.000],  loss: --, mae: --, mean_q: --\n",
            "   967/30000: episode: 162, duration: 0.016s, episode steps:   6, steps per second: 368, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2288.833 [522.000, 5730.000],  loss: --, mae: --, mean_q: --\n",
            "   973/30000: episode: 163, duration: 0.016s, episode steps:   6, steps per second: 365, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 5092.167 [4493.000, 5212.000],  loss: --, mae: --, mean_q: --\n",
            "   979/30000: episode: 164, duration: 0.017s, episode steps:   6, steps per second: 360, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3386.167 [745.000, 5658.000],  loss: --, mae: --, mean_q: --\n",
            "   985/30000: episode: 165, duration: 0.017s, episode steps:   6, steps per second: 356, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2923.167 [14.000, 5251.000],  loss: --, mae: --, mean_q: --\n",
            "   991/30000: episode: 166, duration: 0.017s, episode steps:   6, steps per second: 358, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2358.333 [1530.000, 5753.000],  loss: --, mae: --, mean_q: --\n",
            "   997/30000: episode: 167, duration: 0.020s, episode steps:   6, steps per second: 302, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2916.833 [1091.000, 4765.000],  loss: --, mae: --, mean_q: --\n",
            "  1003/30000: episode: 168, duration: 0.091s, episode steps:   6, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3477.833 [290.000, 5718.000],  loss: 53.158807, mae: 11.041305, mean_q: 24.965823\n",
            "  1009/30000: episode: 169, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3378.167 [2057.000, 4498.000],  loss: 46.182026, mae: 12.740563, mean_q: 27.871101\n",
            "  1015/30000: episode: 170, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2538.167 [459.000, 4109.000],  loss: 32.571880, mae: 12.604178, mean_q: 27.028679\n",
            "  1021/30000: episode: 171, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2570.333 [391.000, 5017.000],  loss: 46.304424, mae: 14.701084, mean_q: 30.994690\n",
            "  1027/30000: episode: 172, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3317.667 [1573.000, 5486.000],  loss: 42.878891, mae: 15.232112, mean_q: 32.203938\n",
            "  1033/30000: episode: 173, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2787.000 [1371.000, 4157.000],  loss: 42.343475, mae: 13.899163, mean_q: 29.554148\n",
            "  1039/30000: episode: 174, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2387.333 [163.000, 4474.000],  loss: 25.936028, mae: 13.592580, mean_q: 28.827131\n",
            "  1045/30000: episode: 175, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2504.333 [269.000, 5027.000],  loss: 43.372295, mae: 13.614914, mean_q: 29.075485\n",
            "  1051/30000: episode: 176, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2260.167 [130.000, 5366.000],  loss: 40.425602, mae: 15.397923, mean_q: 32.315235\n",
            "  1057/30000: episode: 177, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3169.000 [429.000, 5703.000],  loss: 38.648121, mae: 15.309371, mean_q: 31.914125\n",
            "  1063/30000: episode: 178, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3302.500 [1530.000, 5283.000],  loss: 34.419994, mae: 14.140103, mean_q: 29.996178\n",
            "  1069/30000: episode: 179, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2351.167 [230.000, 4185.000],  loss: 58.154591, mae: 14.603433, mean_q: 30.642679\n",
            "  1075/30000: episode: 180, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 2861.000 [1849.000, 4365.000],  loss: 39.980614, mae: 14.491395, mean_q: 30.227743\n",
            "  1081/30000: episode: 181, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [-5.000, 20.000], mean action: 3451.833 [2932.000, 4185.000],  loss: 34.756241, mae: 14.260257, mean_q: 30.141314\n",
            "  1087/30000: episode: 182, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3090.167 [590.000, 5213.000],  loss: 34.866665, mae: 14.257855, mean_q: 30.140015\n",
            "  1093/30000: episode: 183, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3109.667 [1540.000, 4552.000],  loss: 40.173458, mae: 14.055245, mean_q: 29.751722\n",
            "  1099/30000: episode: 184, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 2495.833 [130.000, 4163.000],  loss: 48.751881, mae: 14.188146, mean_q: 30.019415\n",
            "  1105/30000: episode: 185, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2456.333 [130.000, 4464.000],  loss: 43.635548, mae: 14.112430, mean_q: 29.053076\n",
            "  1111/30000: episode: 186, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3285.833 [870.000, 4974.000],  loss: 27.074354, mae: 14.597032, mean_q: 30.423677\n",
            "  1117/30000: episode: 187, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3164.833 [130.000, 4258.000],  loss: 40.073746, mae: 14.776532, mean_q: 31.029074\n",
            "  1123/30000: episode: 188, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2859.167 [238.000, 4975.000],  loss: 36.878017, mae: 14.346726, mean_q: 30.033060\n",
            "  1129/30000: episode: 189, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3416.167 [1172.000, 4616.000],  loss: 34.554955, mae: 14.169837, mean_q: 29.450445\n",
            "  1135/30000: episode: 190, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3055.667 [903.000, 4974.000],  loss: 53.112423, mae: 13.906348, mean_q: 29.447573\n",
            "  1141/30000: episode: 191, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3648.167 [2277.000, 5517.000],  loss: 26.746630, mae: 13.943821, mean_q: 29.641619\n",
            "  1147/30000: episode: 192, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2615.000 [1202.000, 5005.000],  loss: 37.369476, mae: 14.367752, mean_q: 30.340021\n",
            "  1153/30000: episode: 193, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2602.000 [1031.000, 4193.000],  loss: 33.662247, mae: 14.219948, mean_q: 30.101492\n",
            "  1159/30000: episode: 194, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2075.667 [757.000, 3100.000],  loss: 47.039951, mae: 14.575667, mean_q: 30.734869\n",
            "  1165/30000: episode: 195, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 2750.333 [590.000, 5561.000],  loss: 32.289745, mae: 15.135215, mean_q: 32.026196\n",
            "  1171/30000: episode: 196, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3040.500 [2086.000, 5561.000],  loss: 50.558529, mae: 15.059799, mean_q: 32.054855\n",
            "  1177/30000: episode: 197, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2639.500 [1606.000, 4041.000],  loss: 29.181856, mae: 14.323031, mean_q: 31.014603\n",
            "  1183/30000: episode: 198, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2441.500 [982.000, 3847.000],  loss: 38.482296, mae: 14.005305, mean_q: 29.994005\n",
            "  1189/30000: episode: 199, duration: 0.196s, episode steps:   6, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2616.000 [483.000, 3658.000],  loss: 31.096611, mae: 14.199203, mean_q: 30.861551\n",
            "  1195/30000: episode: 200, duration: 0.264s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2092.500 [140.000, 4974.000],  loss: 43.050217, mae: 15.384814, mean_q: 32.634037\n",
            "  1201/30000: episode: 201, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2840.333 [382.000, 4613.000],  loss: 29.916931, mae: 15.391482, mean_q: 32.749481\n",
            "  1207/30000: episode: 202, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3054.500 [1484.000, 4162.000],  loss: 36.085484, mae: 14.668701, mean_q: 31.023684\n",
            "  1213/30000: episode: 203, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3646.167 [2558.000, 5057.000],  loss: 45.090561, mae: 15.335903, mean_q: 32.796764\n",
            "  1219/30000: episode: 204, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2323.000 [924.000, 3825.000],  loss: 30.311163, mae: 15.273834, mean_q: 32.285885\n",
            "  1225/30000: episode: 205, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 2180.000 [107.000, 4372.000],  loss: 23.650330, mae: 14.632186, mean_q: 31.184439\n",
            "  1231/30000: episode: 206, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2578.833 [71.000, 3825.000],  loss: 50.456028, mae: 15.566983, mean_q: 33.524719\n",
            "  1237/30000: episode: 207, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward: 50.000, mean reward:  8.333 [ 0.000, 40.000], mean action: 2772.833 [648.000, 4332.000],  loss: 42.541637, mae: 14.449585, mean_q: 31.555510\n",
            "  1243/30000: episode: 208, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2335.167 [429.000, 5105.000],  loss: 46.064960, mae: 14.414464, mean_q: 30.752640\n",
            "  1249/30000: episode: 209, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3150.667 [1641.000, 5180.000],  loss: 35.621437, mae: 15.243785, mean_q: 32.113453\n",
            "  1255/30000: episode: 210, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2073.833 [451.000, 3253.000],  loss: 50.127735, mae: 14.781509, mean_q: 30.999308\n",
            "  1261/30000: episode: 211, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3232.167 [1002.000, 5118.000],  loss: 28.503008, mae: 14.547116, mean_q: 30.370026\n",
            "  1267/30000: episode: 212, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3601.000 [512.000, 4592.000],  loss: 41.450871, mae: 14.115211, mean_q: 29.839386\n",
            "  1273/30000: episode: 213, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3070.667 [508.000, 5411.000],  loss: 45.349804, mae: 14.147441, mean_q: 29.729767\n",
            "  1279/30000: episode: 214, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 3332.167 [2237.000, 4629.000],  loss: 33.364277, mae: 14.887707, mean_q: 31.136047\n",
            "  1285/30000: episode: 215, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2242.667 [225.000, 3926.000],  loss: 49.059566, mae: 15.240150, mean_q: 31.498337\n",
            "  1291/30000: episode: 216, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2482.333 [36.000, 4486.000],  loss: 55.533459, mae: 14.774100, mean_q: 30.462862\n",
            "  1297/30000: episode: 217, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2866.000 [69.000, 5340.000],  loss: 32.075169, mae: 13.834289, mean_q: 28.971949\n",
            "  1303/30000: episode: 218, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3019.833 [1097.000, 5129.000],  loss: 67.697433, mae: 15.797105, mean_q: 32.672016\n",
            "  1309/30000: episode: 219, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3275.667 [2055.000, 4430.000],  loss: 36.473354, mae: 14.438657, mean_q: 30.055916\n",
            "  1315/30000: episode: 220, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2733.500 [992.000, 4584.000],  loss: 43.112682, mae: 13.746025, mean_q: 28.774527\n",
            "  1321/30000: episode: 221, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1372.500 [290.000, 2620.000],  loss: 41.753700, mae: 13.877811, mean_q: 28.876999\n",
            "  1327/30000: episode: 222, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3673.333 [419.000, 4616.000],  loss: 36.742214, mae: 15.744340, mean_q: 32.688198\n",
            "  1333/30000: episode: 223, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2413.667 [419.000, 4474.000],  loss: 30.050583, mae: 15.360252, mean_q: 32.013554\n",
            "  1339/30000: episode: 224, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2196.667 [419.000, 4474.000],  loss: 29.550398, mae: 13.796800, mean_q: 29.496153\n",
            "  1345/30000: episode: 225, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: 45.000, mean reward:  7.500 [ 0.000, 45.000], mean action: 3150.167 [868.000, 5011.000],  loss: 43.264786, mae: 14.619847, mean_q: 30.839027\n",
            "  1351/30000: episode: 226, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2578.667 [790.000, 4185.000],  loss: 35.134098, mae: 16.200514, mean_q: 33.275414\n",
            "  1357/30000: episode: 227, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 3648.833 [1576.000, 4332.000],  loss: 52.490192, mae: 14.865715, mean_q: 31.796089\n",
            "  1363/30000: episode: 228, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3078.500 [419.000, 5640.000],  loss: 44.579273, mae: 14.861022, mean_q: 31.241282\n",
            "  1369/30000: episode: 229, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2066.500 [419.000, 3723.000],  loss: 33.149036, mae: 14.950686, mean_q: 31.674501\n",
            "  1375/30000: episode: 230, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2028.500 [429.000, 3606.000],  loss: 29.480734, mae: 14.284427, mean_q: 30.478888\n",
            "  1381/30000: episode: 231, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward: 50.000, mean reward:  8.333 [ 0.000, 15.000], mean action: 1855.500 [106.000, 4332.000],  loss: 48.971478, mae: 14.866069, mean_q: 31.518766\n",
            "  1387/30000: episode: 232, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3248.833 [1596.000, 4332.000],  loss: 40.347317, mae: 14.776212, mean_q: 30.971144\n",
            "  1393/30000: episode: 233, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3509.333 [1282.000, 4332.000],  loss: 44.451870, mae: 15.237880, mean_q: 32.283127\n",
            "  1399/30000: episode: 234, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 1411.667 [27.000, 5488.000],  loss: 53.526691, mae: 13.839160, mean_q: 29.380774\n",
            "  1405/30000: episode: 235, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4401.500 [1736.000, 5387.000],  loss: 33.902508, mae: 13.773486, mean_q: 28.535997\n",
            "  1411/30000: episode: 236, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3693.833 [1282.000, 5387.000],  loss: 33.073315, mae: 15.562318, mean_q: 31.385523\n",
            "  1417/30000: episode: 237, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2210.167 [429.000, 4464.000],  loss: 28.819124, mae: 14.344769, mean_q: 29.684198\n",
            "  1423/30000: episode: 238, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3095.667 [238.000, 5504.000],  loss: 37.864639, mae: 14.190370, mean_q: 29.262268\n",
            "  1429/30000: episode: 239, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3220.333 [429.000, 5387.000],  loss: 40.685841, mae: 14.854797, mean_q: 30.737719\n",
            "  1435/30000: episode: 240, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4442.333 [3411.000, 5387.000],  loss: 54.753139, mae: 14.616084, mean_q: 30.336897\n",
            "  1441/30000: episode: 241, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3141.333 [269.000, 5488.000],  loss: 26.056564, mae: 14.441106, mean_q: 29.980528\n",
            "  1447/30000: episode: 242, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4161.167 [2532.000, 5565.000],  loss: 29.702711, mae: 14.432965, mean_q: 30.222023\n",
            "  1453/30000: episode: 243, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4253.000 [2550.000, 5531.000],  loss: 30.939987, mae: 15.395214, mean_q: 31.313864\n",
            "  1459/30000: episode: 244, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4047.667 [3069.000, 5140.000],  loss: 32.969402, mae: 14.883102, mean_q: 30.735613\n",
            "  1465/30000: episode: 245, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2038.333 [359.000, 4576.000],  loss: 42.536865, mae: 15.842372, mean_q: 32.476501\n",
            "  1471/30000: episode: 246, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1788.833 [419.000, 5639.000],  loss: 45.626205, mae: 15.901337, mean_q: 32.935291\n",
            "  1477/30000: episode: 247, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3788.500 [1282.000, 5302.000],  loss: 40.182129, mae: 15.863158, mean_q: 32.837284\n",
            "  1483/30000: episode: 248, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2812.167 [1087.000, 4185.000],  loss: 35.252094, mae: 13.518245, mean_q: 28.644304\n",
            "  1489/30000: episode: 249, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3089.667 [642.000, 5204.000],  loss: 34.131485, mae: 14.306007, mean_q: 29.622559\n",
            "  1495/30000: episode: 250, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2326.667 [1029.000, 4250.000],  loss: 37.039814, mae: 14.777844, mean_q: 30.249258\n",
            "  1501/30000: episode: 251, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1613.667 [419.000, 4176.000],  loss: 41.908512, mae: 15.694423, mean_q: 32.341038\n",
            "  1507/30000: episode: 252, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2684.833 [1098.000, 4922.000],  loss: 32.333359, mae: 13.467952, mean_q: 28.360962\n",
            "  1513/30000: episode: 253, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 3355.000 [3355.000, 3355.000],  loss: 31.331213, mae: 15.924502, mean_q: 32.291298\n",
            "  1519/30000: episode: 254, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3155.000 [419.000, 5362.000],  loss: 46.636021, mae: 14.053544, mean_q: 29.501181\n",
            "  1525/30000: episode: 255, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2707.167 [1596.000, 3949.000],  loss: 41.509037, mae: 14.693974, mean_q: 30.269419\n",
            "  1531/30000: episode: 256, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 110.000, mean reward: 18.333 [ 0.000, 40.000], mean action: 2481.500 [1338.000, 4079.000],  loss: 50.384548, mae: 15.732858, mean_q: 32.216785\n",
            "  1537/30000: episode: 257, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2704.833 [269.000, 5511.000],  loss: 56.749668, mae: 16.103834, mean_q: 32.671406\n",
            "  1543/30000: episode: 258, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1945.000 [316.000, 3748.000],  loss: 30.969559, mae: 15.609539, mean_q: 31.688627\n",
            "  1549/30000: episode: 259, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2806.667 [493.000, 5616.000],  loss: 37.434566, mae: 14.520062, mean_q: 30.073400\n",
            "  1555/30000: episode: 260, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3785.667 [1559.000, 5608.000],  loss: 29.490128, mae: 15.563998, mean_q: 32.114349\n",
            "  1561/30000: episode: 261, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3390.000 [2027.000, 5387.000],  loss: 39.123676, mae: 15.309960, mean_q: 31.715414\n",
            "  1567/30000: episode: 262, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2972.333 [238.000, 4922.000],  loss: 33.439346, mae: 15.941258, mean_q: 32.460819\n",
            "  1573/30000: episode: 263, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 40.000, mean reward:  6.667 [ 0.000, 30.000], mean action: 2863.500 [238.000, 5057.000],  loss: 32.678310, mae: 15.008842, mean_q: 31.487635\n",
            "  1579/30000: episode: 264, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 3572.667 [1957.000, 5608.000],  loss: 38.950748, mae: 14.004474, mean_q: 29.726965\n",
            "  1585/30000: episode: 265, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 3085.667 [238.000, 4954.000],  loss: 27.813253, mae: 15.151540, mean_q: 31.562075\n",
            "  1591/30000: episode: 266, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3439.667 [1736.000, 5014.000],  loss: 33.330433, mae: 14.163625, mean_q: 29.351770\n",
            "  1597/30000: episode: 267, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: 45.000, mean reward:  7.500 [-5.000, 35.000], mean action: 3079.500 [1283.000, 4550.000],  loss: 40.709080, mae: 13.783463, mean_q: 28.944052\n",
            "  1603/30000: episode: 268, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2679.167 [258.000, 5608.000],  loss: 34.204212, mae: 14.114491, mean_q: 29.639519\n",
            "  1609/30000: episode: 269, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4741.667 [3069.000, 5568.000],  loss: 30.133558, mae: 14.522435, mean_q: 30.271063\n",
            "  1615/30000: episode: 270, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2917.500 [1215.000, 5283.000],  loss: 41.470875, mae: 14.513215, mean_q: 30.285439\n",
            "  1621/30000: episode: 271, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3179.000 [2898.000, 4185.000],  loss: 27.175087, mae: 15.571865, mean_q: 31.787285\n",
            "  1627/30000: episode: 272, duration: 0.200s, episode steps:   6, steps per second:  30, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 2222.667 [419.000, 3786.000],  loss: 37.672623, mae: 15.199496, mean_q: 30.722063\n",
            "  1633/30000: episode: 273, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2904.667 [428.000, 5016.000],  loss: 42.660709, mae: 15.439686, mean_q: 31.356195\n",
            "  1639/30000: episode: 274, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 4784.833 [4189.000, 5608.000],  loss: 40.122082, mae: 14.158853, mean_q: 29.148840\n",
            "  1645/30000: episode: 275, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward: 140.000, mean reward: 23.333 [ 0.000, 35.000], mean action: 3569.500 [238.000, 4439.000],  loss: 32.279324, mae: 14.691308, mean_q: 30.752554\n",
            "  1651/30000: episode: 276, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2795.833 [742.000, 5639.000],  loss: 27.736383, mae: 15.071952, mean_q: 31.763748\n",
            "  1657/30000: episode: 277, duration: 0.241s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2495.667 [742.000, 4258.000],  loss: 30.584223, mae: 14.816116, mean_q: 31.346039\n",
            "  1663/30000: episode: 278, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2157.667 [742.000, 3779.000],  loss: 29.968874, mae: 15.047200, mean_q: 31.543116\n",
            "  1669/30000: episode: 279, duration: 0.264s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2941.167 [742.000, 5616.000],  loss: 45.522110, mae: 14.694028, mean_q: 31.229179\n",
            "  1675/30000: episode: 280, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward: 35.000, mean reward:  5.833 [-5.000, 25.000], mean action: 1726.833 [742.000, 3411.000],  loss: 32.890755, mae: 14.799426, mean_q: 31.790421\n",
            "  1681/30000: episode: 281, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3325.000 [1596.000, 4399.000],  loss: 40.250832, mae: 13.266891, mean_q: 28.976416\n",
            "  1687/30000: episode: 282, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2020.500 [742.000, 4009.000],  loss: 50.275639, mae: 15.129745, mean_q: 32.578308\n",
            "  1693/30000: episode: 283, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2844.333 [969.000, 4399.000],  loss: 38.549057, mae: 16.932425, mean_q: 35.915241\n",
            "  1699/30000: episode: 284, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2163.000 [553.000, 5703.000],  loss: 23.234613, mae: 15.345730, mean_q: 32.595398\n",
            "  1705/30000: episode: 285, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2734.833 [394.000, 5475.000],  loss: 35.564381, mae: 14.295783, mean_q: 30.408951\n",
            "  1711/30000: episode: 286, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2966.167 [742.000, 3411.000],  loss: 32.810390, mae: 14.583252, mean_q: 31.412207\n",
            "  1717/30000: episode: 287, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2909.333 [742.000, 5646.000],  loss: 28.656622, mae: 15.019811, mean_q: 31.590662\n",
            "  1723/30000: episode: 288, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2498.833 [607.000, 4530.000],  loss: 36.581104, mae: 15.066753, mean_q: 31.564598\n",
            "  1729/30000: episode: 289, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3423.167 [138.000, 4894.000],  loss: 33.912621, mae: 14.388734, mean_q: 30.550127\n",
            "  1735/30000: episode: 290, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3382.500 [969.000, 5703.000],  loss: 38.025597, mae: 14.451298, mean_q: 30.702810\n",
            "  1741/30000: episode: 291, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2339.667 [295.000, 5656.000],  loss: 48.722424, mae: 14.836267, mean_q: 31.078552\n",
            "  1747/30000: episode: 292, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2821.667 [957.000, 4681.000],  loss: 23.770105, mae: 14.024765, mean_q: 29.912687\n",
            "  1753/30000: episode: 293, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2442.667 [969.000, 5005.000],  loss: 33.929390, mae: 15.980262, mean_q: 33.297062\n",
            "  1759/30000: episode: 294, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3495.833 [969.000, 5027.000],  loss: 39.063046, mae: 16.190187, mean_q: 33.803097\n",
            "  1765/30000: episode: 295, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3413.167 [969.000, 5616.000],  loss: 45.085800, mae: 15.343957, mean_q: 32.134136\n",
            "  1771/30000: episode: 296, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3342.667 [1641.000, 5000.000],  loss: 35.115383, mae: 15.673549, mean_q: 32.453247\n",
            "  1777/30000: episode: 297, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2990.000 [969.000, 5616.000],  loss: 26.203264, mae: 14.506927, mean_q: 30.202982\n",
            "  1783/30000: episode: 298, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2818.167 [1014.000, 4816.000],  loss: 35.198803, mae: 14.431689, mean_q: 30.708757\n",
            "  1789/30000: episode: 299, duration: 0.164s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3180.333 [2197.000, 4849.000],  loss: 26.371202, mae: 15.680951, mean_q: 32.163273\n",
            "  1795/30000: episode: 300, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3113.167 [419.000, 4978.000],  loss: 35.710739, mae: 15.366555, mean_q: 32.224258\n",
            "  1801/30000: episode: 301, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3733.167 [969.000, 5616.000],  loss: 42.148033, mae: 14.824730, mean_q: 30.853060\n",
            "  1807/30000: episode: 302, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2632.833 [1207.000, 4799.000],  loss: 45.951160, mae: 15.804006, mean_q: 33.153980\n",
            "  1813/30000: episode: 303, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 5208.000 [2733.000, 5703.000],  loss: 25.869799, mae: 14.612620, mean_q: 30.929703\n",
            "  1819/30000: episode: 304, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2459.000 [92.000, 5667.000],  loss: 21.174185, mae: 14.308915, mean_q: 30.342661\n",
            "  1825/30000: episode: 305, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 3032.500 [1109.000, 4407.000],  loss: 32.154461, mae: 15.596704, mean_q: 32.387817\n",
            "  1831/30000: episode: 306, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3553.833 [1132.000, 4893.000],  loss: 40.142822, mae: 15.611703, mean_q: 32.079479\n",
            "  1837/30000: episode: 307, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2666.667 [451.000, 5616.000],  loss: 49.685505, mae: 15.061200, mean_q: 31.681374\n",
            "  1843/30000: episode: 308, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 2801.167 [428.000, 5302.000],  loss: 38.335846, mae: 14.552638, mean_q: 30.228943\n",
            "  1849/30000: episode: 309, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2410.500 [969.000, 4185.000],  loss: 36.455292, mae: 14.346527, mean_q: 30.328768\n",
            "  1855/30000: episode: 310, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2486.167 [846.000, 4576.000],  loss: 26.888189, mae: 15.014291, mean_q: 31.756453\n",
            "  1861/30000: episode: 311, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 85.000, mean reward: 14.167 [ 0.000, 25.000], mean action: 2247.167 [709.000, 3531.000],  loss: 33.842522, mae: 16.263865, mean_q: 33.654888\n",
            "  1867/30000: episode: 312, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2609.500 [674.000, 5486.000],  loss: 39.166996, mae: 15.376027, mean_q: 32.257339\n",
            "  1873/30000: episode: 313, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2406.333 [803.000, 3587.000],  loss: 44.676075, mae: 14.953662, mean_q: 31.095818\n",
            "  1879/30000: episode: 314, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2289.833 [200.000, 4874.000],  loss: 30.373896, mae: 15.569362, mean_q: 32.222767\n",
            "  1885/30000: episode: 315, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2559.833 [803.000, 3779.000],  loss: 23.738417, mae: 14.984196, mean_q: 31.259363\n",
            "  1891/30000: episode: 316, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2967.000 [428.000, 5482.000],  loss: 33.475849, mae: 14.930588, mean_q: 31.353872\n",
            "  1897/30000: episode: 317, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3390.833 [1530.000, 5616.000],  loss: 28.852234, mae: 14.939320, mean_q: 31.416885\n",
            "  1903/30000: episode: 318, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3898.167 [1338.000, 5616.000],  loss: 40.314617, mae: 14.613361, mean_q: 30.639189\n",
            "  1909/30000: episode: 319, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 65.000, mean reward: 10.833 [ 0.000, 35.000], mean action: 2708.833 [1596.000, 5351.000],  loss: 21.630424, mae: 15.205383, mean_q: 31.600975\n",
            "  1915/30000: episode: 320, duration: 0.183s, episode steps:   6, steps per second:  33, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1851.333 [428.000, 2679.000],  loss: 28.681459, mae: 14.713862, mean_q: 31.425695\n",
            "  1921/30000: episode: 321, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3274.667 [969.000, 5616.000],  loss: 26.819284, mae: 13.841105, mean_q: 29.427757\n",
            "  1927/30000: episode: 322, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1523.667 [109.000, 4202.000],  loss: 33.668381, mae: 16.047293, mean_q: 33.081753\n",
            "  1933/30000: episode: 323, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2007.667 [72.000, 5448.000],  loss: 25.936821, mae: 13.991530, mean_q: 29.518503\n",
            "  1939/30000: episode: 324, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3814.167 [2483.000, 5283.000],  loss: 25.281410, mae: 13.916859, mean_q: 30.164810\n",
            "  1945/30000: episode: 325, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3907.833 [428.000, 4893.000],  loss: 26.098007, mae: 14.365251, mean_q: 30.641907\n",
            "  1951/30000: episode: 326, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 110.000, mean reward: 18.333 [ 0.000, 35.000], mean action: 2535.833 [238.000, 4185.000],  loss: 29.496817, mae: 15.849013, mean_q: 32.700302\n",
            "  1957/30000: episode: 327, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3563.167 [839.000, 5616.000],  loss: 46.134842, mae: 14.257263, mean_q: 30.385569\n",
            "  1963/30000: episode: 328, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4975.500 [1338.000, 5703.000],  loss: 36.628521, mae: 14.775112, mean_q: 31.148659\n",
            "  1969/30000: episode: 329, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3490.500 [961.000, 5405.000],  loss: 32.130013, mae: 14.485141, mean_q: 30.768900\n",
            "  1975/30000: episode: 330, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2720.333 [811.000, 4407.000],  loss: 35.360744, mae: 14.979728, mean_q: 31.479790\n",
            "  1981/30000: episode: 331, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2788.000 [730.000, 5486.000],  loss: 36.893131, mae: 13.878551, mean_q: 29.919128\n",
            "  1987/30000: episode: 332, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [-5.000,  5.000], mean action: 1619.000 [344.000, 2782.000],  loss: 35.007427, mae: 13.643697, mean_q: 29.632614\n",
            "  1993/30000: episode: 333, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2786.667 [100.000, 4613.000],  loss: 46.129078, mae: 13.869304, mean_q: 29.735880\n",
            "  1999/30000: episode: 334, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3079.000 [442.000, 5486.000],  loss: 34.009903, mae: 14.533900, mean_q: 30.496134\n",
            "  2005/30000: episode: 335, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 4158.000 [2409.000, 4893.000],  loss: 25.106148, mae: 14.737309, mean_q: 30.975534\n",
            "  2011/30000: episode: 336, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3521.833 [1227.000, 5717.000],  loss: 41.650642, mae: 14.849244, mean_q: 31.287149\n",
            "  2017/30000: episode: 337, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 50.000, mean reward:  8.333 [ 0.000, 35.000], mean action: 1875.333 [285.000, 5703.000],  loss: 23.693903, mae: 14.311549, mean_q: 29.989492\n",
            "  2023/30000: episode: 338, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [-5.000, 10.000], mean action: 2210.333 [961.000, 3475.000],  loss: 44.624615, mae: 15.309691, mean_q: 32.417747\n",
            "  2029/30000: episode: 339, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 3328.833 [634.000, 5679.000],  loss: 39.069878, mae: 14.399017, mean_q: 30.931892\n",
            "  2035/30000: episode: 340, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3352.667 [1338.000, 5289.000],  loss: 22.613625, mae: 14.625602, mean_q: 31.819878\n",
            "  2041/30000: episode: 341, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: 55.000, mean reward:  9.167 [ 0.000, 35.000], mean action: 2743.833 [1338.000, 4613.000],  loss: 29.337311, mae: 13.608867, mean_q: 30.023804\n",
            "  2047/30000: episode: 342, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3775.667 [1137.000, 5605.000],  loss: 43.883228, mae: 14.145455, mean_q: 31.072037\n",
            "  2053/30000: episode: 343, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [-5.000, 10.000], mean action: 2301.667 [1202.000, 4893.000],  loss: 38.111897, mae: 13.393143, mean_q: 29.519211\n",
            "  2059/30000: episode: 344, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1670.500 [156.000, 4893.000],  loss: 41.610023, mae: 16.304960, mean_q: 34.614346\n",
            "  2065/30000: episode: 345, duration: 0.260s, episode steps:   6, steps per second:  23, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2292.833 [700.000, 4646.000],  loss: 40.638409, mae: 15.319221, mean_q: 33.284904\n",
            "  2071/30000: episode: 346, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3569.000 [1338.000, 5620.000],  loss: 32.280193, mae: 14.880046, mean_q: 31.875746\n",
            "  2077/30000: episode: 347, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward: 65.000, mean reward: 10.833 [ 0.000, 25.000], mean action: 2868.333 [1202.000, 4954.000],  loss: 29.219757, mae: 14.723371, mean_q: 31.851305\n",
            "  2083/30000: episode: 348, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4942.000 [1137.000, 5703.000],  loss: 28.950836, mae: 13.654416, mean_q: 29.786606\n",
            "  2089/30000: episode: 349, duration: 0.263s, episode steps:   6, steps per second:  23, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3493.333 [1338.000, 4989.000],  loss: 33.445759, mae: 14.008296, mean_q: 30.065699\n",
            "  2095/30000: episode: 350, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 1304.500 [1137.000, 1338.000],  loss: 27.293848, mae: 15.058339, mean_q: 32.321659\n",
            "  2101/30000: episode: 351, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2515.333 [344.000, 4893.000],  loss: 42.128010, mae: 15.335948, mean_q: 32.480152\n",
            "  2107/30000: episode: 352, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2978.500 [156.000, 5717.000],  loss: 49.666294, mae: 15.905385, mean_q: 33.396488\n",
            "  2113/30000: episode: 353, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4001.000 [1338.000, 5218.000],  loss: 38.191235, mae: 14.931495, mean_q: 32.393208\n",
            "  2119/30000: episode: 354, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2871.333 [1128.000, 4464.000],  loss: 34.409275, mae: 14.690442, mean_q: 31.812605\n",
            "  2125/30000: episode: 355, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2658.333 [1128.000, 4407.000],  loss: 20.656904, mae: 14.380076, mean_q: 31.663130\n",
            "  2131/30000: episode: 356, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2085.833 [1338.000, 3638.000],  loss: 36.324360, mae: 14.208084, mean_q: 31.047195\n",
            "  2137/30000: episode: 357, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2698.833 [1634.000, 3985.000],  loss: 28.540724, mae: 15.137549, mean_q: 32.647236\n",
            "  2143/30000: episode: 358, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1773.167 [375.000, 5289.000],  loss: 23.253832, mae: 14.170478, mean_q: 30.728746\n",
            "  2149/30000: episode: 359, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3379.000 [1338.000, 4893.000],  loss: 28.465834, mae: 14.420177, mean_q: 31.808187\n",
            "  2155/30000: episode: 360, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3552.667 [1981.000, 4464.000],  loss: 20.340008, mae: 13.703494, mean_q: 29.728399\n",
            "  2161/30000: episode: 361, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [-5.000, 25.000], mean action: 2019.000 [1530.000, 4464.000],  loss: 28.299561, mae: 14.730004, mean_q: 31.780174\n",
            "  2167/30000: episode: 362, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 4556.167 [1338.000, 5554.000],  loss: 58.221603, mae: 14.697398, mean_q: 31.547045\n",
            "  2173/30000: episode: 363, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2411.500 [1021.000, 5495.000],  loss: 25.434607, mae: 15.564393, mean_q: 33.242828\n",
            "  2179/30000: episode: 364, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2967.833 [1128.000, 4893.000],  loss: 26.173531, mae: 14.595783, mean_q: 31.705969\n",
            "  2185/30000: episode: 365, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1849.167 [1338.000, 4405.000],  loss: 29.953699, mae: 14.097328, mean_q: 30.811302\n",
            "  2191/30000: episode: 366, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3084.833 [108.000, 5743.000],  loss: 45.565105, mae: 14.195970, mean_q: 30.351019\n",
            "  2197/30000: episode: 367, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2963.333 [1102.000, 4849.000],  loss: 27.442900, mae: 14.965054, mean_q: 31.720139\n",
            "  2203/30000: episode: 368, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 1713.833 [271.000, 4981.000],  loss: 22.011953, mae: 15.490102, mean_q: 32.417938\n",
            "  2209/30000: episode: 369, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4146.833 [3221.000, 5039.000],  loss: 44.513454, mae: 13.998996, mean_q: 30.641161\n",
            "  2215/30000: episode: 370, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3190.333 [1988.000, 5722.000],  loss: 27.440489, mae: 15.084633, mean_q: 31.991331\n",
            "  2221/30000: episode: 371, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 1338.000 [1338.000, 1338.000],  loss: 36.230103, mae: 16.138233, mean_q: 34.257732\n",
            "  2227/30000: episode: 372, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1463.833 [375.000, 4840.000],  loss: 45.731632, mae: 15.955890, mean_q: 33.897194\n",
            "  2233/30000: episode: 373, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 4022.667 [975.000, 5476.000],  loss: 26.726181, mae: 15.891463, mean_q: 34.402744\n",
            "  2239/30000: episode: 374, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2391.167 [245.000, 4464.000],  loss: 30.978577, mae: 14.649795, mean_q: 31.900656\n",
            "  2245/30000: episode: 375, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2394.500 [621.000, 4981.000],  loss: 53.342876, mae: 15.911926, mean_q: 33.973061\n",
            "  2251/30000: episode: 376, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: 85.000, mean reward: 14.167 [ 0.000, 30.000], mean action: 3807.333 [556.000, 5331.000],  loss: 30.704231, mae: 13.542728, mean_q: 29.561968\n",
            "  2257/30000: episode: 377, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2707.500 [1338.000, 4824.000],  loss: 27.313934, mae: 13.671123, mean_q: 29.915039\n",
            "  2263/30000: episode: 378, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3404.667 [732.000, 5289.000],  loss: 38.125164, mae: 15.117310, mean_q: 32.140285\n",
            "  2269/30000: episode: 379, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 1980.667 [156.000, 4464.000],  loss: 34.889343, mae: 14.933446, mean_q: 31.547033\n",
            "  2275/30000: episode: 380, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3304.667 [328.000, 5386.000],  loss: 28.453409, mae: 14.669709, mean_q: 31.221956\n",
            "  2281/30000: episode: 381, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4135.167 [1338.000, 5722.000],  loss: 33.219910, mae: 15.179958, mean_q: 31.960363\n",
            "  2287/30000: episode: 382, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 40.000, mean reward:  6.667 [-5.000, 40.000], mean action: 2232.333 [156.000, 4405.000],  loss: 32.356834, mae: 15.064602, mean_q: 31.447594\n",
            "  2293/30000: episode: 383, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3408.333 [1320.000, 5057.000],  loss: 27.030634, mae: 13.879375, mean_q: 29.331421\n",
            "  2299/30000: episode: 384, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 2314.167 [518.000, 4380.000],  loss: 68.452919, mae: 13.900634, mean_q: 29.467676\n",
            "  2305/30000: episode: 385, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4162.500 [2620.000, 4499.000],  loss: 39.582973, mae: 14.475991, mean_q: 30.782862\n",
            "  2311/30000: episode: 386, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3589.667 [1952.000, 5722.000],  loss: 38.165421, mae: 15.043218, mean_q: 31.803541\n",
            "  2317/30000: episode: 387, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2439.833 [273.000, 5163.000],  loss: 36.028671, mae: 16.494455, mean_q: 34.615021\n",
            "  2323/30000: episode: 388, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [-5.000, 25.000], mean action: 3853.333 [375.000, 5072.000],  loss: 29.766485, mae: 15.346726, mean_q: 32.166477\n",
            "  2329/30000: episode: 389, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 2890.167 [907.000, 5072.000],  loss: 34.587032, mae: 14.458992, mean_q: 30.704626\n",
            "  2335/30000: episode: 390, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1516.000 [375.000, 2421.000],  loss: 32.849369, mae: 15.204711, mean_q: 32.175034\n",
            "  2341/30000: episode: 391, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 115.000, mean reward: 19.167 [ 0.000, 35.000], mean action: 2476.167 [238.000, 4185.000],  loss: 33.531780, mae: 14.128776, mean_q: 30.163401\n",
            "  2347/30000: episode: 392, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000,  5.000], mean action: 2843.167 [733.000, 5072.000],  loss: 35.549992, mae: 15.244029, mean_q: 32.334312\n",
            "  2353/30000: episode: 393, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3263.667 [2323.000, 4251.000],  loss: 33.681019, mae: 13.962823, mean_q: 29.674723\n",
            "  2359/30000: episode: 394, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 3601.667 [375.000, 5072.000],  loss: 30.641724, mae: 14.924549, mean_q: 31.862795\n",
            "  2365/30000: episode: 395, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 90.000, mean reward: 15.000 [-5.000, 40.000], mean action: 3131.500 [1191.000, 5072.000],  loss: 54.045933, mae: 14.759568, mean_q: 30.846069\n",
            "  2371/30000: episode: 396, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3583.667 [375.000, 5072.000],  loss: 28.602440, mae: 14.758563, mean_q: 31.090874\n",
            "  2377/30000: episode: 397, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4362.500 [2323.000, 5072.000],  loss: 32.839657, mae: 14.741807, mean_q: 31.049120\n",
            "  2383/30000: episode: 398, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 30.000, mean reward:  5.000 [-5.000, 20.000], mean action: 3068.667 [1202.000, 5072.000],  loss: 36.517609, mae: 15.653958, mean_q: 32.682148\n",
            "  2389/30000: episode: 399, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [-5.000, 25.000], mean action: 3950.833 [1021.000, 5072.000],  loss: 21.941267, mae: 14.830914, mean_q: 31.083246\n",
            "  2395/30000: episode: 400, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3759.000 [2323.000, 4695.000],  loss: 54.125820, mae: 16.148890, mean_q: 33.580368\n",
            "  2401/30000: episode: 401, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4365.667 [1052.000, 5706.000],  loss: 32.378193, mae: 14.994998, mean_q: 31.420397\n",
            "  2407/30000: episode: 402, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3149.000 [375.000, 4981.000],  loss: 32.292934, mae: 14.254139, mean_q: 30.408340\n",
            "  2413/30000: episode: 403, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2471.500 [375.000, 5305.000],  loss: 28.536484, mae: 15.249063, mean_q: 32.087055\n",
            "  2419/30000: episode: 404, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2804.667 [895.000, 4695.000],  loss: 27.249239, mae: 15.789884, mean_q: 33.638683\n",
            "  2425/30000: episode: 405, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3726.500 [1191.000, 5663.000],  loss: 35.389488, mae: 15.454043, mean_q: 32.936520\n",
            "  2431/30000: episode: 406, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1508.500 [4.000, 3091.000],  loss: 31.700640, mae: 16.001810, mean_q: 33.726437\n",
            "  2437/30000: episode: 407, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3383.167 [1245.000, 5626.000],  loss: 23.461998, mae: 14.990699, mean_q: 32.061115\n",
            "  2443/30000: episode: 408, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3538.167 [1269.000, 5579.000],  loss: 34.071964, mae: 15.181973, mean_q: 32.669483\n",
            "  2449/30000: episode: 409, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3444.667 [741.000, 5197.000],  loss: 30.804953, mae: 13.350692, mean_q: 28.837118\n",
            "  2455/30000: episode: 410, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4104.667 [1305.000, 5656.000],  loss: 40.930573, mae: 14.595601, mean_q: 30.568979\n",
            "  2461/30000: episode: 411, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 3067.833 [132.000, 5204.000],  loss: 44.996479, mae: 13.497737, mean_q: 29.209755\n",
            "  2467/30000: episode: 412, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2053.667 [382.000, 5426.000],  loss: 28.917654, mae: 14.247731, mean_q: 30.427940\n",
            "  2473/30000: episode: 413, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [-5.000, 15.000], mean action: 2189.333 [1257.000, 3221.000],  loss: 55.870758, mae: 14.543362, mean_q: 30.887609\n",
            "  2479/30000: episode: 414, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2942.000 [1473.000, 5715.000],  loss: 26.742361, mae: 14.625882, mean_q: 31.510767\n",
            "  2485/30000: episode: 415, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3323.500 [297.000, 5715.000],  loss: 35.946793, mae: 14.490173, mean_q: 30.993460\n",
            "  2491/30000: episode: 416, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3171.500 [375.000, 5707.000],  loss: 40.320148, mae: 14.899986, mean_q: 31.831444\n",
            "  2497/30000: episode: 417, duration: 0.241s, episode steps:   6, steps per second:  25, episode reward: 40.000, mean reward:  6.667 [ 0.000, 20.000], mean action: 2600.000 [1596.000, 3252.000],  loss: 29.741289, mae: 14.963780, mean_q: 32.223351\n",
            "  2503/30000: episode: 418, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2775.000 [295.000, 5146.000],  loss: 28.426781, mae: 14.267274, mean_q: 30.347557\n",
            "  2509/30000: episode: 419, duration: 0.233s, episode steps:   6, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3787.167 [2240.000, 5153.000],  loss: 33.605381, mae: 15.100253, mean_q: 31.908798\n",
            "  2515/30000: episode: 420, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3194.833 [2669.000, 4981.000],  loss: 65.056053, mae: 15.148368, mean_q: 32.302044\n",
            "  2521/30000: episode: 421, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3366.000 [1132.000, 5715.000],  loss: 23.173937, mae: 12.822978, mean_q: 28.259878\n",
            "  2527/30000: episode: 422, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward: 35.000, mean reward:  5.833 [ 0.000, 25.000], mean action: 3015.167 [1190.000, 5299.000],  loss: 23.739304, mae: 14.554592, mean_q: 30.858877\n",
            "  2533/30000: episode: 423, duration: 0.245s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1839.333 [150.000, 3679.000],  loss: 23.216066, mae: 14.844733, mean_q: 31.280294\n",
            "  2539/30000: episode: 424, duration: 0.262s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3696.833 [2240.000, 5057.000],  loss: 33.067509, mae: 15.017968, mean_q: 31.974253\n",
            "  2545/30000: episode: 425, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3003.333 [1530.000, 4193.000],  loss: 46.910248, mae: 15.809571, mean_q: 33.100040\n",
            "  2551/30000: episode: 426, duration: 0.257s, episode steps:   6, steps per second:  23, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 1909.167 [587.000, 3613.000],  loss: 35.803547, mae: 15.533062, mean_q: 32.306667\n",
            "  2557/30000: episode: 427, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward: 85.000, mean reward: 14.167 [ 0.000, 35.000], mean action: 2681.500 [578.000, 4185.000],  loss: 31.194412, mae: 15.814435, mean_q: 33.157139\n",
            "  2563/30000: episode: 428, duration: 0.186s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2808.000 [961.000, 5466.000],  loss: 29.959196, mae: 15.347886, mean_q: 32.284657\n",
            "  2569/30000: episode: 429, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2126.333 [562.000, 4493.000],  loss: 41.729954, mae: 15.164860, mean_q: 32.315533\n",
            "  2575/30000: episode: 430, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000,  5.000], mean action: 2511.833 [1223.000, 4900.000],  loss: 34.679852, mae: 14.555672, mean_q: 30.710556\n",
            "  2581/30000: episode: 431, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3317.000 [1634.000, 5296.000],  loss: 29.195704, mae: 14.775289, mean_q: 31.267790\n",
            "  2587/30000: episode: 432, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward: 30.000, mean reward:  5.000 [-5.000, 20.000], mean action: 3203.000 [1202.000, 4460.000],  loss: 43.013584, mae: 14.694363, mean_q: 31.333351\n",
            "  2593/30000: episode: 433, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2911.167 [375.000, 5259.000],  loss: 38.865326, mae: 15.249237, mean_q: 31.907263\n",
            "  2599/30000: episode: 434, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3352.333 [961.000, 5715.000],  loss: 24.338144, mae: 14.947254, mean_q: 31.413445\n",
            "  2605/30000: episode: 435, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 1811.667 [53.000, 4239.000],  loss: 27.562094, mae: 13.880748, mean_q: 29.209372\n",
            "  2611/30000: episode: 436, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1496.667 [240.000, 3484.000],  loss: 27.866312, mae: 14.728684, mean_q: 31.482681\n",
            "  2617/30000: episode: 437, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2446.000 [351.000, 4981.000],  loss: 32.909298, mae: 14.973264, mean_q: 31.491968\n",
            "  2623/30000: episode: 438, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3854.333 [216.000, 5531.000],  loss: 33.005276, mae: 15.062905, mean_q: 31.680902\n",
            "  2629/30000: episode: 439, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3417.333 [961.000, 5726.000],  loss: 37.515697, mae: 14.786633, mean_q: 31.573706\n",
            "  2635/30000: episode: 440, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3649.333 [2703.000, 4874.000],  loss: 45.392406, mae: 15.217948, mean_q: 31.960640\n",
            "  2641/30000: episode: 441, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2657.500 [375.000, 4826.000],  loss: 27.149504, mae: 14.934588, mean_q: 31.753752\n",
            "  2647/30000: episode: 442, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3400.667 [1498.000, 5270.000],  loss: 26.277771, mae: 14.712165, mean_q: 31.813904\n",
            "  2653/30000: episode: 443, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 2899.167 [2364.000, 3587.000],  loss: 49.918842, mae: 13.766709, mean_q: 29.726883\n",
            "  2659/30000: episode: 444, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3069.500 [1596.000, 5148.000],  loss: 21.847496, mae: 14.804291, mean_q: 31.444845\n",
            "  2665/30000: episode: 445, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2422.000 [375.000, 4185.000],  loss: 55.514874, mae: 15.072212, mean_q: 31.916204\n",
            "  2671/30000: episode: 446, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [-5.000, 40.000], mean action: 2903.667 [1596.000, 3343.000],  loss: 30.582872, mae: 15.032559, mean_q: 31.827188\n",
            "  2677/30000: episode: 447, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2464.667 [375.000, 3683.000],  loss: 34.635841, mae: 15.355582, mean_q: 32.945175\n",
            "  2683/30000: episode: 448, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3652.833 [2558.000, 5027.000],  loss: 33.041515, mae: 13.876096, mean_q: 30.391403\n",
            "  2689/30000: episode: 449, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4108.167 [1429.000, 5715.000],  loss: 48.045902, mae: 14.739079, mean_q: 31.553030\n",
            "  2695/30000: episode: 450, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2747.833 [1042.000, 3828.000],  loss: 43.344070, mae: 15.397672, mean_q: 33.298939\n",
            "  2701/30000: episode: 451, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2969.833 [242.000, 5707.000],  loss: 44.641445, mae: 15.231441, mean_q: 32.712830\n",
            "  2707/30000: episode: 452, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2209.000 [1014.000, 4464.000],  loss: 36.895451, mae: 14.701501, mean_q: 31.883987\n",
            "  2713/30000: episode: 453, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2635.167 [156.000, 5273.000],  loss: 42.065361, mae: 15.057366, mean_q: 32.541721\n",
            "  2719/30000: episode: 454, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3385.000 [162.000, 5703.000],  loss: 28.368315, mae: 14.978310, mean_q: 31.785353\n",
            "  2725/30000: episode: 455, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 3104.167 [1596.000, 5707.000],  loss: 40.438541, mae: 14.775398, mean_q: 31.749140\n",
            "  2731/30000: episode: 456, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 2411.000 [140.000, 3951.000],  loss: 34.912052, mae: 14.701531, mean_q: 31.096273\n",
            "  2737/30000: episode: 457, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 35.000, mean reward:  5.833 [ 0.000, 35.000], mean action: 2924.500 [791.000, 3921.000],  loss: 32.362812, mae: 14.569932, mean_q: 31.213484\n",
            "  2743/30000: episode: 458, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3346.333 [866.000, 4695.000],  loss: 33.207047, mae: 15.360680, mean_q: 32.625809\n",
            "  2749/30000: episode: 459, duration: 0.179s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1800.167 [156.000, 4695.000],  loss: 29.116257, mae: 15.248210, mean_q: 32.727589\n",
            "  2755/30000: episode: 460, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 80.000, mean reward: 13.333 [ 0.000, 25.000], mean action: 2282.167 [1202.000, 3921.000],  loss: 34.963696, mae: 15.590656, mean_q: 33.379765\n",
            "  2761/30000: episode: 461, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3958.500 [1035.000, 5496.000],  loss: 37.233921, mae: 14.918246, mean_q: 31.852121\n",
            "  2767/30000: episode: 462, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 1876.833 [374.000, 3921.000],  loss: 45.968506, mae: 14.269824, mean_q: 30.835154\n",
            "  2773/30000: episode: 463, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3054.167 [523.000, 4981.000],  loss: 23.990013, mae: 14.480385, mean_q: 31.211233\n",
            "  2779/30000: episode: 464, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 40.000, mean reward:  6.667 [ 0.000, 20.000], mean action: 3439.500 [1355.000, 4981.000],  loss: 20.638716, mae: 14.065446, mean_q: 30.846636\n",
            "  2785/30000: episode: 465, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: 45.000, mean reward:  7.500 [ 0.000, 30.000], mean action: 4041.833 [1596.000, 5077.000],  loss: 22.493364, mae: 14.967343, mean_q: 31.952604\n",
            "  2791/30000: episode: 466, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3188.333 [2003.000, 4981.000],  loss: 35.762035, mae: 14.908996, mean_q: 31.784698\n",
            "  2797/30000: episode: 467, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2514.500 [132.000, 5034.000],  loss: 40.004765, mae: 14.890668, mean_q: 31.708715\n",
            "  2803/30000: episode: 468, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2396.667 [493.000, 4981.000],  loss: 33.096195, mae: 15.461413, mean_q: 32.680664\n",
            "  2809/30000: episode: 469, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3952.667 [2365.000, 4749.000],  loss: 24.348534, mae: 14.279739, mean_q: 30.547373\n",
            "  2815/30000: episode: 470, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3074.667 [1097.000, 5616.000],  loss: 33.605042, mae: 14.664033, mean_q: 30.954901\n",
            "  2821/30000: episode: 471, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 40.000, mean reward:  6.667 [ 0.000, 35.000], mean action: 2463.000 [156.000, 5579.000],  loss: 58.806271, mae: 15.068893, mean_q: 31.611975\n",
            "  2827/30000: episode: 472, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3128.000 [316.000, 4397.000],  loss: 26.161102, mae: 15.501282, mean_q: 33.421703\n",
            "  2833/30000: episode: 473, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2712.000 [1202.000, 4954.000],  loss: 23.078840, mae: 15.062997, mean_q: 32.152729\n",
            "  2839/30000: episode: 474, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3143.000 [1305.000, 4588.000],  loss: 29.664305, mae: 14.527762, mean_q: 31.712034\n",
            "  2845/30000: episode: 475, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2708.167 [255.000, 4832.000],  loss: 25.157442, mae: 13.280472, mean_q: 29.231148\n",
            "  2851/30000: episode: 476, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2057.667 [1736.000, 2844.000],  loss: 37.013031, mae: 14.951530, mean_q: 32.338650\n",
            "  2857/30000: episode: 477, duration: 0.183s, episode steps:   6, steps per second:  33, episode reward: -5.000, mean reward: -0.833 [-5.000,  5.000], mean action: 2100.333 [700.000, 3147.000],  loss: 29.946318, mae: 15.897275, mean_q: 33.953648\n",
            "  2863/30000: episode: 478, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3214.000 [1021.000, 5679.000],  loss: 30.993874, mae: 14.514739, mean_q: 31.273691\n",
            "  2869/30000: episode: 479, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [-5.000, 15.000], mean action: 2816.667 [662.000, 4616.000],  loss: 73.774391, mae: 14.128144, mean_q: 30.384371\n",
            "  2875/30000: episode: 480, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2731.000 [423.000, 5154.000],  loss: 40.051743, mae: 14.980312, mean_q: 32.401951\n",
            "  2881/30000: episode: 481, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2545.667 [1596.000, 3839.000],  loss: 52.338848, mae: 14.432465, mean_q: 30.682549\n",
            "  2887/30000: episode: 482, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2269.000 [669.000, 4185.000],  loss: 29.650553, mae: 13.656674, mean_q: 29.570135\n",
            "  2893/30000: episode: 483, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1862.000 [751.000, 3160.000],  loss: 32.192745, mae: 13.445256, mean_q: 29.242592\n",
            "  2899/30000: episode: 484, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3136.333 [1297.000, 5579.000],  loss: 27.733437, mae: 14.006917, mean_q: 30.078163\n",
            "  2905/30000: episode: 485, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3925.833 [741.000, 5544.000],  loss: 41.508717, mae: 15.228199, mean_q: 32.440296\n",
            "  2911/30000: episode: 486, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3261.833 [2193.000, 4584.000],  loss: 31.519476, mae: 15.125777, mean_q: 32.117321\n",
            "  2917/30000: episode: 487, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2558.167 [238.000, 4940.000],  loss: 22.048037, mae: 14.604915, mean_q: 31.676336\n",
            "  2923/30000: episode: 488, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3957.667 [1015.000, 5544.000],  loss: 46.325954, mae: 15.013358, mean_q: 31.894348\n",
            "  2929/30000: episode: 489, duration: 0.262s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  5.000], mean action: 4255.333 [2277.000, 5466.000],  loss: 27.252966, mae: 14.412892, mean_q: 31.391893\n",
            "  2935/30000: episode: 490, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3133.000 [245.000, 5715.000],  loss: 42.189972, mae: 14.486667, mean_q: 30.578629\n",
            "  2941/30000: episode: 491, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 3279.667 [363.000, 5148.000],  loss: 27.906563, mae: 13.325756, mean_q: 29.471085\n",
            "  2947/30000: episode: 492, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3026.167 [295.000, 4711.000],  loss: 32.230808, mae: 14.882220, mean_q: 31.435492\n",
            "  2953/30000: episode: 493, duration: 0.266s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2435.500 [741.000, 5146.000],  loss: 53.064037, mae: 13.904819, mean_q: 29.813623\n",
            "  2959/30000: episode: 494, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3852.333 [2967.000, 4809.000],  loss: 26.215233, mae: 14.688518, mean_q: 31.385263\n",
            "  2965/30000: episode: 495, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2906.333 [197.000, 4695.000],  loss: 28.973282, mae: 13.555332, mean_q: 29.399416\n",
            "  2971/30000: episode: 496, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4422.167 [489.000, 5679.000],  loss: 29.654112, mae: 13.983493, mean_q: 29.955458\n",
            "  2977/30000: episode: 497, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2632.667 [709.000, 5597.000],  loss: 23.073053, mae: 13.951941, mean_q: 29.828661\n",
            "  2983/30000: episode: 498, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4173.500 [1632.000, 5466.000],  loss: 36.486111, mae: 14.513554, mean_q: 30.736969\n",
            "  2989/30000: episode: 499, duration: 0.209s, episode steps:   6, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2799.500 [105.000, 5522.000],  loss: 38.691288, mae: 15.294284, mean_q: 32.821220\n",
            "  2995/30000: episode: 500, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1544.667 [14.000, 4229.000],  loss: 38.232872, mae: 14.946751, mean_q: 31.976271\n",
            "  3001/30000: episode: 501, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3671.167 [356.000, 5466.000],  loss: 34.673626, mae: 13.832390, mean_q: 29.701029\n",
            "  3007/30000: episode: 502, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2947.333 [316.000, 4696.000],  loss: 26.584394, mae: 15.480649, mean_q: 32.551552\n",
            "  3013/30000: episode: 503, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3239.167 [156.000, 5735.000],  loss: 30.826242, mae: 15.182797, mean_q: 32.294834\n",
            "  3019/30000: episode: 504, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3984.000 [1530.000, 5549.000],  loss: 28.180359, mae: 15.014630, mean_q: 32.251019\n",
            "  3025/30000: episode: 505, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4581.000 [4185.000, 5133.000],  loss: 35.420803, mae: 15.128506, mean_q: 32.604656\n",
            "  3031/30000: episode: 506, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3001.333 [1223.000, 5369.000],  loss: 31.296865, mae: 14.130218, mean_q: 30.166824\n",
            "  3037/30000: episode: 507, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [-5.000, 25.000], mean action: 2492.500 [602.000, 4212.000],  loss: 35.915222, mae: 14.074246, mean_q: 30.496552\n",
            "  3043/30000: episode: 508, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2516.667 [576.000, 5579.000],  loss: 39.639935, mae: 13.693803, mean_q: 30.085108\n",
            "  3049/30000: episode: 509, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3318.500 [709.000, 5007.000],  loss: 32.151569, mae: 13.019786, mean_q: 29.053808\n",
            "  3055/30000: episode: 510, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3091.833 [709.000, 5706.000],  loss: 30.635561, mae: 13.627609, mean_q: 30.283144\n",
            "  3061/30000: episode: 511, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 3414.667 [2076.000, 4749.000],  loss: 24.697678, mae: 14.789290, mean_q: 32.322941\n",
            "  3067/30000: episode: 512, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3616.833 [2670.000, 5715.000],  loss: 25.907011, mae: 13.557339, mean_q: 30.052429\n",
            "  3073/30000: episode: 513, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2591.000 [1530.000, 4159.000],  loss: 33.864075, mae: 14.645199, mean_q: 31.693817\n",
            "  3079/30000: episode: 514, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 10.000], mean action: 2465.667 [709.000, 4123.000],  loss: 50.574787, mae: 14.622440, mean_q: 32.028252\n",
            "  3085/30000: episode: 515, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 100.000, mean reward: 16.667 [ 0.000, 35.000], mean action: 3170.833 [709.000, 5148.000],  loss: 23.172663, mae: 14.430739, mean_q: 31.842638\n",
            "  3091/30000: episode: 516, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1998.667 [400.000, 4552.000],  loss: 31.397314, mae: 14.627839, mean_q: 31.853661\n",
            "  3097/30000: episode: 517, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1779.000 [43.000, 5708.000],  loss: 34.147335, mae: 15.630301, mean_q: 33.738770\n",
            "  3103/30000: episode: 518, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 709.000 [709.000, 709.000],  loss: 24.405973, mae: 14.722115, mean_q: 32.039295\n",
            "  3109/30000: episode: 519, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1619.667 [53.000, 5143.000],  loss: 25.217390, mae: 14.956806, mean_q: 32.138092\n",
            "  3115/30000: episode: 520, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3238.167 [709.000, 5597.000],  loss: 34.033001, mae: 15.509312, mean_q: 33.532890\n",
            "  3121/30000: episode: 521, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2385.667 [709.000, 5717.000],  loss: 32.475182, mae: 15.055659, mean_q: 32.197262\n",
            "  3127/30000: episode: 522, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 3028.000 [709.000, 5717.000],  loss: 33.073402, mae: 13.677677, mean_q: 29.701822\n",
            "  3133/30000: episode: 523, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3538.667 [2624.000, 5401.000],  loss: 37.554501, mae: 14.254661, mean_q: 30.472799\n",
            "  3139/30000: episode: 524, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2133.000 [642.000, 3611.000],  loss: 36.620396, mae: 15.114894, mean_q: 31.790777\n",
            "  3145/30000: episode: 525, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2463.500 [461.000, 5579.000],  loss: 23.706675, mae: 14.912940, mean_q: 31.444283\n",
            "  3151/30000: episode: 526, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3101.000 [89.000, 5000.000],  loss: 29.630722, mae: 15.906555, mean_q: 33.383007\n",
            "  3157/30000: episode: 527, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3158.833 [1941.000, 4719.000],  loss: 38.195778, mae: 14.743962, mean_q: 31.304132\n",
            "  3163/30000: episode: 528, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2452.833 [803.000, 5629.000],  loss: 29.729357, mae: 13.685092, mean_q: 29.596281\n",
            "  3169/30000: episode: 529, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 80.000, mean reward: 13.333 [ 0.000, 35.000], mean action: 1868.667 [53.000, 5154.000],  loss: 34.207794, mae: 14.110454, mean_q: 30.062119\n",
            "  3175/30000: episode: 530, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2768.667 [1152.000, 4439.000],  loss: 21.665222, mae: 14.081288, mean_q: 29.691071\n",
            "  3181/30000: episode: 531, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3007.167 [427.000, 4901.000],  loss: 40.983219, mae: 14.257033, mean_q: 30.534883\n",
            "  3187/30000: episode: 532, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2884.500 [1567.000, 4599.000],  loss: 28.063911, mae: 15.225574, mean_q: 31.833799\n",
            "  3193/30000: episode: 533, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2034.667 [140.000, 4630.000],  loss: 34.144558, mae: 15.180346, mean_q: 31.992468\n",
            "  3199/30000: episode: 534, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3244.833 [105.000, 5715.000],  loss: 35.753555, mae: 14.329839, mean_q: 30.884249\n",
            "  3205/30000: episode: 535, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3111.667 [1206.000, 5289.000],  loss: 37.579208, mae: 14.761497, mean_q: 31.761797\n",
            "  3211/30000: episode: 536, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2259.667 [103.000, 5608.000],  loss: 39.849499, mae: 15.336556, mean_q: 32.503086\n",
            "  3217/30000: episode: 537, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 30.000, mean reward:  5.000 [ 0.000, 20.000], mean action: 2808.833 [1828.000, 3921.000],  loss: 28.343847, mae: 14.964362, mean_q: 31.653147\n",
            "  3223/30000: episode: 538, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3289.000 [1632.000, 5629.000],  loss: 19.710714, mae: 15.367919, mean_q: 32.448013\n",
            "  3229/30000: episode: 539, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2264.833 [328.000, 5626.000],  loss: 38.362247, mae: 13.686836, mean_q: 29.733627\n",
            "  3235/30000: episode: 540, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1448.500 [53.000, 5619.000],  loss: 32.773323, mae: 14.898889, mean_q: 31.798052\n",
            "  3241/30000: episode: 541, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3320.667 [2052.000, 4937.000],  loss: 40.185284, mae: 14.405067, mean_q: 31.009722\n",
            "  3247/30000: episode: 542, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 1448.333 [53.000, 3230.000],  loss: 44.942154, mae: 14.882577, mean_q: 31.694677\n",
            "  3253/30000: episode: 543, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2611.500 [461.000, 5747.000],  loss: 45.438766, mae: 14.560922, mean_q: 31.404154\n",
            "  3259/30000: episode: 544, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2468.333 [107.000, 4191.000],  loss: 25.222168, mae: 14.968280, mean_q: 31.977655\n",
            "  3265/30000: episode: 545, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3010.833 [571.000, 5351.000],  loss: 28.131338, mae: 14.241374, mean_q: 30.930206\n",
            "  3271/30000: episode: 546, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3482.667 [990.000, 5579.000],  loss: 50.324329, mae: 13.640437, mean_q: 29.889702\n",
            "  3277/30000: episode: 547, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 4134.833 [1785.000, 5717.000],  loss: 14.886749, mae: 12.877471, mean_q: 28.082956\n",
            "  3283/30000: episode: 548, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4161.500 [2411.000, 5629.000],  loss: 31.903631, mae: 15.318435, mean_q: 32.620361\n",
            "  3289/30000: episode: 549, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2561.167 [1190.000, 4438.000],  loss: 33.075886, mae: 14.624564, mean_q: 31.741659\n",
            "  3295/30000: episode: 550, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2142.000 [826.000, 3996.000],  loss: 33.389050, mae: 14.342094, mean_q: 30.623291\n",
            "  3301/30000: episode: 551, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 1736.333 [0.000, 5061.000],  loss: 35.243320, mae: 14.562696, mean_q: 30.831442\n",
            "  3307/30000: episode: 552, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2492.833 [0.000, 4808.000],  loss: 42.988739, mae: 14.930644, mean_q: 32.083942\n",
            "  3313/30000: episode: 553, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2735.500 [1596.000, 4588.000],  loss: 28.637886, mae: 15.484782, mean_q: 33.198582\n",
            "  3319/30000: episode: 554, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2048.833 [53.000, 5629.000],  loss: 54.006271, mae: 15.271271, mean_q: 32.290909\n",
            "  3325/30000: episode: 555, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3159.833 [71.000, 5579.000],  loss: 29.107172, mae: 15.726954, mean_q: 33.549961\n",
            "  3331/30000: episode: 556, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 35.000, mean reward:  5.833 [ 0.000, 15.000], mean action: 1913.167 [1094.000, 5057.000],  loss: 21.726730, mae: 15.133354, mean_q: 32.205021\n",
            "  3337/30000: episode: 557, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2569.500 [826.000, 5156.000],  loss: 31.583963, mae: 15.239491, mean_q: 32.009502\n",
            "  3343/30000: episode: 558, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3027.333 [2609.000, 4763.000],  loss: 26.945032, mae: 15.246776, mean_q: 32.412594\n",
            "  3349/30000: episode: 559, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [-5.000, 30.000], mean action: 2761.167 [282.000, 5629.000],  loss: 39.062656, mae: 15.193298, mean_q: 32.834042\n",
            "  3355/30000: episode: 560, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 2145.500 [1596.000, 3613.000],  loss: 40.558556, mae: 13.772220, mean_q: 29.828543\n",
            "  3361/30000: episode: 561, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3600.333 [1132.000, 5579.000],  loss: 37.739716, mae: 14.362660, mean_q: 31.098734\n",
            "  3367/30000: episode: 562, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 3129.833 [101.000, 4616.000],  loss: 35.052605, mae: 14.730485, mean_q: 31.699358\n",
            "  3373/30000: episode: 563, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3753.000 [442.000, 5467.000],  loss: 25.990067, mae: 14.767119, mean_q: 31.521187\n",
            "  3379/30000: episode: 564, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2232.333 [461.000, 5255.000],  loss: 22.358932, mae: 14.166444, mean_q: 30.592743\n",
            "  3385/30000: episode: 565, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3617.667 [62.000, 5579.000],  loss: 46.531734, mae: 14.013745, mean_q: 30.285902\n",
            "  3391/30000: episode: 566, duration: 0.238s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2796.333 [53.000, 5656.000],  loss: 37.404449, mae: 14.855494, mean_q: 31.871115\n",
            "  3397/30000: episode: 567, duration: 0.233s, episode steps:   6, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3038.333 [53.000, 5467.000],  loss: 23.983629, mae: 15.650737, mean_q: 33.123692\n",
            "  3403/30000: episode: 568, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward: 50.000, mean reward:  8.333 [ 0.000, 15.000], mean action: 2413.333 [363.000, 5703.000],  loss: 30.177496, mae: 14.236609, mean_q: 30.590563\n",
            "  3409/30000: episode: 569, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3716.167 [150.000, 5392.000],  loss: 30.775177, mae: 14.244315, mean_q: 30.220709\n",
            "  3415/30000: episode: 570, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3123.667 [1694.000, 5640.000],  loss: 22.844444, mae: 14.803241, mean_q: 31.051493\n",
            "  3421/30000: episode: 571, duration: 0.233s, episode steps:   6, steps per second:  26, episode reward: 30.000, mean reward:  5.000 [ 0.000, 30.000], mean action: 2090.833 [751.000, 3951.000],  loss: 43.339874, mae: 14.173351, mean_q: 30.161118\n",
            "  3427/30000: episode: 572, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2720.167 [148.000, 3850.000],  loss: 24.922699, mae: 14.782033, mean_q: 31.572054\n",
            "  3433/30000: episode: 573, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3860.333 [385.000, 5364.000],  loss: 25.903534, mae: 14.315814, mean_q: 30.625961\n",
            "  3439/30000: episode: 574, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2453.000 [101.000, 4894.000],  loss: 33.213314, mae: 14.403163, mean_q: 31.174498\n",
            "  3445/30000: episode: 575, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2463.667 [101.000, 5482.000],  loss: 27.260290, mae: 15.146626, mean_q: 32.395260\n",
            "  3451/30000: episode: 576, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [-5.000, 10.000], mean action: 1155.000 [101.000, 2463.000],  loss: 36.682014, mae: 16.166849, mean_q: 34.401474\n",
            "  3457/30000: episode: 577, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3289.667 [1103.000, 5386.000],  loss: 27.048468, mae: 14.569823, mean_q: 31.178223\n",
            "  3463/30000: episode: 578, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2096.833 [356.000, 3484.000],  loss: 64.003334, mae: 16.015570, mean_q: 33.492710\n",
            "  3469/30000: episode: 579, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2608.500 [826.000, 4717.000],  loss: 28.049225, mae: 14.812401, mean_q: 31.608627\n",
            "  3475/30000: episode: 580, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2545.167 [751.000, 5506.000],  loss: 29.236366, mae: 13.617966, mean_q: 29.481833\n",
            "  3481/30000: episode: 581, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4151.333 [2679.000, 5154.000],  loss: 37.344742, mae: 14.676163, mean_q: 31.336777\n",
            "  3487/30000: episode: 582, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3291.333 [2349.000, 4763.000],  loss: 44.213699, mae: 14.258995, mean_q: 30.505148\n",
            "  3493/30000: episode: 583, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 1983.167 [436.000, 4660.000],  loss: 28.291491, mae: 13.603602, mean_q: 29.393478\n",
            "  3499/30000: episode: 584, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3910.000 [1358.000, 5395.000],  loss: 19.727303, mae: 14.038067, mean_q: 29.908127\n",
            "  3505/30000: episode: 585, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3288.000 [442.000, 5249.000],  loss: 36.032890, mae: 15.481761, mean_q: 33.112930\n",
            "  3511/30000: episode: 586, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3039.333 [1266.000, 4749.000],  loss: 48.712811, mae: 14.744075, mean_q: 31.422113\n",
            "  3517/30000: episode: 587, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2323.167 [889.000, 4212.000],  loss: 43.239929, mae: 15.258481, mean_q: 32.321621\n",
            "  3523/30000: episode: 588, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3228.167 [1772.000, 4939.000],  loss: 25.198214, mae: 14.345023, mean_q: 30.502518\n",
            "  3529/30000: episode: 589, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3544.667 [461.000, 5579.000],  loss: 26.988562, mae: 14.697619, mean_q: 31.288908\n",
            "  3535/30000: episode: 590, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2419.667 [461.000, 5496.000],  loss: 39.911617, mae: 14.380418, mean_q: 31.197416\n",
            "  3541/30000: episode: 591, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1751.000 [461.000, 3101.000],  loss: 34.193829, mae: 14.180524, mean_q: 31.108324\n",
            "  3547/30000: episode: 592, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1783.333 [442.000, 3712.000],  loss: 25.379614, mae: 14.906769, mean_q: 32.110592\n",
            "  3553/30000: episode: 593, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 1511.667 [461.000, 3613.000],  loss: 26.200201, mae: 14.180809, mean_q: 31.066902\n",
            "  3559/30000: episode: 594, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2221.000 [461.000, 4486.000],  loss: 34.056423, mae: 15.501460, mean_q: 32.996738\n",
            "  3565/30000: episode: 595, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 1833.167 [363.000, 4809.000],  loss: 22.963440, mae: 14.099648, mean_q: 30.258112\n",
            "  3571/30000: episode: 596, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2312.167 [788.000, 3469.000],  loss: 33.142235, mae: 14.430149, mean_q: 31.019266\n",
            "  3577/30000: episode: 597, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 3770.667 [1868.000, 5750.000],  loss: 28.571190, mae: 14.051323, mean_q: 30.458094\n",
            "  3583/30000: episode: 598, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2796.667 [419.000, 5646.000],  loss: 25.731071, mae: 14.759587, mean_q: 32.402203\n",
            "  3589/30000: episode: 599, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2685.333 [0.000, 4685.000],  loss: 33.507710, mae: 15.092483, mean_q: 33.489750\n",
            "  3595/30000: episode: 600, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 2338.333 [0.000, 4212.000],  loss: 40.345554, mae: 15.130422, mean_q: 33.452984\n",
            "  3601/30000: episode: 601, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 26.967680, mae: 15.181163, mean_q: 33.647690\n",
            "  3607/30000: episode: 602, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 44.161640, mae: 14.887878, mean_q: 33.426819\n",
            "  3613/30000: episode: 603, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 36.838150, mae: 14.656573, mean_q: 32.696934\n",
            "  3619/30000: episode: 604, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3043.000 [442.000, 5656.000],  loss: 32.631901, mae: 13.614345, mean_q: 30.406691\n",
            "  3625/30000: episode: 605, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2705.167 [0.000, 4516.000],  loss: 33.874077, mae: 14.480869, mean_q: 32.135376\n",
            "  3631/30000: episode: 606, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 52.062347, mae: 14.624476, mean_q: 32.147007\n",
            "  3637/30000: episode: 607, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2399.667 [0.000, 5057.000],  loss: 30.667122, mae: 14.863194, mean_q: 32.775761\n",
            "  3643/30000: episode: 608, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 21.882484, mae: 14.237183, mean_q: 31.548162\n",
            "  3649/30000: episode: 609, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 43.667065, mae: 15.857411, mean_q: 34.766811\n",
            "  3655/30000: episode: 610, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2941.667 [0.000, 4987.000],  loss: 35.436008, mae: 13.713371, mean_q: 30.727892\n",
            "  3661/30000: episode: 611, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [-5.000, 20.000], mean action: 2443.333 [0.000, 3388.000],  loss: 32.881504, mae: 14.356517, mean_q: 31.577423\n",
            "  3667/30000: episode: 612, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 1369.000 [0.000, 2737.000],  loss: 24.618425, mae: 14.737378, mean_q: 32.141788\n",
            "  3673/30000: episode: 613, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1345.500 [0.000, 3613.000],  loss: 34.869148, mae: 15.423020, mean_q: 33.116787\n",
            "  3679/30000: episode: 614, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 60.000, mean reward: 10.000 [ 0.000, 45.000], mean action: 2580.167 [409.000, 4814.000],  loss: 33.438190, mae: 14.257706, mean_q: 30.942831\n",
            "  3685/30000: episode: 615, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 2834.833 [350.000, 5703.000],  loss: 23.970354, mae: 13.181401, mean_q: 28.649080\n",
            "  3691/30000: episode: 616, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3194.333 [0.000, 4914.000],  loss: 23.449717, mae: 14.843964, mean_q: 31.712730\n",
            "  3697/30000: episode: 617, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2338.667 [508.000, 4531.000],  loss: 31.819496, mae: 13.937107, mean_q: 29.743170\n",
            "  3703/30000: episode: 618, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2392.667 [1358.000, 3062.000],  loss: 52.797363, mae: 15.228562, mean_q: 32.630077\n",
            "  3709/30000: episode: 619, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2315.833 [1041.000, 2818.000],  loss: 31.902136, mae: 14.984056, mean_q: 32.426548\n",
            "  3715/30000: episode: 620, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 65.000, mean reward: 10.833 [ 0.000, 35.000], mean action: 3595.000 [1583.000, 5579.000],  loss: 35.145382, mae: 14.632863, mean_q: 31.592703\n",
            "  3721/30000: episode: 621, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2550.833 [382.000, 5729.000],  loss: 31.762808, mae: 14.759614, mean_q: 31.362261\n",
            "  3727/30000: episode: 622, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1664.167 [417.000, 3451.000],  loss: 22.500601, mae: 13.219541, mean_q: 29.044952\n",
            "  3733/30000: episode: 623, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 3937.833 [2669.000, 5579.000],  loss: 35.601917, mae: 15.327886, mean_q: 33.144218\n",
            "  3739/30000: episode: 624, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2693.000 [670.000, 5249.000],  loss: 28.675192, mae: 14.667598, mean_q: 31.412674\n",
            "  3745/30000: episode: 625, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2831.833 [1583.000, 4616.000],  loss: 30.252335, mae: 15.237346, mean_q: 32.775497\n",
            "  3751/30000: episode: 626, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2760.667 [442.000, 5579.000],  loss: 24.298338, mae: 13.807499, mean_q: 30.313265\n",
            "  3757/30000: episode: 627, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4395.667 [1634.000, 5579.000],  loss: 41.334644, mae: 14.009015, mean_q: 30.953728\n",
            "  3763/30000: episode: 628, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3206.667 [1583.000, 4619.000],  loss: 31.530546, mae: 12.965332, mean_q: 29.041979\n",
            "  3769/30000: episode: 629, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1861.333 [138.000, 4616.000],  loss: 24.592590, mae: 14.695323, mean_q: 31.707392\n",
            "  3775/30000: episode: 630, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3604.333 [1596.000, 5703.000],  loss: 30.030357, mae: 14.846692, mean_q: 32.421677\n",
            "  3781/30000: episode: 631, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward: 40.000, mean reward:  6.667 [ 0.000, 25.000], mean action: 2019.333 [271.000, 5579.000],  loss: 25.675783, mae: 14.659974, mean_q: 31.996536\n",
            "  3787/30000: episode: 632, duration: 0.262s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3216.667 [1583.000, 5433.000],  loss: 30.074097, mae: 14.116630, mean_q: 31.207449\n",
            "  3793/30000: episode: 633, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3084.333 [656.000, 5579.000],  loss: 30.415329, mae: 14.231110, mean_q: 31.414452\n",
            "  3799/30000: episode: 634, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2154.000 [138.000, 4719.000],  loss: 31.309677, mae: 15.093574, mean_q: 32.545826\n",
            "  3805/30000: episode: 635, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3459.833 [1029.000, 5433.000],  loss: 26.231750, mae: 15.998603, mean_q: 33.827679\n",
            "  3811/30000: episode: 636, duration: 0.261s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1982.167 [442.000, 4681.000],  loss: 42.812916, mae: 14.054053, mean_q: 30.885048\n",
            "  3817/30000: episode: 637, duration: 0.263s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3088.833 [581.000, 4616.000],  loss: 37.627254, mae: 14.204212, mean_q: 31.035980\n",
            "  3823/30000: episode: 638, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2761.500 [1632.000, 5640.000],  loss: 34.537430, mae: 14.340675, mean_q: 30.681845\n",
            "  3829/30000: episode: 639, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward: 20.000, mean reward:  3.333 [-5.000, 25.000], mean action: 2783.833 [802.000, 4595.000],  loss: 40.152355, mae: 14.728744, mean_q: 32.401474\n",
            "  3835/30000: episode: 640, duration: 0.264s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3309.833 [1783.000, 5729.000],  loss: 27.044952, mae: 14.404870, mean_q: 31.392660\n",
            "  3841/30000: episode: 641, duration: 0.264s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2159.000 [560.000, 4185.000],  loss: 39.237820, mae: 14.671285, mean_q: 32.241806\n",
            "  3847/30000: episode: 642, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2164.667 [1783.000, 3123.000],  loss: 28.675039, mae: 13.500768, mean_q: 30.164270\n",
            "  3853/30000: episode: 643, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4060.000 [1116.000, 5729.000],  loss: 31.994318, mae: 14.279164, mean_q: 31.395449\n",
            "  3859/30000: episode: 644, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 50.000, mean reward:  8.333 [ 0.000, 30.000], mean action: 2721.833 [1405.000, 5200.000],  loss: 25.241417, mae: 13.129088, mean_q: 29.744581\n",
            "  3865/30000: episode: 645, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3189.333 [1145.000, 5533.000],  loss: 29.018248, mae: 14.254276, mean_q: 30.967184\n",
            "  3871/30000: episode: 646, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4096.667 [2930.000, 4683.000],  loss: 31.981857, mae: 14.625778, mean_q: 32.041424\n",
            "  3877/30000: episode: 647, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 1934.833 [476.000, 2700.000],  loss: 33.992321, mae: 14.528224, mean_q: 32.568645\n",
            "  3883/30000: episode: 648, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3238.333 [2372.000, 4403.000],  loss: 40.177029, mae: 14.714688, mean_q: 31.869165\n",
            "  3889/30000: episode: 649, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1700.667 [242.000, 4683.000],  loss: 41.698906, mae: 14.528382, mean_q: 31.173056\n",
            "  3895/30000: episode: 650, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3716.833 [1246.000, 4884.000],  loss: 36.064945, mae: 13.368369, mean_q: 29.154299\n",
            "  3901/30000: episode: 651, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2568.500 [62.000, 5729.000],  loss: 44.225315, mae: 15.294580, mean_q: 32.488735\n",
            "  3907/30000: episode: 652, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3072.167 [1254.000, 5715.000],  loss: 30.104239, mae: 14.574830, mean_q: 31.837748\n",
            "  3913/30000: episode: 653, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2120.333 [216.000, 3583.000],  loss: 42.617970, mae: 13.651169, mean_q: 29.981476\n",
            "  3919/30000: episode: 654, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2627.333 [1310.000, 5331.000],  loss: 33.776211, mae: 15.449286, mean_q: 33.552746\n",
            "  3925/30000: episode: 655, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3185.667 [578.000, 5249.000],  loss: 56.435196, mae: 14.966190, mean_q: 33.005737\n",
            "  3931/30000: episode: 656, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2936.000 [578.000, 5729.000],  loss: 39.413128, mae: 15.137105, mean_q: 32.946697\n",
            "  3937/30000: episode: 657, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 3101.333 [1132.000, 5239.000],  loss: 35.924664, mae: 14.854198, mean_q: 32.606632\n",
            "  3943/30000: episode: 658, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2934.833 [1847.000, 3902.000],  loss: 32.325722, mae: 15.010777, mean_q: 32.795124\n",
            "  3949/30000: episode: 659, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 4058.333 [1646.000, 5545.000],  loss: 41.177120, mae: 14.330440, mean_q: 31.228577\n",
            "  3955/30000: episode: 660, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 3307.667 [1847.000, 3951.000],  loss: 43.993870, mae: 14.079972, mean_q: 30.642883\n",
            "  3961/30000: episode: 661, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1990.667 [209.000, 3510.000],  loss: 39.418209, mae: 14.467480, mean_q: 32.012142\n",
            "  3967/30000: episode: 662, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2863.333 [1583.000, 5503.000],  loss: 26.314409, mae: 13.480919, mean_q: 30.233469\n",
            "  3973/30000: episode: 663, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3442.500 [1850.000, 5656.000],  loss: 32.999161, mae: 13.655698, mean_q: 30.582245\n",
            "  3979/30000: episode: 664, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3406.000 [974.000, 5148.000],  loss: 28.663630, mae: 12.702029, mean_q: 28.998299\n",
            "  3985/30000: episode: 665, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [-5.000, 15.000], mean action: 3656.833 [1583.000, 4683.000],  loss: 23.186434, mae: 15.213321, mean_q: 32.912273\n",
            "  3991/30000: episode: 666, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2878.833 [1617.000, 5433.000],  loss: 23.128220, mae: 13.107429, mean_q: 29.480631\n",
            "  3997/30000: episode: 667, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4065.500 [2421.000, 5557.000],  loss: 34.731766, mae: 15.607488, mean_q: 33.616344\n",
            "  4003/30000: episode: 668, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 1919.667 [138.000, 4970.000],  loss: 27.328627, mae: 15.096066, mean_q: 33.101040\n",
            "  4009/30000: episode: 669, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5009.333 [3442.000, 5750.000],  loss: 26.796244, mae: 13.224088, mean_q: 28.789040\n",
            "  4015/30000: episode: 670, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2445.500 [57.000, 4719.000],  loss: 46.430248, mae: 14.083317, mean_q: 30.975517\n",
            "  4021/30000: episode: 671, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2868.333 [62.000, 5251.000],  loss: 23.365356, mae: 14.230199, mean_q: 30.978508\n",
            "  4027/30000: episode: 672, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2013.333 [101.000, 3469.000],  loss: 31.234598, mae: 13.651299, mean_q: 29.811478\n",
            "  4033/30000: episode: 673, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1721.167 [1041.000, 3442.000],  loss: 38.419060, mae: 15.704669, mean_q: 33.368351\n",
            "  4039/30000: episode: 674, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3741.500 [1648.000, 4619.000],  loss: 26.518549, mae: 13.982263, mean_q: 30.708570\n",
            "  4045/30000: episode: 675, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2221.833 [101.000, 5743.000],  loss: 29.658005, mae: 15.206894, mean_q: 33.038746\n",
            "  4051/30000: episode: 676, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3142.667 [1358.000, 5386.000],  loss: 34.857708, mae: 13.008214, mean_q: 29.119635\n",
            "  4057/30000: episode: 677, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2693.333 [1041.000, 4945.000],  loss: 30.075312, mae: 14.374942, mean_q: 31.551855\n",
            "  4063/30000: episode: 678, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 90.000, mean reward: 15.000 [ 0.000, 35.000], mean action: 2650.500 [138.000, 4619.000],  loss: 36.496490, mae: 13.500422, mean_q: 30.086966\n",
            "  4069/30000: episode: 679, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3923.000 [803.000, 5456.000],  loss: 35.275471, mae: 14.094269, mean_q: 30.951941\n",
            "  4075/30000: episode: 680, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 1784.333 [44.000, 3709.000],  loss: 39.785603, mae: 13.964349, mean_q: 31.179651\n",
            "  4081/30000: episode: 681, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3535.500 [62.000, 4802.000],  loss: 28.083548, mae: 14.487709, mean_q: 31.919405\n",
            "  4087/30000: episode: 682, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1688.833 [477.000, 3921.000],  loss: 43.897259, mae: 15.079449, mean_q: 32.666271\n",
            "  4093/30000: episode: 683, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2549.500 [138.000, 5448.000],  loss: 33.881935, mae: 14.558911, mean_q: 32.312183\n",
            "  4099/30000: episode: 684, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3598.667 [1358.000, 4719.000],  loss: 41.235855, mae: 15.420067, mean_q: 33.775909\n",
            "  4105/30000: episode: 685, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 4028.833 [1358.000, 5243.000],  loss: 31.911407, mae: 15.527490, mean_q: 33.433849\n",
            "  4111/30000: episode: 686, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [-5.000, 15.000], mean action: 2017.667 [1358.000, 2752.000],  loss: 52.323822, mae: 14.566479, mean_q: 33.028011\n",
            "  4117/30000: episode: 687, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3132.833 [1266.000, 4719.000],  loss: 28.943056, mae: 14.485176, mean_q: 32.173077\n",
            "  4123/30000: episode: 688, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3212.333 [1358.000, 4637.000],  loss: 37.012421, mae: 14.807270, mean_q: 32.446976\n",
            "  4129/30000: episode: 689, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2924.833 [1473.000, 4719.000],  loss: 28.826094, mae: 14.290874, mean_q: 31.944778\n",
            "  4135/30000: episode: 690, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 962.667 [138.000, 1358.000],  loss: 30.968817, mae: 14.298017, mean_q: 31.871199\n",
            "  4141/30000: episode: 691, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2427.500 [1137.000, 5729.000],  loss: 39.539665, mae: 14.156284, mean_q: 32.061802\n",
            "  4147/30000: episode: 692, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4241.833 [1137.000, 5715.000],  loss: 33.185688, mae: 14.544128, mean_q: 31.325655\n",
            "  4153/30000: episode: 693, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3004.500 [1219.000, 4973.000],  loss: 30.959860, mae: 14.453820, mean_q: 31.961897\n",
            "  4159/30000: episode: 694, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: 20.000, mean reward:  3.333 [-5.000, 25.000], mean action: 1477.833 [62.000, 4397.000],  loss: 18.371874, mae: 13.821286, mean_q: 31.036860\n",
            "  4165/30000: episode: 695, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 2673.667 [554.000, 4749.000],  loss: 33.467678, mae: 13.952339, mean_q: 31.287985\n",
            "  4171/30000: episode: 696, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3356.667 [551.000, 4719.000],  loss: 32.411823, mae: 14.053256, mean_q: 31.201197\n",
            "  4177/30000: episode: 697, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2155.667 [71.000, 5387.000],  loss: 32.395859, mae: 13.739179, mean_q: 30.665949\n",
            "  4183/30000: episode: 698, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3794.667 [554.000, 5524.000],  loss: 32.380772, mae: 13.754449, mean_q: 30.955673\n",
            "  4189/30000: episode: 699, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1863.167 [62.000, 3587.000],  loss: 36.867226, mae: 13.371587, mean_q: 30.007524\n",
            "  4195/30000: episode: 700, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4219.833 [1020.000, 5476.000],  loss: 29.271341, mae: 13.580082, mean_q: 30.048162\n",
            "  4201/30000: episode: 701, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 1901.500 [1358.000, 4619.000],  loss: 23.401911, mae: 14.810050, mean_q: 32.335278\n",
            "  4207/30000: episode: 702, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1780.333 [238.000, 3083.000],  loss: 24.033438, mae: 13.973073, mean_q: 30.625597\n",
            "  4213/30000: episode: 703, duration: 0.224s, episode steps:   6, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2677.500 [808.000, 4719.000],  loss: 45.757626, mae: 15.459287, mean_q: 32.621311\n",
            "  4219/30000: episode: 704, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2795.500 [547.000, 5200.000],  loss: 30.741287, mae: 14.149281, mean_q: 30.961512\n",
            "  4225/30000: episode: 705, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3673.167 [571.000, 4921.000],  loss: 35.666035, mae: 14.351361, mean_q: 31.698112\n",
            "  4231/30000: episode: 706, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1907.000 [31.000, 4749.000],  loss: 31.191788, mae: 15.930389, mean_q: 34.380344\n",
            "  4237/30000: episode: 707, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2186.167 [38.000, 4681.000],  loss: 28.376974, mae: 15.433076, mean_q: 33.019176\n",
            "  4243/30000: episode: 708, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2607.000 [1617.000, 3382.000],  loss: 37.285229, mae: 14.871484, mean_q: 32.682495\n",
            "  4249/30000: episode: 709, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2907.333 [1110.000, 5715.000],  loss: 24.611517, mae: 14.716454, mean_q: 32.544498\n",
            "  4255/30000: episode: 710, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2839.167 [249.000, 4486.000],  loss: 26.008551, mae: 14.380904, mean_q: 31.707138\n",
            "  4261/30000: episode: 711, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1828.667 [523.000, 4619.000],  loss: 16.370987, mae: 13.536420, mean_q: 30.217340\n",
            "  4267/30000: episode: 712, duration: 0.260s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2491.500 [554.000, 4619.000],  loss: 27.698263, mae: 14.270577, mean_q: 32.153534\n",
            "  4273/30000: episode: 713, duration: 0.264s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2932.667 [363.000, 5305.000],  loss: 31.940531, mae: 14.596428, mean_q: 32.568981\n",
            "  4279/30000: episode: 714, duration: 0.199s, episode steps:   6, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3224.000 [238.000, 5545.000],  loss: 35.942730, mae: 14.033895, mean_q: 30.923777\n",
            "  4285/30000: episode: 715, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3682.667 [1132.000, 5433.000],  loss: 23.962158, mae: 12.962022, mean_q: 28.941477\n",
            "  4291/30000: episode: 716, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2410.667 [873.000, 4683.000],  loss: 18.549505, mae: 14.577712, mean_q: 31.585510\n",
            "  4297/30000: episode: 717, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 40.000, mean reward:  6.667 [ 0.000, 15.000], mean action: 3445.333 [1596.000, 5579.000],  loss: 45.873062, mae: 15.027286, mean_q: 32.226864\n",
            "  4303/30000: episode: 718, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 2802.000 [249.000, 4832.000],  loss: 25.752859, mae: 13.377090, mean_q: 29.526535\n",
            "  4309/30000: episode: 719, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 5004.833 [3993.000, 5674.000],  loss: 30.000595, mae: 14.342506, mean_q: 31.494246\n",
            "  4315/30000: episode: 720, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3496.667 [1609.000, 4548.000],  loss: 24.419426, mae: 14.608634, mean_q: 31.152679\n",
            "  4321/30000: episode: 721, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2351.500 [741.000, 5457.000],  loss: 32.483158, mae: 13.870444, mean_q: 30.467529\n",
            "  4327/30000: episode: 722, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1769.333 [354.000, 3820.000],  loss: 27.706198, mae: 14.881146, mean_q: 32.426144\n",
            "  4333/30000: episode: 723, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2679.833 [1207.000, 5482.000],  loss: 41.298710, mae: 16.120054, mean_q: 34.976929\n",
            "  4339/30000: episode: 724, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3096.000 [1613.000, 4683.000],  loss: 26.512321, mae: 15.085776, mean_q: 33.023609\n",
            "  4345/30000: episode: 725, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1643.500 [199.000, 4604.000],  loss: 36.917511, mae: 13.857919, mean_q: 30.923777\n",
            "  4351/30000: episode: 726, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2890.167 [343.000, 4873.000],  loss: 50.272930, mae: 13.362412, mean_q: 29.464487\n",
            "  4357/30000: episode: 727, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3807.833 [355.000, 5168.000],  loss: 36.595318, mae: 14.288407, mean_q: 31.565466\n",
            "  4363/30000: episode: 728, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2740.833 [38.000, 5564.000],  loss: 31.900061, mae: 14.107510, mean_q: 31.140505\n",
            "  4369/30000: episode: 729, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 55.000, mean reward:  9.167 [ 0.000, 40.000], mean action: 2989.000 [664.000, 5579.000],  loss: 33.874660, mae: 16.383230, mean_q: 35.937786\n",
            "  4375/30000: episode: 730, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 2043.000 [2043.000, 2043.000],  loss: 27.305246, mae: 15.558787, mean_q: 34.042149\n",
            "  4381/30000: episode: 731, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 2043.000 [2043.000, 2043.000],  loss: 37.358723, mae: 13.817609, mean_q: 31.281853\n",
            "  4387/30000: episode: 732, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2608.000 [2043.000, 5433.000],  loss: 32.452938, mae: 14.126761, mean_q: 32.060204\n",
            "  4393/30000: episode: 733, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [-5.000, 15.000], mean action: 4060.500 [3041.000, 5715.000],  loss: 31.206869, mae: 13.946551, mean_q: 31.360277\n",
            "  4399/30000: episode: 734, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 20.000, mean reward:  3.333 [ 0.000, 10.000], mean action: 1634.500 [158.000, 5212.000],  loss: 27.287275, mae: 14.366219, mean_q: 31.945930\n",
            "  4405/30000: episode: 735, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4123.833 [2043.000, 5364.000],  loss: 37.009422, mae: 15.143352, mean_q: 33.669033\n",
            "  4411/30000: episode: 736, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2970.000 [846.000, 5474.000],  loss: 37.132244, mae: 14.402660, mean_q: 32.210674\n",
            "  4417/30000: episode: 737, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 85.000, mean reward: 14.167 [ 0.000, 35.000], mean action: 2308.167 [199.000, 4185.000],  loss: 29.044296, mae: 14.988769, mean_q: 33.510258\n",
            "  4423/30000: episode: 738, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1891.667 [199.000, 5171.000],  loss: 44.898548, mae: 15.133716, mean_q: 33.503887\n",
            "  4429/30000: episode: 739, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2595.667 [1613.000, 3921.000],  loss: 45.651768, mae: 14.606408, mean_q: 32.538090\n",
            "  4435/30000: episode: 740, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3104.667 [1279.000, 5476.000],  loss: 26.088263, mae: 15.036835, mean_q: 33.750439\n",
            "  4441/30000: episode: 741, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2244.667 [907.000, 5410.000],  loss: 48.035843, mae: 14.593577, mean_q: 32.592121\n",
            "  4447/30000: episode: 742, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3054.833 [199.000, 4656.000],  loss: 19.490417, mae: 13.361630, mean_q: 30.059843\n",
            "  4453/30000: episode: 743, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2944.833 [320.000, 4683.000],  loss: 34.543056, mae: 14.466487, mean_q: 31.620161\n",
            "  4459/30000: episode: 744, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 3136.833 [962.000, 5579.000],  loss: 33.665150, mae: 13.001420, mean_q: 29.547277\n",
            "  4465/30000: episode: 745, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3003.833 [667.000, 5747.000],  loss: 30.356894, mae: 14.243081, mean_q: 31.899078\n",
            "  4471/30000: episode: 746, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2261.167 [418.000, 5713.000],  loss: 22.779924, mae: 14.662023, mean_q: 32.153778\n",
            "  4477/30000: episode: 747, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 85.000, mean reward: 14.167 [ 0.000, 20.000], mean action: 2770.333 [560.000, 5146.000],  loss: 24.652189, mae: 14.838840, mean_q: 31.945534\n",
            "  4483/30000: episode: 748, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2788.167 [409.000, 5722.000],  loss: 33.117085, mae: 13.549870, mean_q: 30.120188\n",
            "  4489/30000: episode: 749, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3202.000 [38.000, 5433.000],  loss: 18.987238, mae: 13.461871, mean_q: 30.160719\n",
            "  4495/30000: episode: 750, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2205.000 [1530.000, 5200.000],  loss: 34.490185, mae: 14.212646, mean_q: 31.175684\n",
            "  4501/30000: episode: 751, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3395.500 [2008.000, 4683.000],  loss: 36.022709, mae: 13.705406, mean_q: 30.227365\n",
            "  4507/30000: episode: 752, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [-5.000, 10.000], mean action: 2620.500 [1688.000, 4683.000],  loss: 27.303774, mae: 13.666260, mean_q: 29.634253\n",
            "  4513/30000: episode: 753, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4242.167 [238.000, 5395.000],  loss: 26.533142, mae: 14.092826, mean_q: 31.093033\n",
            "  4519/30000: episode: 754, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2567.000 [328.000, 4838.000],  loss: 24.897066, mae: 14.314597, mean_q: 30.948038\n",
            "  4525/30000: episode: 755, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1732.500 [216.000, 3829.000],  loss: 41.711094, mae: 14.975841, mean_q: 32.363438\n",
            "  4531/30000: episode: 756, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: -15.000, mean reward: -2.500 [-5.000,  5.000], mean action: 3811.167 [547.000, 4464.000],  loss: 22.495039, mae: 14.455013, mean_q: 31.657799\n",
            "  4537/30000: episode: 757, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4607.000 [3008.000, 5549.000],  loss: 20.850660, mae: 14.125072, mean_q: 31.434729\n",
            "  4543/30000: episode: 758, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2086.667 [216.000, 5121.000],  loss: 29.782335, mae: 13.046357, mean_q: 29.360031\n",
            "  4549/30000: episode: 759, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2908.500 [946.000, 5579.000],  loss: 36.485104, mae: 15.554214, mean_q: 33.224361\n",
            "  4555/30000: episode: 760, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2563.500 [385.000, 4838.000],  loss: 29.468481, mae: 13.519108, mean_q: 30.587988\n",
            "  4561/30000: episode: 761, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 140.000, mean reward: 23.333 [ 0.000, 35.000], mean action: 3368.000 [238.000, 4185.000],  loss: 36.106297, mae: 15.493499, mean_q: 33.231750\n",
            "  4567/30000: episode: 762, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 55.000, mean reward:  9.167 [ 0.000, 20.000], mean action: 2980.000 [1596.000, 5676.000],  loss: 28.773407, mae: 14.647523, mean_q: 32.933865\n",
            "  4573/30000: episode: 763, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2955.833 [238.000, 5508.000],  loss: 23.442274, mae: 14.512062, mean_q: 31.787558\n",
            "  4579/30000: episode: 764, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 20.000, mean reward:  3.333 [ 0.000, 10.000], mean action: 3447.667 [1473.000, 5152.000],  loss: 24.699104, mae: 13.934550, mean_q: 30.623999\n",
            "  4585/30000: episode: 765, duration: 0.185s, episode steps:   6, steps per second:  32, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 3357.333 [43.000, 5680.000],  loss: 37.553173, mae: 15.028481, mean_q: 33.114700\n",
            "  4591/30000: episode: 766, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3823.000 [2733.000, 5373.000],  loss: 23.397818, mae: 13.905070, mean_q: 30.511599\n",
            "  4597/30000: episode: 767, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4156.833 [3473.000, 4439.000],  loss: 37.435623, mae: 14.348096, mean_q: 31.308149\n",
            "  4603/30000: episode: 768, duration: 0.185s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3764.167 [2733.000, 5433.000],  loss: 43.425720, mae: 14.641850, mean_q: 31.616194\n",
            "  4609/30000: episode: 769, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2786.500 [101.000, 5369.000],  loss: 47.213337, mae: 14.463824, mean_q: 31.608032\n",
            "  4615/30000: episode: 770, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward: 40.000, mean reward:  6.667 [ 0.000, 25.000], mean action: 2684.167 [669.000, 4552.000],  loss: 34.116344, mae: 15.396321, mean_q: 33.593967\n",
            "  4621/30000: episode: 771, duration: 0.188s, episode steps:   6, steps per second:  32, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 4180.000 [2973.000, 5639.000],  loss: 30.324846, mae: 15.608975, mean_q: 34.039692\n",
            "  4627/30000: episode: 772, duration: 0.186s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3843.500 [1072.000, 5658.000],  loss: 36.790424, mae: 15.058586, mean_q: 32.754597\n",
            "  4633/30000: episode: 773, duration: 0.202s, episode steps:   6, steps per second:  30, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 1630.833 [333.000, 2733.000],  loss: 20.962858, mae: 14.183879, mean_q: 30.791601\n",
            "  4639/30000: episode: 774, duration: 0.268s, episode steps:   6, steps per second:  22, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3287.667 [1058.000, 5656.000],  loss: 57.575581, mae: 14.816997, mean_q: 32.181320\n",
            "  4645/30000: episode: 775, duration: 0.267s, episode steps:   6, steps per second:  22, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2898.333 [238.000, 5508.000],  loss: 30.751076, mae: 14.423400, mean_q: 31.202459\n",
            "  4651/30000: episode: 776, duration: 0.265s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3974.333 [1044.000, 5593.000],  loss: 44.875446, mae: 15.303845, mean_q: 32.821331\n",
            "  4657/30000: episode: 777, duration: 0.262s, episode steps:   6, steps per second:  23, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 4671.000 [3829.000, 5249.000],  loss: 41.339661, mae: 15.329312, mean_q: 33.012196\n",
            "  4663/30000: episode: 778, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3156.167 [1747.000, 5703.000],  loss: 36.956230, mae: 14.595067, mean_q: 31.465857\n",
            "  4669/30000: episode: 779, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3893.833 [1613.000, 5433.000],  loss: 30.414284, mae: 15.063031, mean_q: 31.873764\n",
            "  4675/30000: episode: 780, duration: 0.260s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3811.000 [2304.000, 5707.000],  loss: 35.946178, mae: 14.037519, mean_q: 30.392809\n",
            "  4681/30000: episode: 781, duration: 0.266s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3629.667 [2544.000, 4921.000],  loss: 36.546825, mae: 14.874307, mean_q: 31.576532\n",
            "  4687/30000: episode: 782, duration: 0.274s, episode steps:   6, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2628.667 [685.000, 5482.000],  loss: 23.168816, mae: 13.616199, mean_q: 29.964729\n",
            "  4693/30000: episode: 783, duration: 0.263s, episode steps:   6, steps per second:  23, episode reward: 25.000, mean reward:  4.167 [-5.000, 25.000], mean action: 2357.000 [158.000, 5703.000],  loss: 33.997730, mae: 13.917176, mean_q: 30.525118\n",
            "  4699/30000: episode: 784, duration: 0.219s, episode steps:   6, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3274.167 [571.000, 5471.000],  loss: 24.711267, mae: 14.755024, mean_q: 31.570414\n",
            "  4705/30000: episode: 785, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2721.333 [523.000, 5200.000],  loss: 25.647390, mae: 13.864216, mean_q: 30.350256\n",
            "  4711/30000: episode: 786, duration: 0.183s, episode steps:   6, steps per second:  33, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1556.000 [607.000, 2277.000],  loss: 38.674816, mae: 14.320273, mean_q: 32.201260\n",
            "  4717/30000: episode: 787, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3459.500 [1330.000, 5233.000],  loss: 44.998169, mae: 14.643249, mean_q: 31.836823\n",
            "  4723/30000: episode: 788, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2544.833 [326.000, 4683.000],  loss: 39.477646, mae: 13.336232, mean_q: 29.295671\n",
            "  4729/30000: episode: 789, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3078.167 [1267.000, 5308.000],  loss: 30.428556, mae: 15.148064, mean_q: 32.157627\n",
            "  4735/30000: episode: 790, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3376.167 [962.000, 5193.000],  loss: 41.894436, mae: 14.386518, mean_q: 31.260065\n",
            "  4741/30000: episode: 791, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2182.833 [349.000, 4091.000],  loss: 38.427204, mae: 15.549210, mean_q: 33.596561\n",
            "  4747/30000: episode: 792, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3797.667 [1686.000, 5406.000],  loss: 32.230003, mae: 14.379718, mean_q: 31.551023\n",
            "  4753/30000: episode: 793, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 2112.167 [1202.000, 3221.000],  loss: 30.926477, mae: 14.418645, mean_q: 31.643410\n",
            "  4759/30000: episode: 794, duration: 0.183s, episode steps:   6, steps per second:  33, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 2207.000 [754.000, 4595.000],  loss: 26.371222, mae: 13.519582, mean_q: 30.117502\n",
            "  4765/30000: episode: 795, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2542.500 [363.000, 4630.000],  loss: 18.869692, mae: 14.092814, mean_q: 31.426308\n",
            "  4771/30000: episode: 796, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3106.667 [2277.000, 5703.000],  loss: 35.960133, mae: 14.443944, mean_q: 31.795858\n",
            "  4777/30000: episode: 797, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2887.500 [1484.000, 4567.000],  loss: 28.337671, mae: 14.249724, mean_q: 31.835871\n",
            "  4783/30000: episode: 798, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3488.167 [951.000, 5476.000],  loss: 28.357748, mae: 14.363865, mean_q: 31.934149\n",
            "  4789/30000: episode: 799, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3170.667 [717.000, 4288.000],  loss: 17.796671, mae: 13.148099, mean_q: 29.964430\n",
            "  4795/30000: episode: 800, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2242.333 [1613.000, 2733.000],  loss: 27.948067, mae: 13.496197, mean_q: 30.022783\n",
            "  4801/30000: episode: 801, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 2713.000 [106.000, 5249.000],  loss: 43.808987, mae: 13.829993, mean_q: 31.317930\n",
            "  4807/30000: episode: 802, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: 25.000, mean reward:  4.167 [-5.000, 30.000], mean action: 2468.667 [946.000, 4208.000],  loss: 29.981651, mae: 14.170749, mean_q: 31.836031\n",
            "  4813/30000: episode: 803, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3069.167 [1194.000, 5564.000],  loss: 35.450703, mae: 14.058599, mean_q: 31.139709\n",
            "  4819/30000: episode: 804, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2535.500 [90.000, 5144.000],  loss: 22.265589, mae: 14.739306, mean_q: 31.705566\n",
            "  4825/30000: episode: 805, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 1631.333 [238.000, 1910.000],  loss: 36.951462, mae: 15.855666, mean_q: 33.688030\n",
            "  4831/30000: episode: 806, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2098.000 [818.000, 4110.000],  loss: 30.973310, mae: 14.689396, mean_q: 32.227871\n",
            "  4837/30000: episode: 807, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 3529.500 [1688.000, 5198.000],  loss: 29.488310, mae: 14.770027, mean_q: 31.814377\n",
            "  4843/30000: episode: 808, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3307.667 [607.000, 5563.000],  loss: 20.876102, mae: 14.289517, mean_q: 31.743393\n",
            "  4849/30000: episode: 809, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1599.333 [879.000, 2578.000],  loss: 32.328163, mae: 15.406865, mean_q: 33.143173\n",
            "  4855/30000: episode: 810, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1751.167 [238.000, 3497.000],  loss: 29.313761, mae: 12.828570, mean_q: 29.018600\n",
            "  4861/30000: episode: 811, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2731.167 [518.000, 4840.000],  loss: 45.779896, mae: 14.549787, mean_q: 31.408371\n",
            "  4867/30000: episode: 812, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: 110.000, mean reward: 18.333 [ 0.000, 35.000], mean action: 3935.667 [1912.000, 4683.000],  loss: 28.161819, mae: 14.009644, mean_q: 31.087942\n",
            "  4873/30000: episode: 813, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2200.500 [523.000, 5658.000],  loss: 32.432178, mae: 14.881310, mean_q: 32.412228\n",
            "  4879/30000: episode: 814, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3251.167 [249.000, 5476.000],  loss: 25.644396, mae: 14.543266, mean_q: 31.849817\n",
            "  4885/30000: episode: 815, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2036.167 [330.000, 4680.000],  loss: 32.359413, mae: 14.919347, mean_q: 32.447056\n",
            "  4891/30000: episode: 816, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2297.000 [79.000, 4881.000],  loss: 20.162701, mae: 13.503833, mean_q: 29.162392\n",
            "  4897/30000: episode: 817, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2361.333 [523.000, 3221.000],  loss: 34.558746, mae: 14.124260, mean_q: 30.340569\n",
            "  4903/30000: episode: 818, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2289.500 [656.000, 2752.000],  loss: 35.115299, mae: 14.908489, mean_q: 31.647713\n",
            "  4909/30000: episode: 819, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 1645.500 [1132.000, 2578.000],  loss: 46.950497, mae: 14.694794, mean_q: 31.388557\n",
            "  4915/30000: episode: 820, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2600.333 [271.000, 5753.000],  loss: 24.473883, mae: 15.947598, mean_q: 33.655716\n",
            "  4921/30000: episode: 821, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3925.000 [1558.000, 5605.000],  loss: 25.101252, mae: 14.908435, mean_q: 31.662512\n",
            "  4927/30000: episode: 822, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2303.833 [249.000, 4464.000],  loss: 20.578804, mae: 13.173137, mean_q: 28.907362\n",
            "  4933/30000: episode: 823, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3560.000 [2752.000, 4595.000],  loss: 26.372908, mae: 14.572301, mean_q: 32.102703\n",
            "  4939/30000: episode: 824, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3683.833 [946.000, 4927.000],  loss: 24.973419, mae: 14.658698, mean_q: 32.022228\n",
            "  4945/30000: episode: 825, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1752.000 [663.000, 3100.000],  loss: 31.436308, mae: 14.245460, mean_q: 31.598326\n",
            "  4951/30000: episode: 826, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3199.000 [1558.000, 4913.000],  loss: 30.345385, mae: 14.144973, mean_q: 31.355532\n",
            "  4957/30000: episode: 827, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2729.667 [663.000, 4782.000],  loss: 26.208458, mae: 15.136266, mean_q: 33.455250\n",
            "  4963/30000: episode: 828, duration: 0.194s, episode steps:   6, steps per second:  31, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3158.000 [547.000, 5703.000],  loss: 18.081911, mae: 14.610248, mean_q: 32.644543\n",
            "  4969/30000: episode: 829, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2854.333 [656.000, 4185.000],  loss: 21.410231, mae: 14.603924, mean_q: 32.587627\n",
            "  4975/30000: episode: 830, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3303.167 [667.000, 5743.000],  loss: 37.836056, mae: 15.025780, mean_q: 33.354794\n",
            "  4981/30000: episode: 831, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4030.167 [326.000, 5623.000],  loss: 31.117754, mae: 14.001695, mean_q: 31.394735\n",
            "  4987/30000: episode: 832, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4361.167 [2752.000, 4683.000],  loss: 30.438316, mae: 14.409943, mean_q: 31.200836\n",
            "  4993/30000: episode: 833, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3218.000 [578.000, 5126.000],  loss: 23.564253, mae: 12.851949, mean_q: 28.621851\n",
            "  4999/30000: episode: 834, duration: 0.183s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2331.333 [578.000, 3951.000],  loss: 34.483665, mae: 13.687407, mean_q: 30.159294\n",
            "  5005/30000: episode: 835, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2705.833 [79.000, 5476.000],  loss: 22.461267, mae: 13.534589, mean_q: 29.736486\n",
            "  5011/30000: episode: 836, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1510.333 [459.000, 2785.000],  loss: 25.572477, mae: 14.014568, mean_q: 30.300428\n",
            "  5017/30000: episode: 837, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2055.167 [250.000, 4250.000],  loss: 23.340033, mae: 14.230087, mean_q: 31.231199\n",
            "  5023/30000: episode: 838, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2963.833 [328.000, 4867.000],  loss: 30.288712, mae: 14.021564, mean_q: 30.546152\n",
            "  5029/30000: episode: 839, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2919.500 [1016.000, 5715.000],  loss: 31.173166, mae: 14.489421, mean_q: 32.351662\n",
            "  5035/30000: episode: 840, duration: 0.183s, episode steps:   6, steps per second:  33, episode reward: 25.000, mean reward:  4.167 [-5.000, 20.000], mean action: 3141.000 [142.000, 5395.000],  loss: 26.514299, mae: 14.969219, mean_q: 32.710262\n",
            "  5041/30000: episode: 841, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 35.000, mean reward:  5.833 [ 0.000, 35.000], mean action: 2599.833 [489.000, 5703.000],  loss: 24.312445, mae: 15.272273, mean_q: 32.959686\n",
            "  5047/30000: episode: 842, duration: 0.199s, episode steps:   6, steps per second:  30, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2878.167 [2752.000, 3509.000],  loss: 25.600164, mae: 14.314321, mean_q: 31.212341\n",
            "  5053/30000: episode: 843, duration: 0.272s, episode steps:   6, steps per second:  22, episode reward:  5.000, mean reward:  0.833 [-5.000, 15.000], mean action: 1561.000 [459.000, 4465.000],  loss: 46.188084, mae: 14.808051, mean_q: 32.685772\n",
            "  5059/30000: episode: 844, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 2979.167 [1153.000, 4683.000],  loss: 31.713051, mae: 15.612778, mean_q: 33.294491\n",
            "  5065/30000: episode: 845, duration: 0.267s, episode steps:   6, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2442.667 [147.000, 5148.000],  loss: 31.402466, mae: 14.331635, mean_q: 31.447939\n",
            "  5071/30000: episode: 846, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2209.500 [249.000, 4404.000],  loss: 41.632816, mae: 15.111144, mean_q: 32.374844\n",
            "  5077/30000: episode: 847, duration: 0.269s, episode steps:   6, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2250.000 [547.000, 4718.000],  loss: 36.433140, mae: 15.562171, mean_q: 33.595837\n",
            "  5083/30000: episode: 848, duration: 0.267s, episode steps:   6, steps per second:  22, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2722.667 [1530.000, 5340.000],  loss: 41.422009, mae: 14.545380, mean_q: 32.383865\n",
            "  5089/30000: episode: 849, duration: 0.273s, episode steps:   6, steps per second:  22, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1945.833 [1515.000, 2752.000],  loss: 34.453583, mae: 15.080916, mean_q: 32.624317\n",
            "  5095/30000: episode: 850, duration: 0.266s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3368.833 [547.000, 5218.000],  loss: 32.655052, mae: 15.991443, mean_q: 33.906918\n",
            "  5101/30000: episode: 851, duration: 0.267s, episode steps:   6, steps per second:  23, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3418.667 [1530.000, 4681.000],  loss: 32.944790, mae: 13.789655, mean_q: 30.606058\n",
            "  5107/30000: episode: 852, duration: 0.260s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1516.500 [393.000, 3750.000],  loss: 41.113857, mae: 14.651343, mean_q: 32.188671\n",
            "  5113/30000: episode: 853, duration: 0.236s, episode steps:   6, steps per second:  25, episode reward: 35.000, mean reward:  5.833 [ 0.000, 35.000], mean action: 4017.833 [1596.000, 5666.000],  loss: 36.509762, mae: 13.692665, mean_q: 30.913170\n",
            "  5119/30000: episode: 854, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3128.000 [218.000, 5656.000],  loss: 33.862362, mae: 15.899849, mean_q: 34.626366\n",
            "  5125/30000: episode: 855, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2391.000 [846.000, 5049.000],  loss: 47.207775, mae: 14.463452, mean_q: 31.751112\n",
            "  5131/30000: episode: 856, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2421.167 [656.000, 3654.000],  loss: 33.818489, mae: 14.881429, mean_q: 32.225945\n",
            "  5132/30000: episode: 857, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3750.000 [3750.000, 3750.000],  loss: 37.304089, mae: 11.912145, mean_q: 29.568830\n",
            "  5138/30000: episode: 858, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3564.667 [2936.000, 4377.000],  loss: 30.024359, mae: 14.404869, mean_q: 31.549646\n",
            "  5144/30000: episode: 859, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1100.167 [42.000, 2076.000],  loss: 21.599134, mae: 13.572692, mean_q: 29.990196\n",
            "  5150/30000: episode: 860, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 839.167 [17.000, 1634.000],  loss: 21.946550, mae: 14.669486, mean_q: 31.602570\n",
            "  5156/30000: episode: 861, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3167.667 [1857.000, 5727.000],  loss: 22.381393, mae: 15.113548, mean_q: 32.677139\n",
            "  5162/30000: episode: 862, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2477.000 [141.000, 4796.000],  loss: 32.508221, mae: 14.362782, mean_q: 31.271921\n",
            "  5168/30000: episode: 863, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2519.833 [1074.000, 4829.000],  loss: 23.658936, mae: 14.122735, mean_q: 30.841721\n",
            "  5174/30000: episode: 864, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3014.167 [132.000, 5521.000],  loss: 42.156879, mae: 14.212436, mean_q: 31.424057\n",
            "  5180/30000: episode: 865, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2784.167 [1348.000, 4681.000],  loss: 19.182348, mae: 13.689912, mean_q: 30.812050\n",
            "  5186/30000: episode: 866, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3520.333 [1908.000, 5000.000],  loss: 42.168545, mae: 14.946637, mean_q: 33.078014\n",
            "  5192/30000: episode: 867, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2409.000 [1059.000, 5079.000],  loss: 27.225306, mae: 14.084274, mean_q: 30.802534\n",
            "  5198/30000: episode: 868, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 55.000, mean reward:  9.167 [ 0.000, 20.000], mean action: 3099.000 [1596.000, 4773.000],  loss: 29.298180, mae: 13.983185, mean_q: 31.050558\n",
            "  5204/30000: episode: 869, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3986.833 [1041.000, 4576.000],  loss: 24.417038, mae: 14.444534, mean_q: 31.527960\n",
            "  5210/30000: episode: 870, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1913.500 [561.000, 4576.000],  loss: 36.898743, mae: 15.071178, mean_q: 32.770741\n",
            "  5216/30000: episode: 871, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1974.833 [124.000, 4464.000],  loss: 21.714218, mae: 13.876325, mean_q: 30.913389\n",
            "  5222/30000: episode: 872, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [-5.000, 10.000], mean action: 2025.333 [459.000, 5256.000],  loss: 65.065781, mae: 15.351307, mean_q: 33.075253\n",
            "  5228/30000: episode: 873, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 2711.500 [1491.000, 4185.000],  loss: 20.963728, mae: 14.186265, mean_q: 31.616074\n",
            "  5234/30000: episode: 874, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3221.500 [245.000, 5591.000],  loss: 28.001150, mae: 14.135776, mean_q: 31.415518\n",
            "  5240/30000: episode: 875, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2264.000 [124.000, 5086.000],  loss: 37.154011, mae: 13.766368, mean_q: 30.501513\n",
            "  5246/30000: episode: 876, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2441.667 [1041.000, 4913.000],  loss: 26.096863, mae: 12.931342, mean_q: 29.199141\n",
            "  5252/30000: episode: 877, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2704.167 [1321.000, 3780.000],  loss: 22.699463, mae: 13.038319, mean_q: 29.312880\n",
            "  5258/30000: episode: 878, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3112.667 [1491.000, 5070.000],  loss: 38.015308, mae: 14.727460, mean_q: 32.228779\n",
            "  5264/30000: episode: 879, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 70.000, mean reward: 11.667 [ 0.000, 30.000], mean action: 3660.500 [245.000, 5703.000],  loss: 40.297527, mae: 14.341790, mean_q: 31.367981\n",
            "  5270/30000: episode: 880, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3507.667 [1509.000, 5752.000],  loss: 26.138464, mae: 15.199699, mean_q: 32.999340\n",
            "  5276/30000: episode: 881, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 2895.500 [607.000, 5476.000],  loss: 19.748270, mae: 12.942136, mean_q: 28.873032\n",
            "  5282/30000: episode: 882, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2493.667 [839.000, 3586.000],  loss: 20.576086, mae: 14.455574, mean_q: 31.584219\n",
            "  5288/30000: episode: 883, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 3231.500 [1617.000, 5703.000],  loss: 48.427353, mae: 14.359985, mean_q: 31.586649\n",
            "  5294/30000: episode: 884, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 45.000, mean reward:  7.500 [ 0.000, 30.000], mean action: 3654.000 [2698.000, 5281.000],  loss: 37.594753, mae: 15.277164, mean_q: 33.388123\n",
            "  5300/30000: episode: 885, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4267.333 [3015.000, 5557.000],  loss: 29.504356, mae: 15.338035, mean_q: 33.433212\n",
            "  5306/30000: episode: 886, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3601.500 [1041.000, 5161.000],  loss: 53.683685, mae: 14.226211, mean_q: 31.448244\n",
            "  5312/30000: episode: 887, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3805.833 [2733.000, 4407.000],  loss: 33.043644, mae: 14.079059, mean_q: 31.023855\n",
            "  5318/30000: episode: 888, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2571.667 [818.000, 4531.000],  loss: 31.239182, mae: 14.302371, mean_q: 31.485886\n",
            "  5324/30000: episode: 889, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 35.000, mean reward:  5.833 [-5.000, 20.000], mean action: 3213.667 [2733.000, 3687.000],  loss: 29.868835, mae: 13.291988, mean_q: 29.917473\n",
            "  5330/30000: episode: 890, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2766.833 [1290.000, 4983.000],  loss: 35.596615, mae: 14.217339, mean_q: 31.752390\n",
            "  5336/30000: episode: 891, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 2874.167 [518.000, 5049.000],  loss: 22.613947, mae: 13.563428, mean_q: 30.372551\n",
            "  5342/30000: episode: 892, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2427.000 [150.000, 4975.000],  loss: 28.411222, mae: 14.386491, mean_q: 32.651592\n",
            "  5348/30000: episode: 893, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2384.833 [1509.000, 3687.000],  loss: 18.245302, mae: 14.108250, mean_q: 31.659492\n",
            "  5354/30000: episode: 894, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 2805.833 [1540.000, 4464.000],  loss: 30.418512, mae: 14.775670, mean_q: 32.224979\n",
            "  5360/30000: episode: 895, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 631.667 [242.000, 1540.000],  loss: 33.812794, mae: 14.720216, mean_q: 32.284168\n",
            "  5366/30000: episode: 896, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3206.667 [1575.000, 4774.000],  loss: 19.930815, mae: 15.155692, mean_q: 33.044048\n",
            "  5372/30000: episode: 897, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2315.333 [1509.000, 3221.000],  loss: 38.535999, mae: 14.127507, mean_q: 30.933195\n",
            "  5378/30000: episode: 898, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 35.000, mean reward:  5.833 [-5.000, 40.000], mean action: 4452.000 [1202.000, 5557.000],  loss: 36.006748, mae: 14.759040, mean_q: 32.686428\n",
            "  5384/30000: episode: 899, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3332.333 [1491.000, 5666.000],  loss: 27.031357, mae: 14.172975, mean_q: 32.023106\n",
            "  5390/30000: episode: 900, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3493.167 [1596.000, 5109.000],  loss: 27.143143, mae: 14.526825, mean_q: 31.789225\n",
            "  5396/30000: episode: 901, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3603.500 [1540.000, 4397.000],  loss: 37.660522, mae: 14.354721, mean_q: 31.723532\n",
            "  5402/30000: episode: 902, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3976.500 [3470.000, 4486.000],  loss: 29.090921, mae: 14.428134, mean_q: 31.236406\n",
            "  5408/30000: episode: 903, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 4243.500 [1596.000, 5727.000],  loss: 18.302298, mae: 14.328305, mean_q: 32.142040\n",
            "  5414/30000: episode: 904, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4042.333 [2727.000, 5509.000],  loss: 45.706165, mae: 14.934266, mean_q: 32.299801\n",
            "  5420/30000: episode: 905, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2665.500 [43.000, 5666.000],  loss: 19.232298, mae: 13.706108, mean_q: 30.052843\n",
            "  5426/30000: episode: 906, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: 95.000, mean reward: 15.833 [ 0.000, 35.000], mean action: 3440.167 [1958.000, 4185.000],  loss: 24.632574, mae: 11.857182, mean_q: 26.846626\n",
            "  5432/30000: episode: 907, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 4126.333 [3921.000, 4983.000],  loss: 44.646866, mae: 14.577983, mean_q: 31.351503\n",
            "  5438/30000: episode: 908, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3430.333 [1204.000, 5727.000],  loss: 50.561932, mae: 15.309466, mean_q: 32.694324\n",
            "  5444/30000: episode: 909, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3911.167 [1540.000, 5360.000],  loss: 19.044987, mae: 15.957217, mean_q: 34.400127\n",
            "  5450/30000: episode: 910, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3833.667 [2195.000, 5539.000],  loss: 50.618000, mae: 14.047948, mean_q: 31.162474\n",
            "  5456/30000: episode: 911, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1586.667 [476.000, 3542.000],  loss: 40.025414, mae: 15.798814, mean_q: 34.012413\n",
            "  5462/30000: episode: 912, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2339.833 [806.000, 5666.000],  loss: 23.944693, mae: 14.216025, mean_q: 31.871866\n",
            "  5468/30000: episode: 913, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 70.000, mean reward: 11.667 [-5.000, 40.000], mean action: 3215.667 [1586.000, 5656.000],  loss: 26.911964, mae: 13.601283, mean_q: 30.756907\n",
            "  5474/30000: episode: 914, duration: 0.222s, episode steps:   6, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3033.167 [161.000, 5269.000],  loss: 38.894016, mae: 14.429082, mean_q: 32.633579\n",
            "  5480/30000: episode: 915, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 3802.167 [245.000, 5564.000],  loss: 26.597521, mae: 14.132052, mean_q: 31.214523\n",
            "  5486/30000: episode: 916, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3292.667 [1540.000, 5557.000],  loss: 21.488037, mae: 13.267150, mean_q: 29.721369\n",
            "  5492/30000: episode: 917, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3485.000 [1491.000, 5656.000],  loss: 29.728903, mae: 14.540657, mean_q: 31.251801\n",
            "  5498/30000: episode: 918, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2428.667 [502.000, 4404.000],  loss: 29.824453, mae: 12.562051, mean_q: 28.139521\n",
            "  5504/30000: episode: 919, duration: 0.235s, episode steps:   6, steps per second:  25, episode reward: 20.000, mean reward:  3.333 [ 0.000, 10.000], mean action: 2334.167 [1202.000, 3921.000],  loss: 33.351841, mae: 13.629922, mean_q: 30.250870\n",
            "  5510/30000: episode: 920, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3367.833 [1540.000, 4802.000],  loss: 26.690504, mae: 14.049195, mean_q: 31.241617\n",
            "  5516/30000: episode: 921, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4101.833 [1540.000, 5752.000],  loss: 29.983637, mae: 15.348155, mean_q: 33.834919\n",
            "  5522/30000: episode: 922, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3498.333 [34.000, 4945.000],  loss: 33.321682, mae: 14.433490, mean_q: 31.631910\n",
            "  5528/30000: episode: 923, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3692.000 [1356.000, 5579.000],  loss: 27.836205, mae: 14.903245, mean_q: 32.912701\n",
            "  5534/30000: episode: 924, duration: 0.266s, episode steps:   6, steps per second:  23, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3126.333 [1097.000, 5162.000],  loss: 33.651279, mae: 13.023296, mean_q: 29.433573\n",
            "  5540/30000: episode: 925, duration: 0.271s, episode steps:   6, steps per second:  22, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2836.500 [238.000, 3433.000],  loss: 35.021038, mae: 14.556763, mean_q: 31.992025\n",
            "  5546/30000: episode: 926, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: 60.000, mean reward: 10.000 [ 0.000, 25.000], mean action: 2364.500 [1540.000, 3075.000],  loss: 23.570162, mae: 13.198602, mean_q: 29.404953\n",
            "  5552/30000: episode: 927, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 1465.500 [502.000, 2367.000],  loss: 27.790680, mae: 12.994031, mean_q: 28.979233\n",
            "  5558/30000: episode: 928, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3907.833 [1520.000, 4717.000],  loss: 20.775017, mae: 13.850663, mean_q: 30.789375\n",
            "  5564/30000: episode: 929, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 45.000, mean reward:  7.500 [ 0.000, 45.000], mean action: 3124.500 [1613.000, 4404.000],  loss: 22.083807, mae: 12.667771, mean_q: 28.549881\n",
            "  5570/30000: episode: 930, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2307.500 [772.000, 3839.000],  loss: 29.188932, mae: 14.880330, mean_q: 33.152683\n",
            "  5576/30000: episode: 931, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3100.167 [200.000, 5005.000],  loss: 48.164654, mae: 14.619507, mean_q: 32.153324\n",
            "  5582/30000: episode: 932, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3498.667 [1387.000, 3921.000],  loss: 26.467171, mae: 15.100869, mean_q: 32.869736\n",
            "  5588/30000: episode: 933, duration: 0.188s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1783.500 [923.000, 3906.000],  loss: 36.296295, mae: 14.462463, mean_q: 31.997610\n",
            "  5594/30000: episode: 934, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3087.667 [209.000, 5688.000],  loss: 22.915987, mae: 14.022307, mean_q: 31.319284\n",
            "  5600/30000: episode: 935, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2860.000 [1537.000, 4783.000],  loss: 31.242470, mae: 13.911663, mean_q: 31.410690\n",
            "  5606/30000: episode: 936, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3031.000 [1199.000, 4420.000],  loss: 21.476046, mae: 14.137771, mean_q: 31.089869\n",
            "  5612/30000: episode: 937, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2908.167 [1032.000, 4717.000],  loss: 36.677105, mae: 13.913426, mean_q: 31.188025\n",
            "  5618/30000: episode: 938, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1961.500 [551.000, 3685.000],  loss: 34.498699, mae: 14.688678, mean_q: 32.953354\n",
            "  5624/30000: episode: 939, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1850.167 [1172.000, 3685.000],  loss: 28.693361, mae: 13.360326, mean_q: 29.938614\n",
            "  5630/30000: episode: 940, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4263.667 [3041.000, 5476.000],  loss: 26.036797, mae: 13.051061, mean_q: 29.029177\n",
            "  5636/30000: episode: 941, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2860.500 [489.000, 5129.000],  loss: 26.534033, mae: 14.941090, mean_q: 33.220039\n",
            "  5642/30000: episode: 942, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2137.833 [1520.000, 2698.000],  loss: 44.546703, mae: 13.836693, mean_q: 31.263037\n",
            "  5648/30000: episode: 943, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2919.000 [381.000, 4754.000],  loss: 34.467106, mae: 15.284551, mean_q: 33.011032\n",
            "  5654/30000: episode: 944, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2710.167 [1512.000, 5174.000],  loss: 26.828316, mae: 13.234924, mean_q: 29.213120\n",
            "  5660/30000: episode: 945, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2338.667 [678.000, 3138.000],  loss: 36.097492, mae: 14.640363, mean_q: 32.202248\n",
            "  5666/30000: episode: 946, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2550.167 [1387.000, 4717.000],  loss: 29.709391, mae: 13.866014, mean_q: 30.795563\n",
            "  5672/30000: episode: 947, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2865.000 [958.000, 3918.000],  loss: 42.651814, mae: 14.866405, mean_q: 32.618717\n",
            "  5678/30000: episode: 948, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2795.667 [1032.000, 4717.000],  loss: 31.369471, mae: 13.937249, mean_q: 31.467737\n",
            "  5684/30000: episode: 949, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1643.000 [958.000, 2766.000],  loss: 54.090900, mae: 13.956084, mean_q: 31.297094\n",
            "  5690/30000: episode: 950, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3235.667 [1387.000, 4683.000],  loss: 19.730782, mae: 14.580251, mean_q: 32.151516\n",
            "  5696/30000: episode: 951, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 40.000, mean reward:  6.667 [ 0.000, 20.000], mean action: 2848.500 [1554.000, 5656.000],  loss: 24.857819, mae: 13.955010, mean_q: 31.177980\n",
            "  5702/30000: episode: 952, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3237.333 [1055.000, 4592.000],  loss: 30.758951, mae: 13.814357, mean_q: 31.639442\n",
            "  5708/30000: episode: 953, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2975.000 [1032.000, 5005.000],  loss: 20.712425, mae: 14.629642, mean_q: 32.283176\n",
            "  5714/30000: episode: 954, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2391.833 [822.000, 5557.000],  loss: 33.677250, mae: 15.136174, mean_q: 33.155285\n",
            "  5720/30000: episode: 955, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1907.667 [84.000, 4807.000],  loss: 36.714428, mae: 14.166316, mean_q: 32.022552\n",
            "  5726/30000: episode: 956, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1936.167 [363.000, 3552.000],  loss: 33.560230, mae: 14.223629, mean_q: 31.614655\n",
            "  5732/30000: episode: 957, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 65.000, mean reward: 10.833 [ 0.000, 40.000], mean action: 3204.833 [1484.000, 5539.000],  loss: 29.658197, mae: 13.080917, mean_q: 29.332430\n",
            "  5738/30000: episode: 958, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2331.167 [1569.000, 3858.000],  loss: 35.680054, mae: 14.518985, mean_q: 32.062115\n",
            "  5744/30000: episode: 959, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 80.000, mean reward: 13.333 [ 0.000, 35.000], mean action: 2888.500 [1032.000, 5164.000],  loss: 34.326260, mae: 14.670486, mean_q: 33.240566\n",
            "  5750/30000: episode: 960, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2579.500 [1890.000, 2795.000],  loss: 44.827911, mae: 13.244615, mean_q: 30.272608\n",
            "  5756/30000: episode: 961, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3426.833 [1890.000, 5476.000],  loss: 26.061674, mae: 14.653577, mean_q: 32.501575\n",
            "  5762/30000: episode: 962, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 3247.333 [1575.000, 5187.000],  loss: 20.885536, mae: 14.689017, mean_q: 33.217693\n",
            "  5768/30000: episode: 963, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 75.000, mean reward: 12.500 [ 0.000, 40.000], mean action: 2806.333 [238.000, 4404.000],  loss: 36.644650, mae: 13.262138, mean_q: 30.528498\n",
            "  5774/30000: episode: 964, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1834.000 [1554.000, 1890.000],  loss: 31.233093, mae: 13.950629, mean_q: 31.652189\n",
            "  5780/30000: episode: 965, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [-5.000, 15.000], mean action: 3632.833 [1596.000, 5476.000],  loss: 22.232981, mae: 14.016057, mean_q: 31.223343\n",
            "  5786/30000: episode: 966, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 65.000, mean reward: 10.833 [ 0.000, 30.000], mean action: 2132.167 [852.000, 4696.000],  loss: 30.491249, mae: 15.207137, mean_q: 34.223522\n",
            "  5792/30000: episode: 967, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2098.167 [238.000, 5120.000],  loss: 28.383032, mae: 13.684048, mean_q: 30.717682\n",
            "  5798/30000: episode: 968, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 1890.000 [1890.000, 1890.000],  loss: 23.085815, mae: 14.629088, mean_q: 33.245243\n",
            "  5804/30000: episode: 969, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2256.333 [907.000, 2927.000],  loss: 23.632524, mae: 13.945140, mean_q: 31.442270\n",
            "  5810/30000: episode: 970, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2475.667 [808.000, 5373.000],  loss: 30.513336, mae: 13.858869, mean_q: 31.297119\n",
            "  5816/30000: episode: 971, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 2901.833 [1883.000, 4945.000],  loss: 26.244013, mae: 13.557220, mean_q: 30.995758\n",
            "  5822/30000: episode: 972, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 1890.000 [1890.000, 1890.000],  loss: 40.038280, mae: 14.100735, mean_q: 32.447231\n",
            "  5828/30000: episode: 973, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2917.167 [476.000, 5197.000],  loss: 45.394283, mae: 13.531555, mean_q: 31.470453\n",
            "  5834/30000: episode: 974, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3242.000 [1890.000, 3918.000],  loss: 29.313910, mae: 13.661790, mean_q: 31.587942\n",
            "  5840/30000: episode: 975, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2218.000 [1890.000, 3858.000],  loss: 19.914740, mae: 14.093049, mean_q: 32.386841\n",
            "  5846/30000: episode: 976, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2126.333 [1167.000, 2715.000],  loss: 23.492414, mae: 14.215183, mean_q: 33.142811\n",
            "  5852/30000: episode: 977, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2352.833 [476.000, 5202.000],  loss: 43.147388, mae: 13.395547, mean_q: 31.203817\n",
            "  5858/30000: episode: 978, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3163.833 [461.000, 5718.000],  loss: 30.238686, mae: 13.706250, mean_q: 30.931503\n",
            "  5864/30000: episode: 979, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3983.000 [1890.000, 5327.000],  loss: 47.411556, mae: 15.006328, mean_q: 33.776840\n",
            "  5870/30000: episode: 980, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1950.667 [711.000, 3202.000],  loss: 37.600861, mae: 13.986092, mean_q: 31.703650\n",
            "  5876/30000: episode: 981, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2309.167 [1224.000, 4464.000],  loss: 41.541615, mae: 14.250004, mean_q: 32.058064\n",
            "  5882/30000: episode: 982, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2521.833 [1241.000, 4499.000],  loss: 36.086105, mae: 14.626958, mean_q: 32.950375\n",
            "  5888/30000: episode: 983, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3870.167 [140.000, 4840.000],  loss: 25.313738, mae: 13.318156, mean_q: 30.190729\n",
            "  5894/30000: episode: 984, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3362.167 [1628.000, 5557.000],  loss: 26.002405, mae: 14.999656, mean_q: 32.674816\n",
            "  5900/30000: episode: 985, duration: 0.226s, episode steps:   6, steps per second:  27, episode reward: 30.000, mean reward:  5.000 [ 0.000, 30.000], mean action: 3304.000 [1788.000, 5482.000],  loss: 26.427048, mae: 15.105804, mean_q: 33.121613\n",
            "  5906/30000: episode: 986, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3349.167 [547.000, 5593.000],  loss: 26.990751, mae: 14.244702, mean_q: 31.995483\n",
            "  5912/30000: episode: 987, duration: 0.272s, episode steps:   6, steps per second:  22, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2031.333 [238.000, 3442.000],  loss: 26.492195, mae: 13.552499, mean_q: 30.597397\n",
            "  5918/30000: episode: 988, duration: 0.241s, episode steps:   6, steps per second:  25, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3617.333 [822.000, 5679.000],  loss: 21.502884, mae: 13.415254, mean_q: 30.549103\n",
            "  5924/30000: episode: 989, duration: 0.239s, episode steps:   6, steps per second:  25, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3379.500 [2619.000, 4191.000],  loss: 29.369112, mae: 13.589309, mean_q: 30.704981\n",
            "  5930/30000: episode: 990, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4631.500 [3586.000, 5719.000],  loss: 24.214376, mae: 13.453200, mean_q: 30.045366\n",
            "  5936/30000: episode: 991, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3613.333 [2484.000, 5754.000],  loss: 30.645983, mae: 14.234169, mean_q: 31.701195\n",
            "  5942/30000: episode: 992, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4040.167 [1980.000, 5476.000],  loss: 26.707098, mae: 13.689289, mean_q: 31.021589\n",
            "  5948/30000: episode: 993, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3824.833 [404.000, 5637.000],  loss: 19.243250, mae: 14.236610, mean_q: 31.203829\n",
            "  5954/30000: episode: 994, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3228.833 [1530.000, 4727.000],  loss: 32.247860, mae: 13.439663, mean_q: 29.975878\n",
            "  5960/30000: episode: 995, duration: 0.265s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4340.833 [2620.000, 5299.000],  loss: 40.400387, mae: 14.294194, mean_q: 31.391775\n",
            "  5966/30000: episode: 996, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3767.500 [246.000, 5204.000],  loss: 27.579088, mae: 13.913003, mean_q: 30.890450\n",
            "  5972/30000: episode: 997, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2010.667 [385.000, 5656.000],  loss: 30.342123, mae: 14.425331, mean_q: 31.979101\n",
            "  5978/30000: episode: 998, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3500.833 [1076.000, 5340.000],  loss: 23.486267, mae: 13.824718, mean_q: 30.852791\n",
            "  5984/30000: episode: 999, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2415.500 [385.000, 4531.000],  loss: 35.296299, mae: 13.845703, mean_q: 30.816513\n",
            "  5990/30000: episode: 1000, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 2339.333 [476.000, 4632.000],  loss: 26.137695, mae: 14.086319, mean_q: 31.687696\n",
            "  5996/30000: episode: 1001, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2307.667 [733.000, 4632.000],  loss: 29.028196, mae: 12.614925, mean_q: 29.003075\n",
            "  6002/30000: episode: 1002, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1535.333 [803.000, 2570.000],  loss: 17.260353, mae: 13.050354, mean_q: 29.423111\n",
            "  6008/30000: episode: 1003, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 3528.167 [1596.000, 5718.000],  loss: 31.097799, mae: 13.581573, mean_q: 30.819525\n",
            "  6014/30000: episode: 1004, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3132.333 [1316.000, 5557.000],  loss: 30.362505, mae: 14.103499, mean_q: 31.867758\n",
            "  6020/30000: episode: 1005, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2643.667 [205.000, 5476.000],  loss: 27.822329, mae: 14.665890, mean_q: 33.079002\n",
            "  6026/30000: episode: 1006, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 30.000, mean reward:  5.000 [ 0.000, 30.000], mean action: 2906.333 [803.000, 4632.000],  loss: 42.138157, mae: 14.349232, mean_q: 32.195179\n",
            "  6032/30000: episode: 1007, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2489.333 [803.000, 4797.000],  loss: 38.472004, mae: 14.400983, mean_q: 31.967886\n",
            "  6038/30000: episode: 1008, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2753.833 [205.000, 4954.000],  loss: 27.332146, mae: 14.603770, mean_q: 32.251293\n",
            "  6044/30000: episode: 1009, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2354.833 [267.000, 4632.000],  loss: 22.379372, mae: 14.032716, mean_q: 30.889917\n",
            "  6050/30000: episode: 1010, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1856.167 [476.000, 2852.000],  loss: 28.771704, mae: 14.293473, mean_q: 32.105747\n",
            "  6056/30000: episode: 1011, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3296.000 [487.000, 5292.000],  loss: 22.757254, mae: 13.562183, mean_q: 29.958689\n",
            "  6062/30000: episode: 1012, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3213.667 [608.000, 4632.000],  loss: 22.387726, mae: 14.552238, mean_q: 32.388306\n",
            "  6068/30000: episode: 1013, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2717.000 [1276.000, 4632.000],  loss: 24.927261, mae: 14.562539, mean_q: 32.230103\n",
            "  6074/30000: episode: 1014, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3066.333 [4.000, 5703.000],  loss: 23.999670, mae: 15.355973, mean_q: 33.492863\n",
            "  6080/30000: episode: 1015, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2552.167 [1484.000, 3918.000],  loss: 26.101351, mae: 13.401498, mean_q: 29.839712\n",
            "  6086/30000: episode: 1016, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2459.167 [1282.000, 4745.000],  loss: 27.161173, mae: 13.813670, mean_q: 30.793684\n",
            "  6092/30000: episode: 1017, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2115.500 [393.000, 3493.000],  loss: 39.094776, mae: 13.803574, mean_q: 30.407019\n",
            "  6098/30000: episode: 1018, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2331.500 [358.000, 5743.000],  loss: 31.822180, mae: 14.802273, mean_q: 32.269066\n",
            "  6104/30000: episode: 1019, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2120.667 [328.000, 3830.000],  loss: 25.962908, mae: 14.155194, mean_q: 31.651491\n",
            "  6110/30000: episode: 1020, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2881.667 [522.000, 4365.000],  loss: 29.958193, mae: 14.248307, mean_q: 31.919512\n",
            "  6116/30000: episode: 1021, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1091.000 [547.000, 3113.000],  loss: 16.595140, mae: 13.308006, mean_q: 30.081545\n",
            "  6122/30000: episode: 1022, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3342.167 [3113.000, 3388.000],  loss: 32.900410, mae: 13.378639, mean_q: 30.362116\n",
            "  6128/30000: episode: 1023, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1874.167 [292.000, 3680.000],  loss: 21.491041, mae: 13.773640, mean_q: 31.057808\n",
            "  6134/30000: episode: 1024, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3406.333 [1640.000, 5259.000],  loss: 44.381458, mae: 13.356347, mean_q: 30.327637\n",
            "  6140/30000: episode: 1025, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: 20.000, mean reward:  3.333 [-5.000, 25.000], mean action: 3013.333 [1290.000, 5164.000],  loss: 19.090574, mae: 12.989934, mean_q: 29.893877\n",
            "  6146/30000: episode: 1026, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1784.833 [393.000, 3921.000],  loss: 33.901058, mae: 14.652617, mean_q: 32.213802\n",
            "  6152/30000: episode: 1027, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2901.833 [507.000, 4719.000],  loss: 22.349707, mae: 13.355141, mean_q: 30.513329\n",
            "  6158/30000: episode: 1028, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2263.000 [145.000, 3525.000],  loss: 27.983551, mae: 13.699468, mean_q: 30.949392\n",
            "  6164/30000: episode: 1029, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1868.500 [491.000, 3388.000],  loss: 26.808340, mae: 13.934957, mean_q: 31.034834\n",
            "  6170/30000: episode: 1030, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 45.000, mean reward:  7.500 [ 0.000, 45.000], mean action: 3322.333 [547.000, 5554.000],  loss: 45.128826, mae: 14.913760, mean_q: 33.337025\n",
            "  6176/30000: episode: 1031, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2454.500 [126.000, 5707.000],  loss: 30.530411, mae: 15.150350, mean_q: 33.092388\n",
            "  6182/30000: episode: 1032, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 2257.667 [1571.000, 3202.000],  loss: 62.549625, mae: 14.542206, mean_q: 32.628971\n",
            "  6188/30000: episode: 1033, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2572.500 [126.000, 5152.000],  loss: 22.617249, mae: 13.640453, mean_q: 30.798231\n",
            "  6194/30000: episode: 1034, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2368.167 [427.000, 5637.000],  loss: 23.959845, mae: 13.769806, mean_q: 31.703081\n",
            "  6200/30000: episode: 1035, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3563.667 [1799.000, 5608.000],  loss: 22.893957, mae: 13.973687, mean_q: 31.233065\n",
            "  6206/30000: episode: 1036, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3059.000 [328.000, 5743.000],  loss: 29.278076, mae: 14.837964, mean_q: 33.020618\n",
            "  6212/30000: episode: 1037, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2431.500 [442.000, 4191.000],  loss: 34.464577, mae: 14.400771, mean_q: 32.430290\n",
            "  6218/30000: episode: 1038, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1784.833 [547.000, 3157.000],  loss: 39.882488, mae: 13.729848, mean_q: 30.877113\n",
            "  6224/30000: episode: 1039, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3383.667 [2760.000, 4298.000],  loss: 51.437790, mae: 15.254951, mean_q: 34.156628\n",
            "  6230/30000: episode: 1040, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1712.333 [390.000, 3943.000],  loss: 30.967642, mae: 14.576538, mean_q: 32.184826\n",
            "  6236/30000: episode: 1041, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 30.000, mean reward:  5.000 [-5.000, 20.000], mean action: 1571.500 [238.000, 3797.000],  loss: 28.132833, mae: 14.176800, mean_q: 32.291279\n",
            "  6242/30000: episode: 1042, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3474.833 [2540.000, 4516.000],  loss: 26.087252, mae: 14.175362, mean_q: 31.619499\n",
            "  6248/30000: episode: 1043, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3629.833 [1016.000, 5743.000],  loss: 19.914202, mae: 14.139569, mean_q: 31.893297\n",
            "  6254/30000: episode: 1044, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2588.167 [678.000, 4683.000],  loss: 22.199270, mae: 13.856514, mean_q: 31.383554\n",
            "  6260/30000: episode: 1045, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4312.500 [3113.000, 5063.000],  loss: 30.093576, mae: 14.334427, mean_q: 32.201839\n",
            "  6266/30000: episode: 1046, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2729.167 [238.000, 4719.000],  loss: 37.079227, mae: 13.449279, mean_q: 30.880783\n",
            "  6272/30000: episode: 1047, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1751.500 [326.000, 4630.000],  loss: 21.680784, mae: 14.260845, mean_q: 32.649967\n",
            "  6278/30000: episode: 1048, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2697.167 [1128.000, 5299.000],  loss: 21.961668, mae: 13.830056, mean_q: 31.602079\n",
            "  6284/30000: episode: 1049, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3469.667 [2540.000, 4802.000],  loss: 17.803671, mae: 13.530396, mean_q: 31.309790\n",
            "  6290/30000: episode: 1050, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3123.333 [133.000, 5338.000],  loss: 26.931129, mae: 14.761031, mean_q: 32.471104\n",
            "  6296/30000: episode: 1051, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2358.667 [741.000, 4489.000],  loss: 33.487072, mae: 14.905517, mean_q: 33.182873\n",
            "  6302/30000: episode: 1052, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3345.667 [2143.000, 4444.000],  loss: 28.084480, mae: 13.004139, mean_q: 29.945557\n",
            "  6308/30000: episode: 1053, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 35.000, mean reward:  5.833 [ 0.000, 35.000], mean action: 3178.500 [859.000, 4460.000],  loss: 28.834066, mae: 13.545532, mean_q: 30.728424\n",
            "  6314/30000: episode: 1054, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3329.667 [1202.000, 5649.000],  loss: 32.708439, mae: 14.126079, mean_q: 31.715027\n",
            "  6320/30000: episode: 1055, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1939.167 [238.000, 4683.000],  loss: 35.659054, mae: 14.497111, mean_q: 32.024364\n",
            "  6326/30000: episode: 1056, duration: 0.238s, episode steps:   6, steps per second:  25, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 1397.167 [547.000, 3285.000],  loss: 21.192387, mae: 13.666573, mean_q: 30.710869\n",
            "  6332/30000: episode: 1057, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1691.333 [1137.000, 2277.000],  loss: 24.066950, mae: 14.196031, mean_q: 32.568756\n",
            "  6338/30000: episode: 1058, duration: 0.236s, episode steps:   6, steps per second:  25, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2590.667 [2277.000, 4159.000],  loss: 24.229151, mae: 14.549080, mean_q: 32.544373\n",
            "  6344/30000: episode: 1059, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward: -10.000, mean reward: -1.667 [-5.000,  5.000], mean action: 4796.000 [2540.000, 5338.000],  loss: 41.658363, mae: 14.381058, mean_q: 32.487701\n",
            "  6350/30000: episode: 1060, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2708.167 [288.000, 4613.000],  loss: 35.342136, mae: 14.237967, mean_q: 32.066029\n",
            "  6356/30000: episode: 1061, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2057.833 [100.000, 4316.000],  loss: 36.180939, mae: 15.095521, mean_q: 33.292072\n",
            "  6362/30000: episode: 1062, duration: 0.239s, episode steps:   6, steps per second:  25, episode reward: 110.000, mean reward: 18.333 [ 0.000, 35.000], mean action: 3216.333 [238.000, 5539.000],  loss: 31.224648, mae: 14.272717, mean_q: 32.572899\n",
            "  6368/30000: episode: 1063, duration: 0.240s, episode steps:   6, steps per second:  25, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 2236.500 [238.000, 4191.000],  loss: 25.116312, mae: 12.663028, mean_q: 29.101572\n",
            "  6374/30000: episode: 1064, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2384.167 [1019.000, 4185.000],  loss: 20.561323, mae: 14.082923, mean_q: 31.577139\n",
            "  6380/30000: episode: 1065, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2991.667 [238.000, 5283.000],  loss: 48.661652, mae: 13.359707, mean_q: 30.157654\n",
            "  6386/30000: episode: 1066, duration: 0.257s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2771.667 [238.000, 5299.000],  loss: 34.674839, mae: 13.739755, mean_q: 31.341751\n",
            "  6392/30000: episode: 1067, duration: 0.264s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2694.000 [43.000, 5649.000],  loss: 32.085754, mae: 13.784844, mean_q: 31.077126\n",
            "  6398/30000: episode: 1068, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3134.167 [357.000, 4208.000],  loss: 23.359970, mae: 12.143125, mean_q: 28.058836\n",
            "  6404/30000: episode: 1069, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [-5.000,  5.000], mean action: 3635.500 [709.000, 5338.000],  loss: 42.566708, mae: 14.142207, mean_q: 31.743505\n",
            "  6410/30000: episode: 1070, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2630.500 [10.000, 5338.000],  loss: 47.188797, mae: 13.736202, mean_q: 30.583567\n",
            "  6416/30000: episode: 1071, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 1334.167 [238.000, 3469.000],  loss: 35.790306, mae: 13.351733, mean_q: 30.487551\n",
            "  6422/30000: episode: 1072, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3633.000 [1042.000, 5449.000],  loss: 21.763941, mae: 13.208389, mean_q: 30.088120\n",
            "  6428/30000: episode: 1073, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3468.167 [678.000, 4945.000],  loss: 41.879284, mae: 14.905407, mean_q: 32.462521\n",
            "  6434/30000: episode: 1074, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 40.000, mean reward:  6.667 [ 0.000, 40.000], mean action: 3643.000 [946.000, 5340.000],  loss: 27.329962, mae: 15.816325, mean_q: 34.832851\n",
            "  6440/30000: episode: 1075, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2166.500 [1530.000, 2852.000],  loss: 44.196095, mae: 14.466878, mean_q: 31.824936\n",
            "  6446/30000: episode: 1076, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3801.167 [1340.000, 5656.000],  loss: 35.658951, mae: 14.343910, mean_q: 32.181873\n",
            "  6452/30000: episode: 1077, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3065.333 [328.000, 5148.000],  loss: 42.187241, mae: 13.711007, mean_q: 30.908014\n",
            "  6458/30000: episode: 1078, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2713.833 [238.000, 5550.000],  loss: 21.158638, mae: 14.371293, mean_q: 31.970276\n",
            "  6464/30000: episode: 1079, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2731.667 [323.000, 5028.000],  loss: 20.938173, mae: 13.527778, mean_q: 30.461752\n",
            "  6470/30000: episode: 1080, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2899.833 [1042.000, 4372.000],  loss: 56.090057, mae: 13.977531, mean_q: 31.488884\n",
            "  6476/30000: episode: 1081, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2738.000 [966.000, 5187.000],  loss: 20.850412, mae: 12.301251, mean_q: 27.781830\n",
            "  6482/30000: episode: 1082, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2689.500 [1132.000, 5640.000],  loss: 24.783216, mae: 14.550754, mean_q: 32.578678\n",
            "  6488/30000: episode: 1083, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2222.500 [430.000, 4945.000],  loss: 22.797411, mae: 14.225942, mean_q: 32.390125\n",
            "  6494/30000: episode: 1084, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 3136.167 [816.000, 5736.000],  loss: 13.919304, mae: 13.999976, mean_q: 31.466284\n",
            "  6500/30000: episode: 1085, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1697.333 [249.000, 3274.000],  loss: 21.388506, mae: 14.595764, mean_q: 32.182869\n",
            "  6506/30000: episode: 1086, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 1738.000 [238.000, 4438.000],  loss: 21.848280, mae: 13.249408, mean_q: 30.589060\n",
            "  6512/30000: episode: 1087, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2563.167 [249.000, 5148.000],  loss: 33.162601, mae: 14.344470, mean_q: 32.443161\n",
            "  6518/30000: episode: 1088, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3316.500 [1461.000, 5736.000],  loss: 29.723160, mae: 13.312800, mean_q: 29.799253\n",
            "  6524/30000: episode: 1089, duration: 0.183s, episode steps:   6, steps per second:  33, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2165.500 [140.000, 5187.000],  loss: 28.716970, mae: 13.908101, mean_q: 31.862143\n",
            "  6530/30000: episode: 1090, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2174.333 [238.000, 3643.000],  loss: 30.410196, mae: 14.674260, mean_q: 33.007572\n",
            "  6536/30000: episode: 1091, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 4220.833 [2237.000, 5473.000],  loss: 33.261608, mae: 13.251588, mean_q: 30.576797\n",
            "  6542/30000: episode: 1092, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: 40.000, mean reward:  6.667 [-5.000, 25.000], mean action: 1371.000 [238.000, 4667.000],  loss: 38.502426, mae: 14.468436, mean_q: 32.569599\n",
            "  6548/30000: episode: 1093, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 105.000, mean reward: 17.500 [ 0.000, 35.000], mean action: 3524.833 [1554.000, 4683.000],  loss: 35.723137, mae: 13.700962, mean_q: 30.933187\n",
            "  6554/30000: episode: 1094, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2645.500 [238.000, 4175.000],  loss: 14.942223, mae: 13.345838, mean_q: 31.008024\n",
            "  6560/30000: episode: 1095, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2588.000 [1041.000, 5679.000],  loss: 28.954386, mae: 13.813359, mean_q: 31.600576\n",
            "  6566/30000: episode: 1096, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2538.833 [429.000, 3951.000],  loss: 22.468790, mae: 15.334575, mean_q: 33.940670\n",
            "  6572/30000: episode: 1097, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 2717.000 [183.000, 5471.000],  loss: 52.710011, mae: 14.542500, mean_q: 32.131363\n",
            "  6578/30000: episode: 1098, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3474.500 [1988.000, 5282.000],  loss: 32.036694, mae: 14.557854, mean_q: 32.333298\n",
            "  6584/30000: episode: 1099, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2961.833 [53.000, 3858.000],  loss: 27.713739, mae: 13.564660, mean_q: 30.830719\n",
            "  6590/30000: episode: 1100, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1991.000 [328.000, 3552.000],  loss: 30.588913, mae: 14.312545, mean_q: 31.369509\n",
            "  6596/30000: episode: 1101, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3139.500 [430.000, 4683.000],  loss: 53.210690, mae: 15.000493, mean_q: 33.541920\n",
            "  6602/30000: episode: 1102, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2977.667 [1136.000, 4333.000],  loss: 24.640114, mae: 13.181426, mean_q: 29.922577\n",
            "  6608/30000: episode: 1103, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [-5.000, 15.000], mean action: 4469.500 [3567.000, 5656.000],  loss: 33.878807, mae: 15.020614, mean_q: 33.407475\n",
            "  6614/30000: episode: 1104, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2113.833 [727.000, 5070.000],  loss: 27.753378, mae: 13.115634, mean_q: 31.076874\n",
            "  6620/30000: episode: 1105, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3117.500 [1554.000, 5557.000],  loss: 23.334524, mae: 13.729003, mean_q: 32.204449\n",
            "  6626/30000: episode: 1106, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 40.000, mean reward:  6.667 [ 0.000, 20.000], mean action: 2516.667 [1554.000, 4683.000],  loss: 31.706152, mae: 14.683227, mean_q: 33.881657\n",
            "  6632/30000: episode: 1107, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2036.667 [126.000, 5006.000],  loss: 27.660528, mae: 15.095248, mean_q: 34.393841\n",
            "  6638/30000: episode: 1108, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3100.667 [491.000, 5628.000],  loss: 18.676630, mae: 14.123530, mean_q: 32.746941\n",
            "  6644/30000: episode: 1109, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2163.167 [100.000, 4379.000],  loss: 34.688633, mae: 14.539575, mean_q: 32.777012\n",
            "  6650/30000: episode: 1110, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 1736.667 [69.000, 3100.000],  loss: 49.524166, mae: 13.363441, mean_q: 31.114151\n",
            "  6656/30000: episode: 1111, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2444.500 [1554.000, 4683.000],  loss: 19.685553, mae: 13.968720, mean_q: 32.586365\n",
            "  6662/30000: episode: 1112, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: 75.000, mean reward: 12.500 [-5.000, 35.000], mean action: 2672.167 [1554.000, 4203.000],  loss: 26.025251, mae: 14.301293, mean_q: 32.511539\n",
            "  6668/30000: episode: 1113, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [-5.000, 20.000], mean action: 1543.500 [476.000, 4683.000],  loss: 48.991100, mae: 14.047806, mean_q: 32.202396\n",
            "  6674/30000: episode: 1114, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 4634.833 [1554.000, 5557.000],  loss: 24.887613, mae: 14.478683, mean_q: 33.423443\n",
            "  6680/30000: episode: 1115, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3252.167 [100.000, 5707.000],  loss: 34.612137, mae: 14.591576, mean_q: 33.325790\n",
            "  6686/30000: episode: 1116, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3412.333 [784.000, 4486.000],  loss: 20.369734, mae: 14.550538, mean_q: 32.590122\n",
            "  6692/30000: episode: 1117, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward: 45.000, mean reward:  7.500 [ 0.000, 45.000], mean action: 3876.500 [2752.000, 4893.000],  loss: 29.960787, mae: 13.496651, mean_q: 31.219614\n",
            "  6698/30000: episode: 1118, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3245.167 [755.000, 5338.000],  loss: 29.707743, mae: 13.548610, mean_q: 31.169424\n",
            "  6704/30000: episode: 1119, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2332.000 [811.000, 3656.000],  loss: 24.635340, mae: 14.129178, mean_q: 31.868002\n",
            "  6710/30000: episode: 1120, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2852.000 [844.000, 4185.000],  loss: 36.856495, mae: 13.726134, mean_q: 31.177778\n",
            "  6716/30000: episode: 1121, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3795.333 [1630.000, 5476.000],  loss: 18.980204, mae: 14.198299, mean_q: 32.459454\n",
            "  6722/30000: episode: 1122, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3509.167 [1632.000, 5396.000],  loss: 24.727823, mae: 14.601827, mean_q: 33.192310\n",
            "  6728/30000: episode: 1123, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2015.667 [53.000, 3804.000],  loss: 38.891338, mae: 14.242185, mean_q: 32.490433\n",
            "  6734/30000: episode: 1124, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3670.833 [1554.000, 5743.000],  loss: 31.242218, mae: 16.121365, mean_q: 36.005447\n",
            "  6740/30000: episode: 1125, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2650.167 [249.000, 3625.000],  loss: 36.981197, mae: 14.094570, mean_q: 32.024094\n",
            "  6746/30000: episode: 1126, duration: 0.183s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2723.667 [249.000, 4203.000],  loss: 29.641663, mae: 14.394176, mean_q: 33.011398\n",
            "  6752/30000: episode: 1127, duration: 0.266s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1824.500 [238.000, 3265.000],  loss: 21.565928, mae: 13.325400, mean_q: 31.498220\n",
            "  6758/30000: episode: 1128, duration: 0.259s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2014.667 [90.000, 4847.000],  loss: 46.424511, mae: 14.141667, mean_q: 32.632542\n",
            "  6764/30000: episode: 1129, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3621.000 [807.000, 5506.000],  loss: 31.315186, mae: 13.557762, mean_q: 31.026205\n",
            "  6770/30000: episode: 1130, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3056.333 [249.000, 5554.000],  loss: 33.267780, mae: 13.173276, mean_q: 30.853670\n",
            "  6776/30000: episode: 1131, duration: 0.267s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2084.833 [4.000, 3598.000],  loss: 23.867287, mae: 14.793170, mean_q: 33.766651\n",
            "  6782/30000: episode: 1132, duration: 0.257s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2293.667 [249.000, 4979.000],  loss: 32.784100, mae: 14.832141, mean_q: 33.487873\n",
            "  6788/30000: episode: 1133, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward: 20.000, mean reward:  3.333 [-5.000, 20.000], mean action: 2713.167 [238.000, 4404.000],  loss: 22.480597, mae: 13.319626, mean_q: 31.388403\n",
            "  6794/30000: episode: 1134, duration: 0.262s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2066.833 [428.000, 4122.000],  loss: 34.773930, mae: 14.351376, mean_q: 32.797039\n",
            "  6800/30000: episode: 1135, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward: 30.000, mean reward:  5.000 [ 0.000, 20.000], mean action: 2150.500 [249.000, 3669.000],  loss: 16.052675, mae: 13.997283, mean_q: 32.029636\n",
            "  6806/30000: episode: 1136, duration: 0.264s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3636.500 [1380.000, 5334.000],  loss: 38.074238, mae: 13.991757, mean_q: 32.441212\n",
            "  6812/30000: episode: 1137, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3824.333 [629.000, 5557.000],  loss: 16.156557, mae: 13.255623, mean_q: 30.721918\n",
            "  6818/30000: episode: 1138, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2778.833 [1596.000, 5283.000],  loss: 27.560087, mae: 13.858933, mean_q: 31.544374\n",
            "  6824/30000: episode: 1139, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3216.500 [175.000, 5579.000],  loss: 24.679392, mae: 13.930440, mean_q: 31.979208\n",
            "  6830/30000: episode: 1140, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 30.000, mean reward:  5.000 [ 0.000, 30.000], mean action: 3339.167 [323.000, 5283.000],  loss: 32.237003, mae: 14.974675, mean_q: 33.603718\n",
            "  6836/30000: episode: 1141, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3324.333 [90.000, 4486.000],  loss: 35.129322, mae: 13.860204, mean_q: 31.445223\n",
            "  6842/30000: episode: 1142, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 4672.167 [3951.000, 5640.000],  loss: 30.886339, mae: 14.110957, mean_q: 32.122997\n",
            "  6848/30000: episode: 1143, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3277.167 [430.000, 5028.000],  loss: 20.136274, mae: 15.401021, mean_q: 33.865749\n",
            "  6854/30000: episode: 1144, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2264.833 [14.000, 4363.000],  loss: 36.394039, mae: 13.899738, mean_q: 31.752279\n",
            "  6860/30000: episode: 1145, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3859.000 [2415.000, 4750.000],  loss: 30.217943, mae: 14.398791, mean_q: 32.397873\n",
            "  6866/30000: episode: 1146, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2223.500 [90.000, 4683.000],  loss: 30.186674, mae: 14.523980, mean_q: 32.453049\n",
            "  6872/30000: episode: 1147, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 3579.833 [1258.000, 5028.000],  loss: 32.202694, mae: 15.221681, mean_q: 34.358196\n",
            "  6878/30000: episode: 1148, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3924.500 [2532.000, 4203.000],  loss: 27.846046, mae: 15.092250, mean_q: 34.412613\n",
            "  6884/30000: episode: 1149, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2301.000 [607.000, 4913.000],  loss: 24.552782, mae: 14.088334, mean_q: 32.280964\n",
            "  6890/30000: episode: 1150, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2676.333 [245.000, 5028.000],  loss: 36.470333, mae: 15.031306, mean_q: 33.434021\n",
            "  6896/30000: episode: 1151, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3443.833 [1160.000, 5148.000],  loss: 26.402809, mae: 14.111958, mean_q: 31.569593\n",
            "  6902/30000: episode: 1152, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3746.167 [2277.000, 5121.000],  loss: 19.727751, mae: 13.854072, mean_q: 31.714869\n",
            "  6908/30000: episode: 1153, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2015.500 [331.000, 4122.000],  loss: 41.631676, mae: 14.392371, mean_q: 31.861021\n",
            "  6914/30000: episode: 1154, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3082.500 [1153.000, 4683.000],  loss: 29.754438, mae: 13.178388, mean_q: 30.066488\n",
            "  6920/30000: episode: 1155, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3451.000 [1202.000, 5608.000],  loss: 18.953447, mae: 13.179919, mean_q: 30.228636\n",
            "  6926/30000: episode: 1156, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3827.500 [1586.000, 5651.000],  loss: 24.722933, mae: 13.904309, mean_q: 31.323130\n",
            "  6932/30000: episode: 1157, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3599.500 [1722.000, 5603.000],  loss: 30.453474, mae: 12.832619, mean_q: 29.261438\n",
            "  6938/30000: episode: 1158, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2988.500 [44.000, 5656.000],  loss: 29.871017, mae: 13.646554, mean_q: 30.692703\n",
            "  6944/30000: episode: 1159, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3867.000 [814.000, 5653.000],  loss: 15.593750, mae: 14.085576, mean_q: 31.882078\n",
            "  6950/30000: episode: 1160, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3611.167 [271.000, 5742.000],  loss: 22.778412, mae: 14.017601, mean_q: 32.011883\n",
            "  6956/30000: episode: 1161, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 20.000, mean reward:  3.333 [ 0.000, 10.000], mean action: 2453.000 [1202.000, 3804.000],  loss: 29.525759, mae: 14.231357, mean_q: 31.908682\n",
            "  6962/30000: episode: 1162, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2818.333 [1472.000, 4363.000],  loss: 38.468281, mae: 13.393199, mean_q: 30.759195\n",
            "  6968/30000: episode: 1163, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3187.167 [590.000, 5239.000],  loss: 30.384918, mae: 13.486569, mean_q: 31.106318\n",
            "  6974/30000: episode: 1164, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3444.000 [429.000, 5121.000],  loss: 28.865877, mae: 13.134480, mean_q: 29.818657\n",
            "  6980/30000: episode: 1165, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3282.833 [1242.000, 5129.000],  loss: 30.407511, mae: 13.693066, mean_q: 30.491751\n",
            "  6986/30000: episode: 1166, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 3363.833 [1160.000, 5405.000],  loss: 15.291862, mae: 13.229790, mean_q: 30.151163\n",
            "  6992/30000: episode: 1167, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3637.667 [2688.000, 5721.000],  loss: 20.227144, mae: 14.666986, mean_q: 32.414276\n",
            "  6998/30000: episode: 1168, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3212.000 [990.000, 5148.000],  loss: 25.802511, mae: 13.317550, mean_q: 30.444582\n",
            "  7004/30000: episode: 1169, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2372.833 [852.000, 3608.000],  loss: 24.247202, mae: 13.936867, mean_q: 31.202637\n",
            "  7010/30000: episode: 1170, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 3112.333 [147.000, 5622.000],  loss: 66.315605, mae: 14.877594, mean_q: 33.368053\n",
            "  7016/30000: episode: 1171, duration: 0.185s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4670.167 [2166.000, 5668.000],  loss: 28.808762, mae: 14.807137, mean_q: 32.781975\n",
            "  7022/30000: episode: 1172, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3157.500 [937.000, 5619.000],  loss: 42.179951, mae: 13.926475, mean_q: 31.126577\n",
            "  7028/30000: episode: 1173, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3121.500 [333.000, 4893.000],  loss: 30.035284, mae: 14.295035, mean_q: 32.458714\n",
            "  7034/30000: episode: 1174, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3702.667 [1515.000, 5121.000],  loss: 23.567549, mae: 14.284930, mean_q: 32.401814\n",
            "  7040/30000: episode: 1175, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3190.500 [271.000, 3804.000],  loss: 26.274826, mae: 13.999648, mean_q: 31.973780\n",
            "  7046/30000: episode: 1176, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3051.833 [494.000, 5538.000],  loss: 32.556389, mae: 14.572693, mean_q: 32.173008\n",
            "  7052/30000: episode: 1177, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2943.833 [1083.000, 5121.000],  loss: 43.970547, mae: 14.399193, mean_q: 32.245846\n",
            "  7058/30000: episode: 1178, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2820.167 [535.000, 4918.000],  loss: 36.649426, mae: 14.323045, mean_q: 31.603195\n",
            "  7064/30000: episode: 1179, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2398.000 [269.000, 5396.000],  loss: 21.606636, mae: 14.084385, mean_q: 31.091362\n",
            "  7070/30000: episode: 1180, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1881.333 [69.000, 3221.000],  loss: 22.887802, mae: 14.258766, mean_q: 31.840248\n",
            "  7076/30000: episode: 1181, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3026.833 [358.000, 4831.000],  loss: 34.782024, mae: 14.258458, mean_q: 31.974722\n",
            "  7082/30000: episode: 1182, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [-5.000, 15.000], mean action: 1902.000 [1083.000, 2448.000],  loss: 47.497578, mae: 14.606950, mean_q: 32.400890\n",
            "  7088/30000: episode: 1183, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2007.333 [43.000, 3996.000],  loss: 26.940882, mae: 15.170461, mean_q: 32.920704\n",
            "  7094/30000: episode: 1184, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3386.833 [2356.000, 4464.000],  loss: 33.125469, mae: 14.663867, mean_q: 32.502598\n",
            "  7100/30000: episode: 1185, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2733.333 [685.000, 5283.000],  loss: 30.854807, mae: 14.302189, mean_q: 31.673822\n",
            "  7106/30000: episode: 1186, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4035.500 [2237.000, 5348.000],  loss: 43.430622, mae: 13.887994, mean_q: 30.986717\n",
            "  7112/30000: episode: 1187, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3134.667 [1242.000, 5348.000],  loss: 26.204346, mae: 14.147336, mean_q: 31.802065\n",
            "  7118/30000: episode: 1188, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2660.833 [749.000, 4472.000],  loss: 23.736235, mae: 14.360986, mean_q: 32.747501\n",
            "  7124/30000: episode: 1189, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  5.000], mean action: 3330.333 [3283.000, 3567.000],  loss: 33.319393, mae: 12.835790, mean_q: 30.176025\n",
            "  7130/30000: episode: 1190, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2736.333 [1044.000, 4725.000],  loss: 30.804001, mae: 13.604568, mean_q: 31.519690\n",
            "  7136/30000: episode: 1191, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3810.667 [1596.000, 4703.000],  loss: 34.489460, mae: 13.297986, mean_q: 31.231890\n",
            "  7142/30000: episode: 1192, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 3567.000 [3567.000, 3567.000],  loss: 22.584478, mae: 14.277606, mean_q: 33.126942\n",
            "  7148/30000: episode: 1193, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2807.667 [90.000, 3567.000],  loss: 41.897640, mae: 13.747749, mean_q: 31.525465\n",
            "  7154/30000: episode: 1194, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 35.000, mean reward:  5.833 [ 0.000, 25.000], mean action: 2534.500 [1666.000, 3712.000],  loss: 25.071402, mae: 12.275697, mean_q: 29.772532\n",
            "  7160/30000: episode: 1195, duration: 0.188s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2741.333 [951.000, 3719.000],  loss: 39.291065, mae: 13.719556, mean_q: 31.661139\n",
            "  7166/30000: episode: 1196, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2498.333 [1305.000, 4185.000],  loss: 21.174627, mae: 13.746109, mean_q: 32.039902\n",
            "  7172/30000: episode: 1197, duration: 0.263s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2906.167 [375.000, 4681.000],  loss: 24.017012, mae: 13.102687, mean_q: 30.288095\n",
            "  7178/30000: episode: 1198, duration: 0.266s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3374.000 [1090.000, 5730.000],  loss: 47.118328, mae: 14.003520, mean_q: 31.340408\n",
            "  7184/30000: episode: 1199, duration: 0.266s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2871.000 [227.000, 5474.000],  loss: 33.978359, mae: 13.828697, mean_q: 30.839149\n",
            "  7190/30000: episode: 1200, duration: 0.257s, episode steps:   6, steps per second:  23, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 1835.500 [24.000, 4372.000],  loss: 28.466646, mae: 14.202271, mean_q: 32.220341\n",
            "  7196/30000: episode: 1201, duration: 0.263s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4229.333 [1160.000, 5467.000],  loss: 38.354389, mae: 14.453601, mean_q: 31.766886\n",
            "  7202/30000: episode: 1202, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2826.833 [578.000, 5474.000],  loss: 36.653530, mae: 14.908162, mean_q: 32.808929\n",
            "  7208/30000: episode: 1203, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3165.167 [2441.000, 4477.000],  loss: 29.319418, mae: 14.702807, mean_q: 32.944660\n",
            "  7214/30000: episode: 1204, duration: 0.261s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2175.333 [43.000, 5348.000],  loss: 21.833136, mae: 13.850413, mean_q: 31.369530\n",
            "  7220/30000: episode: 1205, duration: 0.259s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3150.667 [79.000, 5467.000],  loss: 25.083357, mae: 14.354787, mean_q: 31.383089\n",
            "  7226/30000: episode: 1206, duration: 0.268s, episode steps:   6, steps per second:  22, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2684.833 [689.000, 4360.000],  loss: 35.357513, mae: 13.656943, mean_q: 31.105865\n",
            "  7232/30000: episode: 1207, duration: 0.262s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2588.667 [134.000, 5449.000],  loss: 40.976528, mae: 13.573049, mean_q: 30.967951\n",
            "  7238/30000: episode: 1208, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2175.333 [267.000, 3741.000],  loss: 23.427881, mae: 14.036195, mean_q: 31.838686\n",
            "  7244/30000: episode: 1209, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3090.833 [1000.000, 4612.000],  loss: 29.235529, mae: 12.928162, mean_q: 29.965521\n",
            "  7250/30000: episode: 1210, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3546.500 [238.000, 5199.000],  loss: 32.954842, mae: 13.853268, mean_q: 31.096685\n",
            "  7256/30000: episode: 1211, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2141.667 [913.000, 4316.000],  loss: 19.929523, mae: 13.347198, mean_q: 29.858667\n",
            "  7262/30000: episode: 1212, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1785.833 [642.000, 3922.000],  loss: 29.030266, mae: 13.172363, mean_q: 29.433252\n",
            "  7268/30000: episode: 1213, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2454.333 [90.000, 5674.000],  loss: 20.729769, mae: 13.554001, mean_q: 31.013588\n",
            "  7274/30000: episode: 1214, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 75.000, mean reward: 12.500 [-5.000, 35.000], mean action: 3839.167 [238.000, 5721.000],  loss: 26.235373, mae: 13.478517, mean_q: 29.968246\n",
            "  7280/30000: episode: 1215, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3237.000 [282.000, 5253.000],  loss: 34.591156, mae: 13.563728, mean_q: 31.120970\n",
            "  7286/30000: episode: 1216, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3818.333 [2484.000, 5554.000],  loss: 39.282284, mae: 13.541713, mean_q: 30.605249\n",
            "  7292/30000: episode: 1217, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 60.000, mean reward: 10.000 [ 0.000, 30.000], mean action: 3293.167 [1630.000, 5390.000],  loss: 27.365652, mae: 13.497554, mean_q: 30.340734\n",
            "  7298/30000: episode: 1218, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2684.500 [1229.000, 5146.000],  loss: 35.979771, mae: 14.193515, mean_q: 31.274626\n",
            "  7304/30000: episode: 1219, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2377.667 [603.000, 4767.000],  loss: 20.103920, mae: 13.661079, mean_q: 30.472586\n",
            "  7310/30000: episode: 1220, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2203.500 [238.000, 4677.000],  loss: 18.354715, mae: 14.581412, mean_q: 32.661629\n",
            "  7316/30000: episode: 1221, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2224.000 [271.000, 4745.000],  loss: 30.280523, mae: 13.800475, mean_q: 30.714945\n",
            "  7322/30000: episode: 1222, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 40.000, mean reward:  6.667 [ 0.000, 35.000], mean action: 3725.167 [590.000, 5383.000],  loss: 21.411352, mae: 14.229245, mean_q: 32.074234\n",
            "  7328/30000: episode: 1223, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2733.333 [42.000, 5410.000],  loss: 26.681890, mae: 14.058208, mean_q: 31.781958\n",
            "  7334/30000: episode: 1224, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward: 25.000, mean reward:  4.167 [ 0.000, 10.000], mean action: 2629.000 [590.000, 5645.000],  loss: 35.041553, mae: 15.116745, mean_q: 33.406872\n",
            "  7340/30000: episode: 1225, duration: 0.185s, episode steps:   6, steps per second:  32, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 2493.000 [69.000, 3946.000],  loss: 37.717529, mae: 13.327788, mean_q: 29.951118\n",
            "  7346/30000: episode: 1226, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2813.000 [42.000, 5628.000],  loss: 35.467098, mae: 13.994433, mean_q: 31.628054\n",
            "  7352/30000: episode: 1227, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1302.167 [69.000, 2221.000],  loss: 30.110781, mae: 14.005131, mean_q: 31.132217\n",
            "  7358/30000: episode: 1228, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2811.667 [1160.000, 4934.000],  loss: 35.512020, mae: 14.004726, mean_q: 31.666138\n",
            "  7364/30000: episode: 1229, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4478.333 [3202.000, 5652.000],  loss: 33.052006, mae: 14.118011, mean_q: 31.523474\n",
            "  7370/30000: episode: 1230, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2830.667 [1905.000, 3573.000],  loss: 21.921127, mae: 12.392075, mean_q: 28.910284\n",
            "  7376/30000: episode: 1231, duration: 0.185s, episode steps:   6, steps per second:  32, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2240.667 [540.000, 5404.000],  loss: 32.311703, mae: 14.666121, mean_q: 32.976101\n",
            "  7382/30000: episode: 1232, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2540.667 [590.000, 5395.000],  loss: 52.797260, mae: 13.184354, mean_q: 30.716026\n",
            "  7388/30000: episode: 1233, duration: 0.185s, episode steps:   6, steps per second:  32, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3084.500 [1059.000, 5721.000],  loss: 22.146233, mae: 15.515166, mean_q: 34.239090\n",
            "  7394/30000: episode: 1234, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2770.333 [106.000, 5671.000],  loss: 25.720625, mae: 14.786453, mean_q: 32.468048\n",
            "  7400/30000: episode: 1235, duration: 0.183s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3621.167 [1166.000, 5474.000],  loss: 22.304604, mae: 14.067819, mean_q: 32.243858\n",
            "  7406/30000: episode: 1236, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2416.833 [42.000, 4185.000],  loss: 25.558662, mae: 13.829811, mean_q: 31.436859\n",
            "  7412/30000: episode: 1237, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward: 45.000, mean reward:  7.500 [-5.000, 30.000], mean action: 2609.667 [363.000, 5721.000],  loss: 34.502918, mae: 13.094687, mean_q: 30.252268\n",
            "  7418/30000: episode: 1238, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3680.500 [590.000, 5721.000],  loss: 26.648499, mae: 13.795892, mean_q: 31.520041\n",
            "  7424/30000: episode: 1239, duration: 0.179s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3725.500 [1959.000, 5721.000],  loss: 27.104925, mae: 13.977786, mean_q: 31.817286\n",
            "  7430/30000: episode: 1240, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2222.667 [1202.000, 2837.000],  loss: 56.039719, mae: 13.954090, mean_q: 31.420908\n",
            "  7436/30000: episode: 1241, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3059.000 [1148.000, 5721.000],  loss: 31.264839, mae: 13.391866, mean_q: 31.165407\n",
            "  7442/30000: episode: 1242, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 3517.500 [238.000, 5721.000],  loss: 42.903172, mae: 14.567851, mean_q: 32.512573\n",
            "  7448/30000: episode: 1243, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2645.667 [1.000, 4332.000],  loss: 31.272867, mae: 14.482960, mean_q: 33.410557\n",
            "  7454/30000: episode: 1244, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3142.833 [590.000, 5626.000],  loss: 21.542152, mae: 13.647164, mean_q: 31.390562\n",
            "  7460/30000: episode: 1245, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 30.000, mean reward:  5.000 [ 0.000, 25.000], mean action: 2035.167 [106.000, 4589.000],  loss: 32.297180, mae: 15.045666, mean_q: 33.503963\n",
            "  7466/30000: episode: 1246, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [-5.000,  5.000], mean action: 2323.333 [199.000, 5730.000],  loss: 19.027636, mae: 13.357820, mean_q: 29.985468\n",
            "  7472/30000: episode: 1247, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3800.667 [2904.000, 5721.000],  loss: 21.606344, mae: 13.594028, mean_q: 30.546618\n",
            "  7478/30000: episode: 1248, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3272.667 [1976.000, 5012.000],  loss: 29.709803, mae: 14.181278, mean_q: 31.809427\n",
            "  7484/30000: episode: 1249, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2195.167 [69.000, 5362.000],  loss: 36.766319, mae: 13.848881, mean_q: 30.761370\n",
            "  7490/30000: episode: 1250, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2673.333 [551.000, 5608.000],  loss: 35.299778, mae: 13.324654, mean_q: 30.715399\n",
            "  7496/30000: episode: 1251, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1178.167 [238.000, 5187.000],  loss: 33.668880, mae: 14.109844, mean_q: 31.489332\n",
            "  7502/30000: episode: 1252, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1729.167 [42.000, 3797.000],  loss: 28.869896, mae: 15.021770, mean_q: 33.272671\n",
            "  7508/30000: episode: 1253, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 2844.333 [590.000, 5253.000],  loss: 33.687737, mae: 13.110105, mean_q: 30.278992\n",
            "  7514/30000: episode: 1254, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1869.333 [42.000, 4438.000],  loss: 39.832325, mae: 13.889839, mean_q: 31.102896\n",
            "  7520/30000: episode: 1255, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2730.667 [590.000, 5579.000],  loss: 31.499878, mae: 13.283584, mean_q: 30.919035\n",
            "  7526/30000: episode: 1256, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1675.500 [590.000, 3719.000],  loss: 23.808561, mae: 13.722949, mean_q: 31.055323\n",
            "  7532/30000: episode: 1257, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3575.167 [751.000, 5424.000],  loss: 26.071974, mae: 15.099949, mean_q: 33.560528\n",
            "  7538/30000: episode: 1258, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 2183.500 [326.000, 4265.000],  loss: 26.502283, mae: 14.551267, mean_q: 32.484913\n",
            "  7544/30000: episode: 1259, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 4266.500 [2688.000, 5570.000],  loss: 44.565266, mae: 14.422832, mean_q: 33.061970\n",
            "  7550/30000: episode: 1260, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3472.333 [1868.000, 5564.000],  loss: 26.797918, mae: 14.300132, mean_q: 32.349133\n",
            "  7556/30000: episode: 1261, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2603.500 [1555.000, 4265.000],  loss: 23.530115, mae: 14.646703, mean_q: 32.885387\n",
            "  7562/30000: episode: 1262, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2189.500 [42.000, 4265.000],  loss: 30.396688, mae: 14.150760, mean_q: 32.000961\n",
            "  7568/30000: episode: 1263, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3937.667 [2710.000, 5474.000],  loss: 25.418543, mae: 13.366437, mean_q: 30.840424\n",
            "  7574/30000: episode: 1264, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2502.333 [238.000, 4696.000],  loss: 40.809067, mae: 13.549077, mean_q: 31.272264\n",
            "  7580/30000: episode: 1265, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3625.000 [1501.000, 5280.000],  loss: 30.449554, mae: 13.885311, mean_q: 31.465553\n",
            "  7586/30000: episode: 1266, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3725.333 [1609.000, 4816.000],  loss: 38.362587, mae: 12.990105, mean_q: 30.058357\n",
            "  7592/30000: episode: 1267, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2867.500 [86.000, 4265.000],  loss: 42.863499, mae: 12.892585, mean_q: 30.406954\n",
            "  7598/30000: episode: 1268, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3541.000 [562.000, 4616.000],  loss: 26.861364, mae: 13.186152, mean_q: 30.201120\n",
            "  7604/30000: episode: 1269, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 3605.167 [858.000, 5679.000],  loss: 39.427174, mae: 14.141376, mean_q: 32.475590\n",
            "  7610/30000: episode: 1270, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4108.000 [2558.000, 5386.000],  loss: 22.745096, mae: 14.548436, mean_q: 32.942757\n",
            "  7616/30000: episode: 1271, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3603.167 [1609.000, 5640.000],  loss: 31.827234, mae: 15.550048, mean_q: 34.571365\n",
            "  7622/30000: episode: 1272, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2963.167 [100.000, 5564.000],  loss: 27.370438, mae: 14.243858, mean_q: 32.257198\n",
            "  7628/30000: episode: 1273, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2675.167 [808.000, 4379.000],  loss: 34.637360, mae: 14.747689, mean_q: 32.914333\n",
            "  7634/30000: episode: 1274, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2718.667 [100.000, 4412.000],  loss: 26.249716, mae: 12.411864, mean_q: 29.070776\n",
            "  7640/30000: episode: 1275, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward: 110.000, mean reward: 18.333 [ 0.000, 35.000], mean action: 3577.000 [238.000, 4353.000],  loss: 24.966003, mae: 13.087884, mean_q: 30.229849\n",
            "  7646/30000: episode: 1276, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 4265.000 [4265.000, 4265.000],  loss: 33.697399, mae: 13.594177, mean_q: 31.126076\n",
            "  7652/30000: episode: 1277, duration: 0.260s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2865.667 [326.000, 5602.000],  loss: 21.972620, mae: 15.158114, mean_q: 33.989635\n",
            "  7658/30000: episode: 1278, duration: 0.214s, episode steps:   6, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2192.333 [100.000, 5424.000],  loss: 25.283442, mae: 12.555610, mean_q: 29.192057\n",
            "  7664/30000: episode: 1279, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2276.000 [100.000, 4265.000],  loss: 33.735046, mae: 13.576332, mean_q: 30.952469\n",
            "  7670/30000: episode: 1280, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3204.500 [803.000, 5204.000],  loss: 21.716825, mae: 13.586437, mean_q: 30.445852\n",
            "  7676/30000: episode: 1281, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3605.333 [1491.000, 4337.000],  loss: 37.438587, mae: 12.764904, mean_q: 29.372675\n",
            "  7682/30000: episode: 1282, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1193.000 [798.000, 1596.000],  loss: 41.908386, mae: 14.256768, mean_q: 32.492001\n",
            "  7688/30000: episode: 1283, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3394.667 [205.000, 5750.000],  loss: 42.027977, mae: 13.998397, mean_q: 31.798151\n",
            "  7694/30000: episode: 1284, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3287.500 [1596.000, 5424.000],  loss: 26.273043, mae: 15.117538, mean_q: 33.877628\n",
            "  7700/30000: episode: 1285, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3586.667 [1419.000, 5151.000],  loss: 21.994438, mae: 14.060902, mean_q: 32.195042\n",
            "  7706/30000: episode: 1286, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 50.000, mean reward:  8.333 [-5.000, 20.000], mean action: 2287.833 [427.000, 4212.000],  loss: 32.854694, mae: 13.964364, mean_q: 31.702913\n",
            "  7712/30000: episode: 1287, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3010.500 [1491.000, 5134.000],  loss: 21.239716, mae: 13.599843, mean_q: 30.994995\n",
            "  7718/30000: episode: 1288, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2618.833 [790.000, 5296.000],  loss: 22.887346, mae: 13.311801, mean_q: 30.886475\n",
            "  7724/30000: episode: 1289, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3503.167 [487.000, 5386.000],  loss: 25.859085, mae: 14.133060, mean_q: 32.252468\n",
            "  7730/30000: episode: 1290, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 1828.167 [97.000, 5057.000],  loss: 46.612881, mae: 13.928462, mean_q: 31.911608\n",
            "  7736/30000: episode: 1291, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3771.500 [1202.000, 5727.000],  loss: 26.538963, mae: 13.618675, mean_q: 31.106918\n",
            "  7742/30000: episode: 1292, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3177.833 [1438.000, 4794.000],  loss: 33.579182, mae: 15.514922, mean_q: 33.955914\n",
            "  7748/30000: episode: 1293, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 2319.000 [1257.000, 5233.000],  loss: 39.057274, mae: 13.678884, mean_q: 32.189835\n",
            "  7754/30000: episode: 1294, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2434.333 [1103.000, 5187.000],  loss: 38.047291, mae: 12.992534, mean_q: 29.997200\n",
            "  7760/30000: episode: 1295, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3293.333 [996.000, 5256.000],  loss: 33.742344, mae: 14.006982, mean_q: 31.660337\n",
            "  7766/30000: episode: 1296, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3523.667 [2076.000, 4316.000],  loss: 22.616350, mae: 13.554532, mean_q: 31.134291\n",
            "  7772/30000: episode: 1297, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2654.000 [69.000, 4412.000],  loss: 22.892092, mae: 13.375974, mean_q: 31.132269\n",
            "  7778/30000: episode: 1298, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3031.833 [1491.000, 5418.000],  loss: 36.428783, mae: 13.193539, mean_q: 30.176476\n",
            "  7784/30000: episode: 1299, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 5026.333 [3572.000, 5742.000],  loss: 23.712564, mae: 13.845538, mean_q: 31.619795\n",
            "  7790/30000: episode: 1300, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2086.167 [238.000, 4392.000],  loss: 26.047548, mae: 13.843510, mean_q: 31.113569\n",
            "  7796/30000: episode: 1301, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 35.000, mean reward:  5.833 [ 0.000, 25.000], mean action: 3044.333 [560.000, 5411.000],  loss: 18.183161, mae: 12.963604, mean_q: 29.391037\n",
            "  7802/30000: episode: 1302, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2382.000 [1202.000, 3921.000],  loss: 32.545895, mae: 14.642437, mean_q: 33.294613\n",
            "  7808/30000: episode: 1303, duration: 0.179s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3082.333 [100.000, 4826.000],  loss: 35.895996, mae: 13.651647, mean_q: 30.788832\n",
            "  7814/30000: episode: 1304, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3312.333 [1162.000, 4831.000],  loss: 35.605618, mae: 13.137122, mean_q: 30.005095\n",
            "  7820/30000: episode: 1305, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2060.000 [610.000, 3921.000],  loss: 22.376417, mae: 14.719666, mean_q: 32.915577\n",
            "  7826/30000: episode: 1306, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3055.667 [2255.000, 4191.000],  loss: 32.365955, mae: 13.737209, mean_q: 31.540064\n",
            "  7832/30000: episode: 1307, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2474.500 [483.000, 5494.000],  loss: 29.721350, mae: 13.704079, mean_q: 31.906874\n",
            "  7838/30000: episode: 1308, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 2987.000 [1202.000, 4719.000],  loss: 18.314518, mae: 13.165123, mean_q: 30.639204\n",
            "  7844/30000: episode: 1309, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 45.000, mean reward:  7.500 [ 0.000, 20.000], mean action: 2121.167 [238.000, 5199.000],  loss: 30.419746, mae: 14.084995, mean_q: 32.266949\n",
            "  7850/30000: episode: 1310, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: 45.000, mean reward:  7.500 [ 0.000, 15.000], mean action: 1986.500 [1202.000, 3712.000],  loss: 30.578970, mae: 14.113395, mean_q: 33.025158\n",
            "  7856/30000: episode: 1311, duration: 0.179s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2495.333 [1260.000, 3745.000],  loss: 29.906950, mae: 14.100632, mean_q: 32.171612\n",
            "  7862/30000: episode: 1312, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3622.500 [1042.000, 5148.000],  loss: 42.412029, mae: 15.050282, mean_q: 33.202499\n",
            "  7868/30000: episode: 1313, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2531.833 [848.000, 4006.000],  loss: 25.366949, mae: 13.192642, mean_q: 30.530619\n",
            "  7874/30000: episode: 1314, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3766.167 [2311.000, 4745.000],  loss: 26.304903, mae: 13.635653, mean_q: 31.012991\n",
            "  7880/30000: episode: 1315, duration: 0.185s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3335.667 [68.000, 5424.000],  loss: 20.970419, mae: 13.900555, mean_q: 30.844910\n",
            "  7886/30000: episode: 1316, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2436.833 [1202.000, 5395.000],  loss: 55.616760, mae: 14.195430, mean_q: 32.175865\n",
            "  7892/30000: episode: 1317, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3655.667 [2565.000, 5386.000],  loss: 22.048323, mae: 13.598105, mean_q: 30.921774\n",
            "  7898/30000: episode: 1318, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2063.500 [702.000, 4404.000],  loss: 26.926016, mae: 13.445815, mean_q: 31.167925\n",
            "  7904/30000: episode: 1319, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2019.167 [320.000, 5574.000],  loss: 35.209389, mae: 14.341248, mean_q: 32.057419\n",
            "  7910/30000: episode: 1320, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3736.500 [922.000, 5424.000],  loss: 52.284573, mae: 13.814410, mean_q: 31.209938\n",
            "  7916/30000: episode: 1321, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3122.333 [238.000, 5151.000],  loss: 22.392092, mae: 14.734055, mean_q: 32.592754\n",
            "  7922/30000: episode: 1322, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3799.667 [2116.000, 5730.000],  loss: 30.945213, mae: 13.037747, mean_q: 29.974939\n",
            "  7928/30000: episode: 1323, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2994.333 [1596.000, 5730.000],  loss: 28.619104, mae: 14.126297, mean_q: 32.018196\n",
            "  7934/30000: episode: 1324, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2524.500 [2277.000, 3591.000],  loss: 28.653305, mae: 14.693653, mean_q: 33.482140\n",
            "  7940/30000: episode: 1325, duration: 0.187s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3796.667 [3020.000, 5140.000],  loss: 26.790878, mae: 13.283519, mean_q: 30.981058\n",
            "  7946/30000: episode: 1326, duration: 0.229s, episode steps:   6, steps per second:  26, episode reward: 75.000, mean reward: 12.500 [ 0.000, 35.000], mean action: 2895.833 [663.000, 4191.000],  loss: 32.450180, mae: 13.545578, mean_q: 30.547440\n",
            "  7952/30000: episode: 1327, duration: 0.194s, episode steps:   6, steps per second:  31, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2665.333 [100.000, 5466.000],  loss: 40.665874, mae: 14.492810, mean_q: 32.463947\n",
            "  7958/30000: episode: 1328, duration: 0.281s, episode steps:   6, steps per second:  21, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3864.333 [3534.000, 4464.000],  loss: 30.624163, mae: 14.786068, mean_q: 33.887325\n",
            "  7964/30000: episode: 1329, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3770.833 [3261.000, 4780.000],  loss: 24.482210, mae: 13.892975, mean_q: 31.976831\n",
            "  7970/30000: episode: 1330, duration: 0.216s, episode steps:   6, steps per second:  28, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2838.000 [1622.000, 5727.000],  loss: 30.872999, mae: 14.085303, mean_q: 33.187710\n",
            "  7976/30000: episode: 1331, duration: 0.288s, episode steps:   6, steps per second:  21, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3767.167 [808.000, 5730.000],  loss: 29.088053, mae: 13.884114, mean_q: 31.369759\n",
            "  7982/30000: episode: 1332, duration: 0.320s, episode steps:   6, steps per second:  19, episode reward: 40.000, mean reward:  6.667 [ 0.000, 25.000], mean action: 3106.833 [1199.000, 5424.000],  loss: 21.255655, mae: 12.697881, mean_q: 29.483114\n",
            "  7988/30000: episode: 1333, duration: 0.323s, episode steps:   6, steps per second:  19, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2435.500 [756.000, 5072.000],  loss: 36.397289, mae: 13.417359, mean_q: 31.142160\n",
            "  7994/30000: episode: 1334, duration: 0.576s, episode steps:   6, steps per second:  10, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 2603.833 [512.000, 5730.000],  loss: 41.758221, mae: 13.755181, mean_q: 30.943085\n",
            "  8000/30000: episode: 1335, duration: 0.661s, episode steps:   6, steps per second:   9, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3257.667 [158.000, 5649.000],  loss: 35.914337, mae: 14.309315, mean_q: 32.424046\n",
            "  8006/30000: episode: 1336, duration: 0.257s, episode steps:   6, steps per second:  23, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2213.167 [238.000, 4719.000],  loss: 37.154972, mae: 13.383766, mean_q: 30.630327\n",
            "  8012/30000: episode: 1337, duration: 0.362s, episode steps:   6, steps per second:  17, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3366.167 [442.000, 4921.000],  loss: 67.756958, mae: 14.179359, mean_q: 32.005039\n",
            "  8018/30000: episode: 1338, duration: 0.413s, episode steps:   6, steps per second:  15, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4059.667 [943.000, 4683.000],  loss: 44.143635, mae: 14.405177, mean_q: 31.960733\n",
            "  8024/30000: episode: 1339, duration: 0.533s, episode steps:   6, steps per second:  11, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3146.333 [1634.000, 5459.000],  loss: 20.093649, mae: 14.143182, mean_q: 31.711142\n",
            "  8030/30000: episode: 1340, duration: 0.419s, episode steps:   6, steps per second:  14, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2840.667 [1793.000, 5072.000],  loss: 22.696182, mae: 14.271420, mean_q: 32.070271\n",
            "  8036/30000: episode: 1341, duration: 0.380s, episode steps:   6, steps per second:  16, episode reward: 50.000, mean reward:  8.333 [-5.000, 20.000], mean action: 3104.333 [1596.000, 5656.000],  loss: 29.446707, mae: 13.250752, mean_q: 30.656801\n",
            "  8042/30000: episode: 1342, duration: 0.263s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3046.500 [43.000, 5257.000],  loss: 17.907341, mae: 13.209020, mean_q: 30.180931\n",
            "  8048/30000: episode: 1343, duration: 0.268s, episode steps:   6, steps per second:  22, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3129.167 [25.000, 4913.000],  loss: 24.395964, mae: 14.413017, mean_q: 32.382938\n",
            "  8054/30000: episode: 1344, duration: 0.217s, episode steps:   6, steps per second:  28, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4855.333 [4683.000, 5717.000],  loss: 50.970425, mae: 13.367363, mean_q: 30.613684\n",
            "  8060/30000: episode: 1345, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3040.333 [2067.000, 3869.000],  loss: 38.949940, mae: 13.683269, mean_q: 32.667103\n",
            "  8066/30000: episode: 1346, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4105.667 [2837.000, 5404.000],  loss: 20.535856, mae: 13.428737, mean_q: 30.974421\n",
            "  8072/30000: episode: 1347, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4630.667 [4159.000, 4725.000],  loss: 29.379801, mae: 14.188025, mean_q: 31.667143\n",
            "  8078/30000: episode: 1348, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3570.000 [1539.000, 5740.000],  loss: 27.812513, mae: 13.489079, mean_q: 31.237970\n",
            "  8084/30000: episode: 1349, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3341.000 [560.000, 5579.000],  loss: 36.983791, mae: 14.928866, mean_q: 33.126785\n",
            "  8090/30000: episode: 1350, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [-5.000, 30.000], mean action: 4372.167 [3936.000, 5239.000],  loss: 53.711689, mae: 13.491252, mean_q: 30.618021\n",
            "  8096/30000: episode: 1351, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 30.000, mean reward:  5.000 [ 0.000, 30.000], mean action: 3276.167 [273.000, 5334.000],  loss: 25.269279, mae: 14.776802, mean_q: 33.944077\n",
            "  8102/30000: episode: 1352, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2777.833 [756.000, 5180.000],  loss: 30.675827, mae: 13.629815, mean_q: 32.014156\n",
            "  8108/30000: episode: 1353, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1329.500 [184.000, 2454.000],  loss: 25.808090, mae: 14.246727, mean_q: 31.927462\n",
            "  8114/30000: episode: 1354, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2673.500 [756.000, 5180.000],  loss: 28.761942, mae: 13.276160, mean_q: 30.996758\n",
            "  8120/30000: episode: 1355, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3432.833 [126.000, 4858.000],  loss: 20.389431, mae: 13.988957, mean_q: 31.590925\n",
            "  8126/30000: episode: 1356, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4397.167 [2968.000, 4683.000],  loss: 44.166920, mae: 13.655818, mean_q: 31.429499\n",
            "  8132/30000: episode: 1357, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3363.167 [273.000, 5117.000],  loss: 27.554430, mae: 13.246708, mean_q: 30.770720\n",
            "  8138/30000: episode: 1358, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2226.333 [381.000, 4407.000],  loss: 28.113306, mae: 13.506206, mean_q: 30.302500\n",
            "  8144/30000: episode: 1359, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2543.000 [1772.000, 3400.000],  loss: 33.011879, mae: 13.133995, mean_q: 29.951540\n",
            "  8150/30000: episode: 1360, duration: 0.164s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4025.833 [747.000, 5506.000],  loss: 37.509438, mae: 14.446176, mean_q: 33.018326\n",
            "  8156/30000: episode: 1361, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2716.833 [262.000, 5256.000],  loss: 29.600008, mae: 13.858161, mean_q: 31.245285\n",
            "  8162/30000: episode: 1362, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3126.500 [403.000, 5478.000],  loss: 29.375692, mae: 13.940289, mean_q: 32.171768\n",
            "  8168/30000: episode: 1363, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 3184.833 [314.000, 5434.000],  loss: 36.283798, mae: 14.120488, mean_q: 32.375660\n",
            "  8174/30000: episode: 1364, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3496.167 [2861.000, 5362.000],  loss: 36.331417, mae: 14.626751, mean_q: 32.482307\n",
            "  8180/30000: episode: 1365, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3278.000 [2116.000, 4391.000],  loss: 28.837183, mae: 12.783222, mean_q: 29.352125\n",
            "  8186/30000: episode: 1366, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2713.667 [1160.000, 5717.000],  loss: 27.792032, mae: 13.852833, mean_q: 32.137104\n",
            "  8192/30000: episode: 1367, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3398.167 [2168.000, 4683.000],  loss: 35.572369, mae: 12.892724, mean_q: 29.993576\n",
            "  8198/30000: episode: 1368, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2752.000 [915.000, 4630.000],  loss: 37.740784, mae: 13.873867, mean_q: 31.672606\n",
            "  8204/30000: episode: 1369, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2376.667 [926.000, 3221.000],  loss: 20.589619, mae: 14.366940, mean_q: 31.858522\n",
            "  8210/30000: episode: 1370, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 1644.833 [205.000, 4159.000],  loss: 40.222805, mae: 13.360532, mean_q: 30.760706\n",
            "  8216/30000: episode: 1371, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2949.167 [2075.000, 4921.000],  loss: 27.690744, mae: 14.311856, mean_q: 31.343012\n",
            "  8222/30000: episode: 1372, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3805.333 [1665.000, 5272.000],  loss: 37.169228, mae: 13.504722, mean_q: 30.455330\n",
            "  8228/30000: episode: 1373, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2500.833 [879.000, 4819.000],  loss: 28.923689, mae: 12.956321, mean_q: 29.891470\n",
            "  8234/30000: episode: 1374, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: 45.000, mean reward:  7.500 [ 0.000, 45.000], mean action: 1338.500 [88.000, 3921.000],  loss: 26.432837, mae: 13.812528, mean_q: 31.135849\n",
            "  8240/30000: episode: 1375, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2956.333 [344.000, 5679.000],  loss: 26.853790, mae: 14.719075, mean_q: 32.324066\n",
            "  8246/30000: episode: 1376, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3402.500 [1024.000, 5089.000],  loss: 22.679407, mae: 13.122527, mean_q: 30.530107\n",
            "  8252/30000: episode: 1377, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2670.333 [879.000, 5239.000],  loss: 26.590399, mae: 13.451495, mean_q: 30.111441\n",
            "  8258/30000: episode: 1378, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3666.833 [527.000, 5715.000],  loss: 28.535410, mae: 14.048757, mean_q: 31.748230\n",
            "  8264/30000: episode: 1379, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3032.167 [560.000, 5538.000],  loss: 34.013760, mae: 14.641926, mean_q: 31.990351\n",
            "  8270/30000: episode: 1380, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3004.167 [527.000, 5057.000],  loss: 33.192455, mae: 13.188428, mean_q: 30.200096\n",
            "  8276/30000: episode: 1381, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3453.667 [587.000, 5723.000],  loss: 28.595320, mae: 13.678509, mean_q: 31.117487\n",
            "  8282/30000: episode: 1382, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3845.667 [1876.000, 4783.000],  loss: 39.784149, mae: 15.099347, mean_q: 33.687191\n",
            "  8288/30000: episode: 1383, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3367.500 [1666.000, 4954.000],  loss: 40.960865, mae: 14.375259, mean_q: 32.675392\n",
            "  8294/30000: episode: 1384, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2406.167 [150.000, 5742.000],  loss: 34.342552, mae: 13.535767, mean_q: 31.038017\n",
            "  8300/30000: episode: 1385, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3019.500 [238.000, 4783.000],  loss: 23.143675, mae: 12.735211, mean_q: 29.493582\n",
            "  8306/30000: episode: 1386, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 4107.500 [2829.000, 4838.000],  loss: 44.441357, mae: 14.046102, mean_q: 32.196636\n",
            "  8312/30000: episode: 1387, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 45.000, mean reward:  7.500 [ 0.000, 45.000], mean action: 2601.833 [238.000, 4858.000],  loss: 17.363174, mae: 12.917786, mean_q: 30.087694\n",
            "  8318/30000: episode: 1388, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3531.500 [489.000, 5532.000],  loss: 17.456366, mae: 12.934266, mean_q: 30.029694\n",
            "  8324/30000: episode: 1389, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 45.000, mean reward:  7.500 [ 0.000, 25.000], mean action: 3342.833 [1596.000, 5405.000],  loss: 31.347353, mae: 13.628475, mean_q: 31.267603\n",
            "  8330/30000: episode: 1390, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 3522.500 [10.000, 5140.000],  loss: 24.937439, mae: 12.831593, mean_q: 30.325014\n",
            "  8336/30000: episode: 1391, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2550.833 [946.000, 4472.000],  loss: 41.020142, mae: 13.617631, mean_q: 31.374634\n",
            "  8342/30000: episode: 1392, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1328.000 [489.000, 2547.000],  loss: 25.269743, mae: 14.292493, mean_q: 32.337574\n",
            "  8348/30000: episode: 1393, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3296.833 [2665.000, 3906.000],  loss: 30.265631, mae: 13.633353, mean_q: 31.797035\n",
            "  8354/30000: episode: 1394, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3699.000 [2026.000, 5360.000],  loss: 33.260685, mae: 13.986796, mean_q: 32.232143\n",
            "  8360/30000: episode: 1395, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3122.833 [489.000, 5656.000],  loss: 32.053982, mae: 15.119152, mean_q: 33.579929\n",
            "  8366/30000: episode: 1396, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [-5.000, 20.000], mean action: 1749.333 [238.000, 5004.000],  loss: 14.302278, mae: 13.511609, mean_q: 30.566854\n",
            "  8372/30000: episode: 1397, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 3028.333 [489.000, 3921.000],  loss: 27.218552, mae: 14.910806, mean_q: 33.489056\n",
            "  8378/30000: episode: 1398, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3378.000 [476.000, 5257.000],  loss: 35.063007, mae: 13.980483, mean_q: 31.820635\n",
            "  8384/30000: episode: 1399, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3451.667 [1339.000, 5656.000],  loss: 24.449778, mae: 15.075272, mean_q: 33.958744\n",
            "  8390/30000: episode: 1400, duration: 0.235s, episode steps:   6, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2723.500 [489.000, 5554.000],  loss: 22.056490, mae: 14.443408, mean_q: 34.012264\n",
            "  8396/30000: episode: 1401, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2530.167 [271.000, 5261.000],  loss: 37.474133, mae: 14.343905, mean_q: 32.484959\n",
            "  8402/30000: episode: 1402, duration: 0.262s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2552.167 [535.000, 3829.000],  loss: 28.316580, mae: 13.563931, mean_q: 31.724854\n",
            "  8408/30000: episode: 1403, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 2970.667 [267.000, 5486.000],  loss: 32.107777, mae: 14.024549, mean_q: 32.239723\n",
            "  8414/30000: episode: 1404, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3176.167 [489.000, 4832.000],  loss: 41.394939, mae: 12.887668, mean_q: 29.796000\n",
            "  8420/30000: episode: 1405, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 3865.500 [1530.000, 5719.000],  loss: 44.710064, mae: 13.350181, mean_q: 30.881744\n",
            "  8426/30000: episode: 1406, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2341.833 [1019.000, 3643.000],  loss: 28.220467, mae: 14.769162, mean_q: 32.682804\n",
            "  8432/30000: episode: 1407, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2012.667 [339.000, 4407.000],  loss: 41.270924, mae: 14.178035, mean_q: 32.430408\n",
            "  8438/30000: episode: 1408, duration: 0.260s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1669.167 [545.000, 5532.000],  loss: 34.940125, mae: 12.986373, mean_q: 30.067053\n",
            "  8444/30000: episode: 1409, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3038.167 [1128.000, 5486.000],  loss: 30.600885, mae: 12.953328, mean_q: 29.956360\n",
            "  8450/30000: episode: 1410, duration: 0.265s, episode steps:   6, steps per second:  23, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3789.000 [88.000, 5679.000],  loss: 28.200914, mae: 12.513313, mean_q: 29.295158\n",
            "  8456/30000: episode: 1411, duration: 0.229s, episode steps:   6, steps per second:  26, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2922.333 [489.000, 5534.000],  loss: 28.881090, mae: 14.121264, mean_q: 31.616438\n",
            "  8462/30000: episode: 1412, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3229.167 [487.000, 5564.000],  loss: 28.775984, mae: 14.400665, mean_q: 33.187061\n",
            "  8468/30000: episode: 1413, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [-5.000, 20.000], mean action: 2734.000 [205.000, 5503.000],  loss: 36.239567, mae: 13.004348, mean_q: 30.585745\n",
            "  8474/30000: episode: 1414, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 40.000, mean reward:  6.667 [ 0.000, 25.000], mean action: 3286.333 [1322.000, 5679.000],  loss: 46.988232, mae: 13.756264, mean_q: 31.767029\n",
            "  8480/30000: episode: 1415, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2605.333 [606.000, 5743.000],  loss: 24.714159, mae: 14.007716, mean_q: 31.389040\n",
            "  8486/30000: episode: 1416, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3036.167 [648.000, 4802.000],  loss: 33.134228, mae: 12.403717, mean_q: 28.827591\n",
            "  8492/30000: episode: 1417, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2228.833 [88.000, 5212.000],  loss: 23.680784, mae: 13.482784, mean_q: 31.632307\n",
            "  8498/30000: episode: 1418, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2949.833 [489.000, 4992.000],  loss: 36.115643, mae: 14.011352, mean_q: 32.572418\n",
            "  8504/30000: episode: 1419, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2459.667 [518.000, 4460.000],  loss: 25.631851, mae: 13.113257, mean_q: 30.622488\n",
            "  8510/30000: episode: 1420, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3604.500 [2213.000, 5120.000],  loss: 40.866932, mae: 13.903244, mean_q: 32.088348\n",
            "  8516/30000: episode: 1421, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3474.167 [1245.000, 5239.000],  loss: 22.337469, mae: 13.387313, mean_q: 30.927629\n",
            "  8522/30000: episode: 1422, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 2308.500 [42.000, 4464.000],  loss: 39.094215, mae: 13.444453, mean_q: 31.534754\n",
            "  8528/30000: episode: 1423, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2093.667 [43.000, 5626.000],  loss: 29.828909, mae: 13.326215, mean_q: 31.209799\n",
            "  8534/30000: episode: 1424, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2601.167 [138.000, 5140.000],  loss: 20.386938, mae: 13.765246, mean_q: 32.052059\n",
            "  8540/30000: episode: 1425, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2569.000 [751.000, 5679.000],  loss: 18.593164, mae: 13.826222, mean_q: 31.898336\n",
            "  8546/30000: episode: 1426, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1836.333 [751.000, 3921.000],  loss: 40.326138, mae: 14.937482, mean_q: 34.109650\n",
            "  8552/30000: episode: 1427, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1988.833 [1103.000, 3826.000],  loss: 36.250668, mae: 13.573323, mean_q: 31.540619\n",
            "  8558/30000: episode: 1428, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1846.000 [907.000, 2585.000],  loss: 26.624815, mae: 14.485066, mean_q: 33.034996\n",
            "  8564/30000: episode: 1429, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1790.000 [162.000, 3542.000],  loss: 42.680874, mae: 14.021358, mean_q: 33.142918\n",
            "  8570/30000: episode: 1430, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 3838.167 [2585.000, 5679.000],  loss: 41.940411, mae: 13.649410, mean_q: 32.251049\n",
            "  8576/30000: episode: 1431, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3086.500 [489.000, 5486.000],  loss: 39.271439, mae: 14.603763, mean_q: 33.797890\n",
            "  8582/30000: episode: 1432, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2719.667 [1044.000, 5257.000],  loss: 25.816330, mae: 13.450183, mean_q: 31.011999\n",
            "  8588/30000: episode: 1433, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2443.833 [1002.000, 3951.000],  loss: 30.775873, mae: 13.133389, mean_q: 30.879263\n",
            "  8594/30000: episode: 1434, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2533.667 [2277.000, 2585.000],  loss: 40.075356, mae: 12.501380, mean_q: 29.632441\n",
            "  8600/30000: episode: 1435, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2584.667 [535.000, 5170.000],  loss: 29.329803, mae: 13.531318, mean_q: 31.659559\n",
            "  8606/30000: episode: 1436, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 140.000, mean reward: 23.333 [ 0.000, 35.000], mean action: 2987.500 [238.000, 4185.000],  loss: 32.342827, mae: 14.261340, mean_q: 32.635082\n",
            "  8612/30000: episode: 1437, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1613.667 [326.000, 3101.000],  loss: 17.360739, mae: 13.674968, mean_q: 31.125986\n",
            "  8618/30000: episode: 1438, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1986.000 [365.000, 3469.000],  loss: 38.875042, mae: 13.708316, mean_q: 31.578867\n",
            "  8624/30000: episode: 1439, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2508.500 [487.000, 5188.000],  loss: 21.342461, mae: 13.256993, mean_q: 31.331253\n",
            "  8630/30000: episode: 1440, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 4769.667 [3089.000, 5600.000],  loss: 32.478542, mae: 12.958743, mean_q: 30.063440\n",
            "  8636/30000: episode: 1441, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2048.500 [95.000, 3091.000],  loss: 22.863585, mae: 13.576657, mean_q: 31.072670\n",
            "  8642/30000: episode: 1442, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1769.333 [489.000, 3804.000],  loss: 28.303038, mae: 13.367397, mean_q: 29.960695\n",
            "  8648/30000: episode: 1443, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2387.500 [522.000, 5544.000],  loss: 31.500542, mae: 14.731323, mean_q: 33.882454\n",
            "  8654/30000: episode: 1444, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2133.167 [1560.000, 3643.000],  loss: 38.615711, mae: 14.304802, mean_q: 32.077179\n",
            "  8660/30000: episode: 1445, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3237.167 [1112.000, 5532.000],  loss: 34.271137, mae: 14.475246, mean_q: 33.116215\n",
            "  8666/30000: episode: 1446, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1834.000 [158.000, 4185.000],  loss: 28.638319, mae: 13.214902, mean_q: 30.227949\n",
            "  8672/30000: episode: 1447, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3283.333 [518.000, 5626.000],  loss: 35.960941, mae: 13.646026, mean_q: 31.922569\n",
            "  8678/30000: episode: 1448, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2267.167 [590.000, 2844.000],  loss: 22.289454, mae: 13.447818, mean_q: 31.281248\n",
            "  8684/30000: episode: 1449, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3341.667 [1038.000, 4630.000],  loss: 36.765438, mae: 14.003604, mean_q: 31.690622\n",
            "  8690/30000: episode: 1450, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 2681.167 [419.000, 5644.000],  loss: 26.432877, mae: 14.725803, mean_q: 33.365688\n",
            "  8696/30000: episode: 1451, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3670.167 [2580.000, 5144.000],  loss: 27.119917, mae: 13.923654, mean_q: 31.630285\n",
            "  8702/30000: episode: 1452, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2666.500 [1835.000, 3921.000],  loss: 26.821722, mae: 14.286096, mean_q: 32.245296\n",
            "  8708/30000: episode: 1453, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 40.000, mean reward:  6.667 [ 0.000, 20.000], mean action: 2850.667 [522.000, 5651.000],  loss: 22.308249, mae: 14.451393, mean_q: 33.063488\n",
            "  8714/30000: episode: 1454, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3201.333 [522.000, 5651.000],  loss: 39.273586, mae: 13.241679, mean_q: 30.975235\n",
            "  8720/30000: episode: 1455, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2590.333 [779.000, 4630.000],  loss: 41.074314, mae: 13.507809, mean_q: 31.252737\n",
            "  8726/30000: episode: 1456, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [-5.000, 10.000], mean action: 2144.000 [1202.000, 5679.000],  loss: 24.974642, mae: 13.772552, mean_q: 31.310411\n",
            "  8732/30000: episode: 1457, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2767.167 [1098.000, 4954.000],  loss: 26.295835, mae: 12.387230, mean_q: 28.807785\n",
            "  8738/30000: episode: 1458, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 30.000, mean reward:  5.000 [ 0.000, 10.000], mean action: 3231.833 [445.000, 5233.000],  loss: 28.306953, mae: 13.387782, mean_q: 31.324305\n",
            "  8744/30000: episode: 1459, duration: 0.179s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1552.500 [518.000, 3656.000],  loss: 29.227213, mae: 13.879619, mean_q: 32.153671\n",
            "  8750/30000: episode: 1460, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1213.667 [326.000, 2435.000],  loss: 38.469700, mae: 14.310060, mean_q: 32.441818\n",
            "  8756/30000: episode: 1461, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3088.500 [2201.000, 4193.000],  loss: 20.481745, mae: 14.301063, mean_q: 32.762283\n",
            "  8762/30000: episode: 1462, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2513.167 [560.000, 4921.000],  loss: 30.098618, mae: 14.580147, mean_q: 33.043812\n",
            "  8768/30000: episode: 1463, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2242.333 [26.000, 5388.000],  loss: 21.914062, mae: 15.314265, mean_q: 35.091434\n",
            "  8774/30000: episode: 1464, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3204.500 [2125.000, 4233.000],  loss: 15.951268, mae: 14.520531, mean_q: 32.938965\n",
            "  8780/30000: episode: 1465, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: 15.000, mean reward:  2.500 [-5.000, 15.000], mean action: 2358.833 [1340.000, 4412.000],  loss: 37.809864, mae: 13.887469, mean_q: 32.228722\n",
            "  8786/30000: episode: 1466, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 533.833 [400.000, 1203.000],  loss: 20.927650, mae: 13.770247, mean_q: 31.769800\n",
            "  8792/30000: episode: 1467, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1559.833 [400.000, 4910.000],  loss: 29.415049, mae: 14.424427, mean_q: 32.945393\n",
            "  8798/30000: episode: 1468, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3347.333 [1634.000, 4906.000],  loss: 48.956356, mae: 13.309319, mean_q: 30.792389\n",
            "  8804/30000: episode: 1469, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 35.000, mean reward:  5.833 [-5.000, 30.000], mean action: 1676.333 [363.000, 4133.000],  loss: 27.558649, mae: 13.395947, mean_q: 31.009354\n",
            "  8810/30000: episode: 1470, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3771.500 [1793.000, 4910.000],  loss: 31.952177, mae: 13.699249, mean_q: 31.680059\n",
            "  8816/30000: episode: 1471, duration: 0.234s, episode steps:   6, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2501.667 [852.000, 5486.000],  loss: 23.881706, mae: 13.028941, mean_q: 30.695314\n",
            "  8822/30000: episode: 1472, duration: 0.259s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2691.500 [1222.000, 4136.000],  loss: 39.760025, mae: 13.999391, mean_q: 32.844608\n",
            "  8828/30000: episode: 1473, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3733.000 [1202.000, 5338.000],  loss: 28.319670, mae: 14.179672, mean_q: 32.870663\n",
            "  8834/30000: episode: 1474, duration: 0.238s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4432.500 [3091.000, 5607.000],  loss: 26.054850, mae: 14.113343, mean_q: 32.310341\n",
            "  8840/30000: episode: 1475, duration: 0.263s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2886.333 [419.000, 5463.000],  loss: 28.953001, mae: 13.972217, mean_q: 32.450901\n",
            "  8846/30000: episode: 1476, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 2725.667 [1202.000, 4656.000],  loss: 30.060722, mae: 12.942472, mean_q: 30.347536\n",
            "  8852/30000: episode: 1477, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2970.833 [419.000, 5180.000],  loss: 23.852196, mae: 13.993481, mean_q: 31.773804\n",
            "  8858/30000: episode: 1478, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3143.333 [749.000, 4137.000],  loss: 17.860508, mae: 12.539737, mean_q: 29.666700\n",
            "  8864/30000: episode: 1479, duration: 0.295s, episode steps:   6, steps per second:  20, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2570.500 [1190.000, 4425.000],  loss: 29.962593, mae: 13.176686, mean_q: 31.168983\n",
            "  8870/30000: episode: 1480, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 2787.333 [808.000, 5596.000],  loss: 34.572659, mae: 13.973460, mean_q: 32.089405\n",
            "  8876/30000: episode: 1481, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1877.000 [88.000, 3831.000],  loss: 32.321320, mae: 13.567719, mean_q: 31.573938\n",
            "  8882/30000: episode: 1482, duration: 0.240s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3371.667 [419.000, 5152.000],  loss: 25.274551, mae: 14.377983, mean_q: 33.045780\n",
            "  8888/30000: episode: 1483, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3430.167 [2149.000, 5152.000],  loss: 35.610233, mae: 13.540214, mean_q: 31.895208\n",
            "  8894/30000: episode: 1484, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3009.833 [124.000, 5152.000],  loss: 34.365837, mae: 13.164753, mean_q: 30.696859\n",
            "  8900/30000: episode: 1485, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3132.333 [430.000, 5062.000],  loss: 33.894360, mae: 14.277355, mean_q: 32.748611\n",
            "  8906/30000: episode: 1486, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3978.167 [2529.000, 4921.000],  loss: 50.566357, mae: 14.152557, mean_q: 32.253292\n",
            "  8912/30000: episode: 1487, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 1422.500 [88.000, 3484.000],  loss: 29.822771, mae: 13.528451, mean_q: 31.239462\n",
            "  8918/30000: episode: 1488, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3277.333 [460.000, 4683.000],  loss: 28.166964, mae: 13.993386, mean_q: 31.947973\n",
            "  8924/30000: episode: 1489, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 1796.833 [375.000, 4185.000],  loss: 24.466177, mae: 12.870479, mean_q: 29.534315\n",
            "  8930/30000: episode: 1490, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2511.333 [92.000, 5656.000],  loss: 32.495510, mae: 13.148164, mean_q: 30.221161\n",
            "  8936/30000: episode: 1491, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 75.000, mean reward: 12.500 [ 0.000, 40.000], mean action: 2610.167 [419.000, 5656.000],  loss: 29.231653, mae: 13.631196, mean_q: 31.117544\n",
            "  8942/30000: episode: 1492, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: 20.000, mean reward:  3.333 [ 0.000, 10.000], mean action: 4051.333 [2237.000, 5688.000],  loss: 23.964136, mae: 13.747308, mean_q: 31.383850\n",
            "  8948/30000: episode: 1493, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3125.667 [267.000, 5742.000],  loss: 42.756222, mae: 14.393505, mean_q: 32.673504\n",
            "  8954/30000: episode: 1494, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1884.667 [88.000, 4460.000],  loss: 42.052395, mae: 14.521664, mean_q: 32.831493\n",
            "  8960/30000: episode: 1495, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 2063.167 [10.000, 3820.000],  loss: 46.255871, mae: 15.391434, mean_q: 34.759899\n",
            "  8966/30000: episode: 1496, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2577.333 [1292.000, 5743.000],  loss: 21.690744, mae: 14.428945, mean_q: 33.654613\n",
            "  8972/30000: episode: 1497, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2402.833 [346.000, 5621.000],  loss: 37.217010, mae: 12.832774, mean_q: 30.288345\n",
            "  8978/30000: episode: 1498, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2622.000 [1459.000, 4927.000],  loss: 26.201057, mae: 14.264003, mean_q: 32.879776\n",
            "  8984/30000: episode: 1499, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2399.000 [1041.000, 4682.000],  loss: 33.288631, mae: 13.790122, mean_q: 31.050867\n",
            "  8990/30000: episode: 1500, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3058.833 [1576.000, 4188.000],  loss: 37.626553, mae: 14.274905, mean_q: 32.842686\n",
            "  8996/30000: episode: 1501, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3677.000 [1710.000, 4630.000],  loss: 27.836548, mae: 13.782997, mean_q: 31.867918\n",
            "  9002/30000: episode: 1502, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3940.333 [1315.000, 5729.000],  loss: 68.386116, mae: 14.734473, mean_q: 32.924576\n",
            "  9008/30000: episode: 1503, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 30.000, mean reward:  5.000 [ 0.000, 20.000], mean action: 2174.167 [1239.000, 3157.000],  loss: 25.177847, mae: 13.930645, mean_q: 31.663729\n",
            "  9014/30000: episode: 1504, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1868.167 [669.000, 4427.000],  loss: 22.479265, mae: 12.949981, mean_q: 30.251814\n",
            "  9020/30000: episode: 1505, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2336.333 [44.000, 4765.000],  loss: 21.723322, mae: 12.943024, mean_q: 30.278532\n",
            "  9026/30000: episode: 1506, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4136.667 [2733.000, 5509.000],  loss: 37.743427, mae: 13.338878, mean_q: 31.210546\n",
            "  9032/30000: episode: 1507, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3097.167 [1041.000, 4695.000],  loss: 23.989540, mae: 13.282174, mean_q: 30.920279\n",
            "  9038/30000: episode: 1508, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 3238.333 [1710.000, 5656.000],  loss: 26.107271, mae: 13.245086, mean_q: 30.540497\n",
            "  9044/30000: episode: 1509, duration: 0.148s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2169.500 [14.000, 5398.000],  loss: 18.155745, mae: 12.659125, mean_q: 29.889267\n",
            "  9050/30000: episode: 1510, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1416.167 [560.000, 3157.000],  loss: 24.854601, mae: 12.965065, mean_q: 30.240654\n",
            "  9056/30000: episode: 1511, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 35.000, mean reward:  5.833 [ 0.000, 35.000], mean action: 2430.833 [88.000, 4464.000],  loss: 16.633951, mae: 13.746452, mean_q: 31.607697\n",
            "  9062/30000: episode: 1512, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2747.167 [375.000, 5742.000],  loss: 35.547508, mae: 13.271050, mean_q: 31.056185\n",
            "  9068/30000: episode: 1513, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2621.333 [88.000, 5340.000],  loss: 23.693514, mae: 13.908741, mean_q: 31.987997\n",
            "  9074/30000: episode: 1514, duration: 0.179s, episode steps:   6, steps per second:  33, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2098.000 [273.000, 4491.000],  loss: 38.713085, mae: 13.227340, mean_q: 30.205154\n",
            "  9080/30000: episode: 1515, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2574.333 [142.000, 3954.000],  loss: 35.060585, mae: 14.532697, mean_q: 33.370564\n",
            "  9086/30000: episode: 1516, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 862.833 [88.000, 2858.000],  loss: 28.943649, mae: 13.449241, mean_q: 31.567526\n",
            "  9092/30000: episode: 1517, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2167.667 [522.000, 4420.000],  loss: 30.403204, mae: 14.194837, mean_q: 32.653091\n",
            "  9098/30000: episode: 1518, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  5.000], mean action: 3689.000 [3343.000, 4412.000],  loss: 23.727617, mae: 13.718038, mean_q: 32.077641\n",
            "  9104/30000: episode: 1519, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2987.000 [810.000, 5126.000],  loss: 26.035767, mae: 13.342727, mean_q: 31.826447\n",
            "  9110/30000: episode: 1520, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3527.000 [844.000, 5620.000],  loss: 36.908665, mae: 13.636922, mean_q: 31.856873\n",
            "  9116/30000: episode: 1521, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1753.333 [316.000, 3656.000],  loss: 29.830610, mae: 14.336154, mean_q: 32.315590\n",
            "  9122/30000: episode: 1522, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3017.500 [1559.000, 5608.000],  loss: 43.330662, mae: 13.821778, mean_q: 32.038700\n",
            "  9128/30000: episode: 1523, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3122.333 [542.000, 5233.000],  loss: 34.389225, mae: 14.236229, mean_q: 33.527943\n",
            "  9134/30000: episode: 1524, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1426.333 [273.000, 2555.000],  loss: 30.957426, mae: 14.699849, mean_q: 34.394886\n",
            "  9140/30000: episode: 1525, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3540.333 [1530.000, 5608.000],  loss: 38.418606, mae: 13.516843, mean_q: 31.890512\n",
            "  9146/30000: episode: 1526, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2607.833 [88.000, 5486.000],  loss: 35.369961, mae: 14.053409, mean_q: 32.509033\n",
            "  9152/30000: episode: 1527, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 130.000, mean reward: 21.667 [ 0.000, 35.000], mean action: 3081.833 [1530.000, 4185.000],  loss: 26.197630, mae: 13.423976, mean_q: 31.514757\n",
            "  9158/30000: episode: 1528, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 50.000, mean reward:  8.333 [ 0.000, 30.000], mean action: 3094.500 [1530.000, 5579.000],  loss: 42.252071, mae: 14.507935, mean_q: 33.633808\n",
            "  9164/30000: episode: 1529, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 1853.167 [858.000, 3606.000],  loss: 33.671234, mae: 14.045062, mean_q: 33.055752\n",
            "  9170/30000: episode: 1530, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1758.333 [10.000, 4894.000],  loss: 29.042168, mae: 14.679515, mean_q: 33.373753\n",
            "  9176/30000: episode: 1531, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 70.000, mean reward: 11.667 [ 0.000, 35.000], mean action: 2418.667 [1530.000, 4941.000],  loss: 32.528645, mae: 13.580327, mean_q: 31.951721\n",
            "  9182/30000: episode: 1532, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 742.167 [328.000, 1530.000],  loss: 36.552807, mae: 14.343491, mean_q: 33.440083\n",
            "  9188/30000: episode: 1533, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 30.000, mean reward:  5.000 [-5.000, 30.000], mean action: 2892.333 [1530.000, 4695.000],  loss: 37.944881, mae: 14.080189, mean_q: 32.580750\n",
            "  9194/30000: episode: 1534, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2336.000 [984.000, 3961.000],  loss: 28.865534, mae: 13.316998, mean_q: 31.038063\n",
            "  9200/30000: episode: 1535, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3657.500 [428.000, 5734.000],  loss: 25.676664, mae: 13.766557, mean_q: 31.827402\n",
            "  9206/30000: episode: 1536, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward: 30.000, mean reward:  5.000 [-5.000, 20.000], mean action: 2159.667 [88.000, 4212.000],  loss: 16.567970, mae: 13.694444, mean_q: 32.281330\n",
            "  9212/30000: episode: 1537, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: -10.000, mean reward: -1.667 [-5.000, 10.000], mean action: 1472.833 [1042.000, 1559.000],  loss: 18.736475, mae: 14.211749, mean_q: 32.753216\n",
            "  9218/30000: episode: 1538, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2470.333 [394.000, 5281.000],  loss: 35.826084, mae: 13.933606, mean_q: 32.703629\n",
            "  9224/30000: episode: 1539, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1604.833 [88.000, 4138.000],  loss: 32.873215, mae: 13.308697, mean_q: 31.321527\n",
            "  9230/30000: episode: 1540, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [-5.000, 15.000], mean action: 1211.333 [88.000, 4420.000],  loss: 27.286745, mae: 14.808112, mean_q: 34.070343\n",
            "  9236/30000: episode: 1541, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2876.167 [199.000, 5629.000],  loss: 29.302368, mae: 13.203162, mean_q: 31.144945\n",
            "  9242/30000: episode: 1542, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2762.667 [88.000, 5334.000],  loss: 38.988308, mae: 13.564427, mean_q: 31.683022\n",
            "  9248/30000: episode: 1543, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2965.000 [414.000, 5057.000],  loss: 28.699900, mae: 13.171944, mean_q: 30.740282\n",
            "  9254/30000: episode: 1544, duration: 0.262s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2333.667 [328.000, 4802.000],  loss: 24.099380, mae: 12.971242, mean_q: 31.097343\n",
            "  9260/30000: episode: 1545, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2383.167 [88.000, 5334.000],  loss: 34.063190, mae: 12.786958, mean_q: 30.413773\n",
            "  9266/30000: episode: 1546, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2087.500 [88.000, 4523.000],  loss: 60.901321, mae: 13.285619, mean_q: 32.027752\n",
            "  9272/30000: episode: 1547, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1313.667 [88.000, 4229.000],  loss: 27.567148, mae: 14.849773, mean_q: 34.813473\n",
            "  9278/30000: episode: 1548, duration: 0.274s, episode steps:   6, steps per second:  22, episode reward: 25.000, mean reward:  4.167 [-5.000, 30.000], mean action: 2786.000 [1530.000, 5656.000],  loss: 40.328335, mae: 14.426430, mean_q: 33.340851\n",
            "  9284/30000: episode: 1549, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1743.833 [328.000, 5605.000],  loss: 31.180504, mae: 14.610551, mean_q: 33.973507\n",
            "  9290/30000: episode: 1550, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward: 40.000, mean reward:  6.667 [-5.000, 25.000], mean action: 1923.333 [1530.000, 2367.000],  loss: 38.437851, mae: 13.647771, mean_q: 32.822346\n",
            "  9296/30000: episode: 1551, duration: 0.260s, episode steps:   6, steps per second:  23, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2828.500 [1202.000, 5707.000],  loss: 22.579096, mae: 13.035942, mean_q: 31.350021\n",
            "  9302/30000: episode: 1552, duration: 0.270s, episode steps:   6, steps per second:  22, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 4446.833 [1530.000, 5679.000],  loss: 28.562155, mae: 13.746437, mean_q: 32.930561\n",
            "  9308/30000: episode: 1553, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1954.833 [733.000, 5023.000],  loss: 19.104519, mae: 13.980046, mean_q: 32.909779\n",
            "  9314/30000: episode: 1554, duration: 0.233s, episode steps:   6, steps per second:  26, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 2059.500 [907.000, 4941.000],  loss: 27.588707, mae: 13.873029, mean_q: 32.897572\n",
            "  9320/30000: episode: 1555, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 25.000, mean reward:  4.167 [-5.000, 25.000], mean action: 2206.333 [1530.000, 3606.000],  loss: 28.599840, mae: 13.950302, mean_q: 33.130203\n",
            "  9326/30000: episode: 1556, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1885.000 [167.000, 5023.000],  loss: 22.653183, mae: 13.822814, mean_q: 33.271587\n",
            "  9332/30000: episode: 1557, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 40.000, mean reward:  6.667 [-5.000, 25.000], mean action: 1780.333 [781.000, 2367.000],  loss: 37.320805, mae: 14.202571, mean_q: 33.657856\n",
            "  9338/30000: episode: 1558, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1877.000 [1128.000, 2973.000],  loss: 31.013479, mae: 14.645108, mean_q: 34.247646\n",
            "  9344/30000: episode: 1559, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2177.833 [751.000, 3625.000],  loss: 34.203365, mae: 14.091286, mean_q: 32.604401\n",
            "  9350/30000: episode: 1560, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2753.500 [1512.000, 4921.000],  loss: 25.757261, mae: 13.603816, mean_q: 32.543953\n",
            "  9356/30000: episode: 1561, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3872.667 [1530.000, 5608.000],  loss: 32.009705, mae: 12.409377, mean_q: 30.886953\n",
            "  9362/30000: episode: 1562, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2133.000 [69.000, 4203.000],  loss: 26.603951, mae: 12.837325, mean_q: 30.515869\n",
            "  9368/30000: episode: 1563, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3503.000 [1530.000, 4844.000],  loss: 47.701275, mae: 13.483962, mean_q: 31.973391\n",
            "  9374/30000: episode: 1564, duration: 0.183s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3190.000 [1634.000, 4412.000],  loss: 42.470585, mae: 14.152366, mean_q: 33.268627\n",
            "  9380/30000: episode: 1565, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2165.333 [1021.000, 3587.000],  loss: 23.514387, mae: 13.297150, mean_q: 31.606400\n",
            "  9386/30000: episode: 1566, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3634.500 [1530.000, 5334.000],  loss: 23.311083, mae: 14.462177, mean_q: 33.726131\n",
            "  9392/30000: episode: 1567, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 2511.333 [1358.000, 4765.000],  loss: 38.802963, mae: 14.084615, mean_q: 33.222107\n",
            "  9398/30000: episode: 1568, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2076.667 [1139.000, 4259.000],  loss: 22.037378, mae: 12.761981, mean_q: 30.369219\n",
            "  9404/30000: episode: 1569, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3729.500 [238.000, 5486.000],  loss: 23.251755, mae: 14.621062, mean_q: 33.711742\n",
            "  9410/30000: episode: 1570, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2165.833 [591.000, 3545.000],  loss: 23.012018, mae: 13.433701, mean_q: 31.468546\n",
            "  9416/30000: episode: 1571, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4060.667 [1530.000, 5742.000],  loss: 29.571531, mae: 13.020583, mean_q: 30.775301\n",
            "  9422/30000: episode: 1572, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3670.167 [238.000, 5486.000],  loss: 33.639591, mae: 12.172397, mean_q: 29.620842\n",
            "  9428/30000: episode: 1573, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2100.667 [1730.000, 3954.000],  loss: 35.727085, mae: 12.234454, mean_q: 29.737143\n",
            "  9434/30000: episode: 1574, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3755.833 [522.000, 5486.000],  loss: 46.048157, mae: 13.988494, mean_q: 32.833729\n",
            "  9440/30000: episode: 1575, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2656.833 [1110.000, 5089.000],  loss: 36.170902, mae: 14.204831, mean_q: 33.423916\n",
            "  9446/30000: episode: 1576, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3376.500 [1145.000, 5239.000],  loss: 22.333977, mae: 12.995922, mean_q: 31.675026\n",
            "  9452/30000: episode: 1577, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2298.667 [519.000, 4738.000],  loss: 30.703644, mae: 14.023749, mean_q: 32.731773\n",
            "  9458/30000: episode: 1578, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2589.333 [363.000, 3804.000],  loss: 40.600498, mae: 13.208934, mean_q: 31.868921\n",
            "  9464/30000: episode: 1579, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3248.167 [105.000, 5258.000],  loss: 29.956762, mae: 14.281108, mean_q: 33.303589\n",
            "  9470/30000: episode: 1580, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 1728.667 [365.000, 3221.000],  loss: 34.926022, mae: 14.409343, mean_q: 33.713909\n",
            "  9476/30000: episode: 1581, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3150.667 [1530.000, 5656.000],  loss: 34.605541, mae: 14.111308, mean_q: 33.457973\n",
            "  9482/30000: episode: 1582, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2597.333 [262.000, 5168.000],  loss: 28.323654, mae: 12.968266, mean_q: 31.011927\n",
            "  9488/30000: episode: 1583, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3082.333 [1628.000, 5640.000],  loss: 27.482681, mae: 14.795119, mean_q: 34.748432\n",
            "  9494/30000: episode: 1584, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2106.500 [1730.000, 3921.000],  loss: 29.573837, mae: 15.039307, mean_q: 34.878403\n",
            "  9500/30000: episode: 1585, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2100.667 [1730.000, 3954.000],  loss: 35.737438, mae: 13.239179, mean_q: 31.149176\n",
            "  9506/30000: episode: 1586, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 35.000, mean reward:  5.833 [ 0.000, 25.000], mean action: 2846.833 [238.000, 4307.000],  loss: 24.921753, mae: 13.783753, mean_q: 33.309349\n",
            "  9512/30000: episode: 1587, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3159.333 [159.000, 5742.000],  loss: 28.405632, mae: 12.691436, mean_q: 30.928650\n",
            "  9518/30000: episode: 1588, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2799.833 [24.000, 4695.000],  loss: 21.324902, mae: 14.181043, mean_q: 33.662750\n",
            "  9524/30000: episode: 1589, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1996.000 [1559.000, 2795.000],  loss: 40.326355, mae: 14.242359, mean_q: 34.017216\n",
            "  9530/30000: episode: 1590, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 35.000, mean reward:  5.833 [ 0.000, 35.000], mean action: 2864.667 [1102.000, 4403.000],  loss: 33.717316, mae: 14.414463, mean_q: 33.479488\n",
            "  9536/30000: episode: 1591, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1311.500 [750.000, 1901.000],  loss: 30.220703, mae: 13.873296, mean_q: 32.608467\n",
            "  9542/30000: episode: 1592, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1555.000 [42.000, 3712.000],  loss: 29.666557, mae: 13.037407, mean_q: 30.957909\n",
            "  9548/30000: episode: 1593, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1744.667 [238.000, 2125.000],  loss: 17.708780, mae: 13.206730, mean_q: 30.797438\n",
            "  9554/30000: episode: 1594, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3498.833 [1476.000, 5057.000],  loss: 37.794353, mae: 12.383621, mean_q: 29.716728\n",
            "  9560/30000: episode: 1595, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3238.500 [363.000, 5707.000],  loss: 26.186890, mae: 13.318822, mean_q: 31.762121\n",
            "  9566/30000: episode: 1596, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4346.000 [2969.000, 5629.000],  loss: 27.571686, mae: 14.028385, mean_q: 32.075588\n",
            "  9572/30000: episode: 1597, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2896.500 [1530.000, 5023.000],  loss: 32.995930, mae: 13.651015, mean_q: 32.289898\n",
            "  9578/30000: episode: 1598, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2814.500 [591.000, 5483.000],  loss: 50.815781, mae: 13.154603, mean_q: 30.904745\n",
            "  9584/30000: episode: 1599, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2714.167 [560.000, 4407.000],  loss: 21.034126, mae: 14.293748, mean_q: 33.300110\n",
            "  9590/30000: episode: 1600, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2871.000 [238.000, 4250.000],  loss: 27.837732, mae: 13.552348, mean_q: 31.691582\n",
            "  9596/30000: episode: 1601, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward: 15.000, mean reward:  2.500 [-5.000, 25.000], mean action: 2230.667 [238.000, 3054.000],  loss: 30.207132, mae: 14.221542, mean_q: 33.252453\n",
            "  9602/30000: episode: 1602, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 2669.167 [238.000, 5486.000],  loss: 25.400864, mae: 13.715112, mean_q: 32.537624\n",
            "  9608/30000: episode: 1603, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3686.667 [3395.000, 4332.000],  loss: 45.555027, mae: 13.034554, mean_q: 31.053522\n",
            "  9614/30000: episode: 1604, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3936.667 [238.000, 4937.000],  loss: 28.126333, mae: 13.375306, mean_q: 31.481798\n",
            "  9620/30000: episode: 1605, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1509.167 [756.000, 2440.000],  loss: 46.030537, mae: 13.669662, mean_q: 31.881174\n",
            "  9626/30000: episode: 1606, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 3032.500 [1059.000, 5579.000],  loss: 25.482172, mae: 12.808162, mean_q: 30.926422\n",
            "  9632/30000: episode: 1607, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3469.167 [858.000, 5410.000],  loss: 24.087334, mae: 13.284424, mean_q: 31.846102\n",
            "  9638/30000: episode: 1608, duration: 0.186s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3324.167 [2291.000, 5404.000],  loss: 25.396826, mae: 15.709966, mean_q: 36.030434\n",
            "  9644/30000: episode: 1609, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4092.000 [2667.000, 5171.000],  loss: 30.039595, mae: 14.473171, mean_q: 33.897930\n",
            "  9650/30000: episode: 1610, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3358.167 [1622.000, 5467.000],  loss: 29.958967, mae: 14.285252, mean_q: 34.129810\n",
            "  9656/30000: episode: 1611, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3266.167 [394.000, 4185.000],  loss: 24.630606, mae: 13.072015, mean_q: 31.190683\n",
            "  9662/30000: episode: 1612, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 3243.667 [527.000, 5338.000],  loss: 39.618511, mae: 13.222568, mean_q: 31.509710\n",
            "  9668/30000: episode: 1613, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3098.667 [587.000, 5656.000],  loss: 28.134943, mae: 14.516411, mean_q: 34.247929\n",
            "  9674/30000: episode: 1614, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2441.833 [1202.000, 5149.000],  loss: 35.889389, mae: 12.684292, mean_q: 31.053225\n",
            "  9680/30000: episode: 1615, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3262.000 [1530.000, 5679.000],  loss: 38.989979, mae: 13.104976, mean_q: 31.650778\n",
            "  9686/30000: episode: 1616, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3663.833 [1276.000, 5023.000],  loss: 30.961029, mae: 14.765584, mean_q: 35.472794\n",
            "  9692/30000: episode: 1617, duration: 0.232s, episode steps:   6, steps per second:  26, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3207.667 [755.000, 5289.000],  loss: 46.741192, mae: 14.537571, mean_q: 33.683540\n",
            "  9698/30000: episode: 1618, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2386.333 [1398.000, 3712.000],  loss: 33.488750, mae: 13.695778, mean_q: 32.064999\n",
            "  9704/30000: episode: 1619, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2888.500 [1223.000, 5219.000],  loss: 35.738262, mae: 13.972679, mean_q: 32.986019\n",
            "  9710/30000: episode: 1620, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2440.667 [1239.000, 3606.000],  loss: 39.274544, mae: 13.237656, mean_q: 31.167818\n",
            "  9716/30000: episode: 1621, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3077.000 [238.000, 5023.000],  loss: 22.460037, mae: 15.569183, mean_q: 36.318027\n",
            "  9722/30000: episode: 1622, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3167.667 [642.000, 5656.000],  loss: 46.717453, mae: 14.401894, mean_q: 33.278858\n",
            "  9728/30000: episode: 1623, duration: 0.257s, episode steps:   6, steps per second:  23, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2724.833 [1522.000, 3606.000],  loss: 47.546753, mae: 13.168266, mean_q: 30.797615\n",
            "  9734/30000: episode: 1624, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 1989.167 [238.000, 5579.000],  loss: 25.450872, mae: 14.453504, mean_q: 32.804836\n",
            "  9740/30000: episode: 1625, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3691.667 [238.000, 5486.000],  loss: 46.759594, mae: 13.019099, mean_q: 30.997293\n",
            "  9746/30000: episode: 1626, duration: 0.240s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2707.333 [1094.000, 5264.000],  loss: 27.628889, mae: 13.186290, mean_q: 30.894747\n",
            "  9752/30000: episode: 1627, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3268.000 [1372.000, 5193.000],  loss: 16.447206, mae: 13.717435, mean_q: 32.171490\n",
            "  9758/30000: episode: 1628, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3347.833 [547.000, 5656.000],  loss: 33.474232, mae: 13.365863, mean_q: 31.342520\n",
            "  9764/30000: episode: 1629, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 20.000, mean reward:  3.333 [-5.000, 25.000], mean action: 2646.667 [320.000, 3904.000],  loss: 40.342789, mae: 13.145890, mean_q: 31.003027\n",
            "  9770/30000: episode: 1630, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4490.500 [2984.000, 5723.000],  loss: 20.971972, mae: 13.361537, mean_q: 30.727438\n",
            "  9776/30000: episode: 1631, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 35.000, mean reward:  5.833 [ 0.000, 15.000], mean action: 2083.500 [158.000, 4208.000],  loss: 36.191814, mae: 12.595126, mean_q: 30.092857\n",
            "  9782/30000: episode: 1632, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4341.833 [2277.000, 5640.000],  loss: 32.438049, mae: 13.707664, mean_q: 31.476236\n",
            "  9788/30000: episode: 1633, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2515.667 [273.000, 5023.000],  loss: 32.847939, mae: 14.044983, mean_q: 31.833822\n",
            "  9794/30000: episode: 1634, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2800.167 [848.000, 5576.000],  loss: 30.405237, mae: 14.433553, mean_q: 32.733547\n",
            "  9800/30000: episode: 1635, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 2861.333 [946.000, 5023.000],  loss: 42.521992, mae: 13.795346, mean_q: 31.954674\n",
            "  9806/30000: episode: 1636, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2772.833 [1809.000, 5263.000],  loss: 25.839178, mae: 13.050236, mean_q: 30.746183\n",
            "  9812/30000: episode: 1637, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2025.500 [69.000, 3509.000],  loss: 43.682209, mae: 13.204715, mean_q: 31.061638\n",
            "  9818/30000: episode: 1638, duration: 0.185s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2354.000 [493.000, 5013.000],  loss: 22.081564, mae: 13.258763, mean_q: 31.366150\n",
            "  9824/30000: episode: 1639, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3435.333 [68.000, 4208.000],  loss: 30.464159, mae: 13.605828, mean_q: 32.476727\n",
            "  9830/30000: episode: 1640, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 1960.333 [476.000, 4208.000],  loss: 27.997633, mae: 14.118037, mean_q: 33.028378\n",
            "  9836/30000: episode: 1641, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 4208.000 [4208.000, 4208.000],  loss: 28.952085, mae: 12.913361, mean_q: 31.282328\n",
            "  9842/30000: episode: 1642, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3920.833 [3219.000, 5073.000],  loss: 47.502888, mae: 14.029289, mean_q: 33.284882\n",
            "  9848/30000: episode: 1643, duration: 0.164s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3439.833 [2263.000, 4675.000],  loss: 29.570387, mae: 13.937112, mean_q: 33.542439\n",
            "  9854/30000: episode: 1644, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 4208.000 [4208.000, 4208.000],  loss: 22.892344, mae: 13.002243, mean_q: 31.666725\n",
            "  9860/30000: episode: 1645, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1948.833 [1419.000, 2544.000],  loss: 40.763378, mae: 13.361819, mean_q: 32.448723\n",
            "  9866/30000: episode: 1646, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3372.167 [1287.000, 5340.000],  loss: 30.058084, mae: 12.902145, mean_q: 31.701080\n",
            "  9872/30000: episode: 1647, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [-5.000, 20.000], mean action: 5279.500 [3712.000, 5593.000],  loss: 22.471411, mae: 13.749446, mean_q: 33.329880\n",
            "  9878/30000: episode: 1648, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3007.833 [158.000, 5747.000],  loss: 40.372509, mae: 13.765373, mean_q: 32.807102\n",
            "  9884/30000: episode: 1649, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3752.000 [1153.000, 5023.000],  loss: 30.989725, mae: 13.301137, mean_q: 31.507774\n",
            "  9890/30000: episode: 1650, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [-5.000, 10.000], mean action: 2884.333 [273.000, 4954.000],  loss: 23.175232, mae: 14.104718, mean_q: 33.012413\n",
            "  9896/30000: episode: 1651, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [-5.000, 20.000], mean action: 4966.000 [3712.000, 5593.000],  loss: 25.618238, mae: 13.460305, mean_q: 32.373482\n",
            "  9902/30000: episode: 1652, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 4461.000 [3651.000, 5263.000],  loss: 45.608425, mae: 13.382233, mean_q: 32.343327\n",
            "  9908/30000: episode: 1653, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2967.000 [781.000, 4208.000],  loss: 40.606709, mae: 13.902393, mean_q: 32.520401\n",
            "  9914/30000: episode: 1654, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3969.500 [3509.000, 4840.000],  loss: 26.201075, mae: 13.590039, mean_q: 32.155056\n",
            "  9920/30000: episode: 1655, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2987.167 [591.000, 4589.000],  loss: 32.761894, mae: 13.721330, mean_q: 33.055550\n",
            "  9926/30000: episode: 1656, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3224.833 [124.000, 4702.000],  loss: 53.323654, mae: 14.150546, mean_q: 33.992264\n",
            "  9932/30000: episode: 1657, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3777.167 [2255.000, 5233.000],  loss: 26.256845, mae: 14.885058, mean_q: 34.675938\n",
            "  9938/30000: episode: 1658, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3100.667 [1415.000, 4824.000],  loss: 33.347420, mae: 14.587453, mean_q: 34.311749\n",
            "  9944/30000: episode: 1659, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3669.000 [1693.000, 5662.000],  loss: 32.515362, mae: 13.913983, mean_q: 32.827702\n",
            "  9950/30000: episode: 1660, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3229.500 [249.000, 4404.000],  loss: 22.395676, mae: 13.524730, mean_q: 32.531937\n",
            "  9956/30000: episode: 1661, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 3732.500 [1596.000, 5742.000],  loss: 21.059204, mae: 14.331590, mean_q: 33.066452\n",
            "  9962/30000: episode: 1662, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2820.333 [4.000, 5147.000],  loss: 37.381741, mae: 13.937411, mean_q: 32.637390\n",
            "  9968/30000: episode: 1663, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2725.333 [249.000, 3712.000],  loss: 33.731197, mae: 13.462841, mean_q: 31.853912\n",
            "  9974/30000: episode: 1664, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2642.333 [1253.000, 3951.000],  loss: 23.844687, mae: 11.808988, mean_q: 29.259855\n",
            "  9980/30000: episode: 1665, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2848.000 [534.000, 5508.000],  loss: 25.470404, mae: 13.980306, mean_q: 32.667957\n",
            "  9986/30000: episode: 1666, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 1699.167 [1419.000, 3100.000],  loss: 30.401159, mae: 13.519184, mean_q: 31.982445\n",
            "  9992/30000: episode: 1667, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2899.667 [1132.000, 4208.000],  loss: 20.106222, mae: 13.454404, mean_q: 31.999445\n",
            "  9998/30000: episode: 1668, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3108.500 [238.000, 5449.000],  loss: 21.398405, mae: 14.026765, mean_q: 32.330894\n",
            " 10004/30000: episode: 1669, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 3179.500 [1473.000, 5017.000],  loss: 42.027908, mae: 13.910975, mean_q: 32.819916\n",
            " 10010/30000: episode: 1670, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2350.667 [312.000, 3820.000],  loss: 37.962818, mae: 15.054709, mean_q: 34.911869\n",
            " 10016/30000: episode: 1671, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2635.167 [252.000, 5626.000],  loss: 34.972912, mae: 14.948201, mean_q: 34.515408\n",
            " 10022/30000: episode: 1672, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 35.000, mean reward:  5.833 [ 0.000, 30.000], mean action: 3327.500 [150.000, 5703.000],  loss: 38.179657, mae: 16.900442, mean_q: 37.850414\n",
            " 10028/30000: episode: 1673, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3519.833 [1393.000, 5703.000],  loss: 56.524914, mae: 17.980402, mean_q: 40.102951\n",
            " 10034/30000: episode: 1674, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  5.000], mean action: 5312.333 [3479.000, 5679.000],  loss: 27.088661, mae: 16.303576, mean_q: 37.346924\n",
            " 10040/30000: episode: 1675, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2375.333 [150.000, 4573.000],  loss: 42.465424, mae: 16.420458, mean_q: 37.055721\n",
            " 10046/30000: episode: 1676, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2518.333 [150.000, 5593.000],  loss: 38.298069, mae: 15.955752, mean_q: 36.684185\n",
            " 10052/30000: episode: 1677, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 30.000, mean reward:  5.000 [ 0.000, 25.000], mean action: 3349.167 [425.000, 5703.000],  loss: 32.603680, mae: 16.624809, mean_q: 37.869114\n",
            " 10058/30000: episode: 1678, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2750.000 [150.000, 4725.000],  loss: 27.551247, mae: 14.752780, mean_q: 35.249844\n",
            " 10064/30000: episode: 1679, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2390.500 [440.000, 4138.000],  loss: 69.941460, mae: 16.172575, mean_q: 37.756931\n",
            " 10070/30000: episode: 1680, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 4607.667 [1596.000, 5742.000],  loss: 38.364391, mae: 14.934650, mean_q: 35.958942\n",
            " 10076/30000: episode: 1681, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 35.000, mean reward:  5.833 [ 0.000, 25.000], mean action: 4012.000 [2640.000, 5628.000],  loss: 42.004848, mae: 14.841924, mean_q: 34.577065\n",
            " 10082/30000: episode: 1682, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3308.167 [1322.000, 4486.000],  loss: 45.472622, mae: 14.412297, mean_q: 34.145699\n",
            " 10088/30000: episode: 1683, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3176.667 [1110.000, 5273.000],  loss: 52.679226, mae: 16.217388, mean_q: 36.429379\n",
            " 10094/30000: episode: 1684, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3951.000 [1132.000, 4718.000],  loss: 31.602091, mae: 15.006757, mean_q: 34.991402\n",
            " 10100/30000: episode: 1685, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 4464.000 [4464.000, 4464.000],  loss: 56.431149, mae: 16.207644, mean_q: 37.950741\n",
            " 10106/30000: episode: 1686, duration: 0.265s, episode steps:   6, steps per second:  23, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 4464.000 [4464.000, 4464.000],  loss: 50.244980, mae: 14.803410, mean_q: 35.235401\n",
            " 10112/30000: episode: 1687, duration: 0.245s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3383.167 [1439.000, 4498.000],  loss: 53.126125, mae: 15.585991, mean_q: 37.010601\n",
            " 10118/30000: episode: 1688, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3640.167 [1128.000, 4905.000],  loss: 45.108021, mae: 15.681119, mean_q: 36.656818\n",
            " 10124/30000: episode: 1689, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 4464.000 [4464.000, 4464.000],  loss: 53.792683, mae: 17.171280, mean_q: 40.531727\n",
            " 10130/30000: episode: 1690, duration: 0.238s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2709.000 [1020.000, 4464.000],  loss: 46.450787, mae: 16.833624, mean_q: 39.892563\n",
            " 10136/30000: episode: 1691, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 4464.000 [4464.000, 4464.000],  loss: 47.621311, mae: 15.067409, mean_q: 36.114918\n",
            " 10142/30000: episode: 1692, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 4464.000 [4464.000, 4464.000],  loss: 37.507366, mae: 14.941422, mean_q: 36.235069\n",
            " 10148/30000: episode: 1693, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 4464.000 [4464.000, 4464.000],  loss: 32.835102, mae: 15.332730, mean_q: 36.541233\n",
            " 10154/30000: episode: 1694, duration: 0.262s, episode steps:   6, steps per second:  23, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3948.500 [935.000, 5348.000],  loss: 48.901295, mae: 16.271788, mean_q: 37.519123\n",
            " 10160/30000: episode: 1695, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2640.167 [394.000, 4464.000],  loss: 48.548450, mae: 15.527877, mean_q: 36.448071\n",
            " 10166/30000: episode: 1696, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward: 20.000, mean reward:  3.333 [ 0.000, 10.000], mean action: 2798.000 [1021.000, 4464.000],  loss: 47.981426, mae: 16.893915, mean_q: 39.446789\n",
            " 10172/30000: episode: 1697, duration: 0.240s, episode steps:   6, steps per second:  25, episode reward: -10.000, mean reward: -1.667 [-5.000,  5.000], mean action: 3866.333 [1630.000, 4464.000],  loss: 49.594086, mae: 15.406987, mean_q: 36.296650\n",
            " 10178/30000: episode: 1698, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3890.167 [1153.000, 5126.000],  loss: 36.751205, mae: 16.860840, mean_q: 39.179253\n",
            " 10184/30000: episode: 1699, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2988.333 [844.000, 5449.000],  loss: 64.420898, mae: 15.747100, mean_q: 36.889492\n",
            " 10190/30000: episode: 1700, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: 130.000, mean reward: 21.667 [ 0.000, 35.000], mean action: 3977.167 [1988.000, 5608.000],  loss: 51.739758, mae: 16.923347, mean_q: 39.310825\n",
            " 10196/30000: episode: 1701, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 40.000, mean reward:  6.667 [ 0.000, 20.000], mean action: 3197.000 [2008.000, 5426.000],  loss: 45.918346, mae: 14.529988, mean_q: 34.308273\n",
            " 10202/30000: episode: 1702, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3394.500 [527.000, 4464.000],  loss: 38.812378, mae: 16.537931, mean_q: 38.250858\n",
            " 10208/30000: episode: 1703, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 135.000, mean reward: 22.500 [ 5.000, 35.000], mean action: 3911.167 [1596.000, 4852.000],  loss: 39.560516, mae: 15.469581, mean_q: 36.417416\n",
            " 10214/30000: episode: 1704, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 55.000, mean reward:  9.167 [ 0.000, 25.000], mean action: 1819.333 [106.000, 4852.000],  loss: 47.554855, mae: 16.193811, mean_q: 37.746780\n",
            " 10220/30000: episode: 1705, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4018.333 [2555.000, 5474.000],  loss: 36.126701, mae: 16.167805, mean_q: 36.913345\n",
            " 10226/30000: episode: 1706, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [-5.000, 10.000], mean action: 3782.667 [1520.000, 4849.000],  loss: 36.320038, mae: 14.973956, mean_q: 35.096004\n",
            " 10232/30000: episode: 1707, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 2426.333 [1074.000, 3921.000],  loss: 28.552454, mae: 14.962342, mean_q: 35.080795\n",
            " 10238/30000: episode: 1708, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1431.500 [442.000, 3587.000],  loss: 44.008305, mae: 15.543817, mean_q: 35.928715\n",
            " 10244/30000: episode: 1709, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3100.833 [489.000, 5348.000],  loss: 52.058186, mae: 15.340085, mean_q: 36.223644\n",
            " 10250/30000: episode: 1710, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3545.333 [350.000, 5348.000],  loss: 46.796169, mae: 15.010852, mean_q: 35.608978\n",
            " 10256/30000: episode: 1711, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3869.500 [1308.000, 5668.000],  loss: 56.120472, mae: 13.938218, mean_q: 33.223476\n",
            " 10262/30000: episode: 1712, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 1992.000 [356.000, 4954.000],  loss: 46.980164, mae: 15.387886, mean_q: 35.679832\n",
            " 10268/30000: episode: 1713, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1939.000 [1139.000, 2752.000],  loss: 31.221907, mae: 15.778889, mean_q: 36.790607\n",
            " 10274/30000: episode: 1714, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3184.000 [1413.000, 5679.000],  loss: 31.195017, mae: 15.022296, mean_q: 35.222836\n",
            " 10280/30000: episode: 1715, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2946.000 [238.000, 5099.000],  loss: 48.732220, mae: 15.261273, mean_q: 35.559345\n",
            " 10286/30000: episode: 1716, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [-5.000, 15.000], mean action: 2647.667 [1202.000, 4464.000],  loss: 43.768215, mae: 15.288902, mean_q: 35.468685\n",
            " 10292/30000: episode: 1717, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3528.833 [1139.000, 5608.000],  loss: 35.001686, mae: 14.433379, mean_q: 33.753338\n",
            " 10298/30000: episode: 1718, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 1972.500 [650.000, 2896.000],  loss: 44.553211, mae: 15.836698, mean_q: 35.942753\n",
            " 10304/30000: episode: 1719, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3070.333 [1596.000, 5233.000],  loss: 35.594852, mae: 15.307706, mean_q: 35.819717\n",
            " 10310/30000: episode: 1720, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3643.333 [1322.000, 4185.000],  loss: 35.827698, mae: 15.922646, mean_q: 36.727295\n",
            " 10316/30000: episode: 1721, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2836.667 [1714.000, 3888.000],  loss: 57.973782, mae: 15.995262, mean_q: 36.584824\n",
            " 10322/30000: episode: 1722, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 4140.500 [2027.000, 5099.000],  loss: 71.176109, mae: 16.543550, mean_q: 37.866531\n",
            " 10328/30000: episode: 1723, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2217.333 [106.000, 5129.000],  loss: 31.738752, mae: 16.883112, mean_q: 38.351559\n",
            " 10334/30000: episode: 1724, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4034.000 [1322.000, 4992.000],  loss: 44.056683, mae: 14.714382, mean_q: 34.210560\n",
            " 10340/30000: episode: 1725, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2985.000 [323.000, 4852.000],  loss: 42.082443, mae: 14.591395, mean_q: 33.792336\n",
            " 10346/30000: episode: 1726, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1733.500 [450.000, 4767.000],  loss: 36.976025, mae: 17.555264, mean_q: 39.551891\n",
            " 10352/30000: episode: 1727, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 1877.000 [1322.000, 1988.000],  loss: 39.702549, mae: 16.206461, mean_q: 37.092403\n",
            " 10358/30000: episode: 1728, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 1948.167 [1057.000, 2555.000],  loss: 42.190315, mae: 15.595802, mean_q: 36.284023\n",
            " 10364/30000: episode: 1729, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3391.667 [1988.000, 5256.000],  loss: 29.475164, mae: 16.037655, mean_q: 36.920456\n",
            " 10370/30000: episode: 1730, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 1816.500 [959.000, 1988.000],  loss: 34.052364, mae: 15.987742, mean_q: 36.852615\n",
            " 10376/30000: episode: 1731, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2848.667 [2279.000, 4048.000],  loss: 44.167217, mae: 15.447427, mean_q: 35.669666\n",
            " 10382/30000: episode: 1732, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2618.000 [172.000, 3711.000],  loss: 38.964657, mae: 14.667267, mean_q: 34.691525\n",
            " 10388/30000: episode: 1733, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1838.167 [571.000, 3100.000],  loss: 36.451401, mae: 14.563756, mean_q: 33.825008\n",
            " 10394/30000: episode: 1734, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: 30.000, mean reward:  5.000 [ 0.000, 30.000], mean action: 2423.667 [946.000, 3238.000],  loss: 29.566088, mae: 16.488184, mean_q: 37.913219\n",
            " 10400/30000: episode: 1735, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3123.500 [1245.000, 4796.000],  loss: 29.239450, mae: 15.331566, mean_q: 35.627182\n",
            " 10406/30000: episode: 1736, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2984.500 [650.000, 5652.000],  loss: 35.281605, mae: 14.626217, mean_q: 34.718452\n",
            " 10412/30000: episode: 1737, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 70.000, mean reward: 11.667 [ 0.000, 20.000], mean action: 3159.167 [2008.000, 4464.000],  loss: 40.968388, mae: 16.423141, mean_q: 38.475513\n",
            " 10418/30000: episode: 1738, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [-5.000, 30.000], mean action: 2645.500 [1988.000, 4163.000],  loss: 61.617512, mae: 15.072112, mean_q: 35.481205\n",
            " 10424/30000: episode: 1739, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1669.500 [1575.000, 2104.000],  loss: 48.312107, mae: 15.141457, mean_q: 35.537415\n",
            " 10430/30000: episode: 1740, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3610.167 [1693.000, 5099.000],  loss: 59.164829, mae: 17.291052, mean_q: 39.344120\n",
            " 10436/30000: episode: 1741, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2098.167 [61.000, 4632.000],  loss: 34.738213, mae: 15.847867, mean_q: 36.650734\n",
            " 10442/30000: episode: 1742, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3265.667 [2104.000, 3498.000],  loss: 44.835331, mae: 15.617009, mean_q: 36.749699\n",
            " 10448/30000: episode: 1743, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 3076.833 [1191.000, 4479.000],  loss: 31.474485, mae: 15.836207, mean_q: 36.709919\n",
            " 10454/30000: episode: 1744, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 3591.333 [1988.000, 5334.000],  loss: 43.354519, mae: 14.477786, mean_q: 34.251446\n",
            " 10460/30000: episode: 1745, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2554.667 [383.000, 4412.000],  loss: 25.861361, mae: 14.209538, mean_q: 33.416798\n",
            " 10466/30000: episode: 1746, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 2104.000 [2104.000, 2104.000],  loss: 45.904629, mae: 14.406878, mean_q: 34.028355\n",
            " 10472/30000: episode: 1747, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4089.000 [2104.000, 4486.000],  loss: 54.020367, mae: 16.541525, mean_q: 38.079769\n",
            " 10478/30000: episode: 1748, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 3139.667 [2184.000, 4486.000],  loss: 39.358780, mae: 16.608290, mean_q: 38.472057\n",
            " 10484/30000: episode: 1749, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4676.167 [3191.000, 5490.000],  loss: 52.038990, mae: 15.500175, mean_q: 36.425167\n",
            " 10490/30000: episode: 1750, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4077.333 [2809.000, 5269.000],  loss: 51.899036, mae: 16.255966, mean_q: 37.066105\n",
            " 10496/30000: episode: 1751, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3239.167 [2104.000, 5249.000],  loss: 48.207676, mae: 16.837412, mean_q: 38.799999\n",
            " 10502/30000: episode: 1752, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3375.500 [1523.000, 4954.000],  loss: 40.812965, mae: 13.653533, mean_q: 32.451572\n",
            " 10508/30000: episode: 1753, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3265.667 [2104.000, 3498.000],  loss: 31.443350, mae: 17.620405, mean_q: 40.265827\n",
            " 10514/30000: episode: 1754, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2660.833 [487.000, 3674.000],  loss: 67.977501, mae: 16.067909, mean_q: 37.493473\n",
            " 10520/30000: episode: 1755, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2918.000 [522.000, 5411.000],  loss: 39.957470, mae: 14.853803, mean_q: 34.886734\n",
            " 10526/30000: episode: 1756, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: 30.000, mean reward:  5.000 [-5.000, 35.000], mean action: 2693.000 [1613.000, 4632.000],  loss: 25.808863, mae: 14.602589, mean_q: 34.371250\n",
            " 10532/30000: episode: 1757, duration: 0.239s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3079.167 [1710.000, 5099.000],  loss: 23.718058, mae: 15.365817, mean_q: 35.984573\n",
            " 10538/30000: episode: 1758, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2155.667 [404.000, 4802.000],  loss: 61.363342, mae: 15.973499, mean_q: 36.924046\n",
            " 10544/30000: episode: 1759, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3265.667 [2104.000, 3498.000],  loss: 49.913376, mae: 14.122592, mean_q: 32.903866\n",
            " 10550/30000: episode: 1760, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2251.333 [350.000, 4590.000],  loss: 45.757050, mae: 14.800851, mean_q: 34.009151\n",
            " 10556/30000: episode: 1761, duration: 0.240s, episode steps:   6, steps per second:  25, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2547.000 [2104.000, 3766.000],  loss: 40.110516, mae: 15.520473, mean_q: 36.353664\n",
            " 10562/30000: episode: 1762, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3781.667 [2708.000, 4918.000],  loss: 43.462261, mae: 15.927505, mean_q: 36.193630\n",
            " 10568/30000: episode: 1763, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3204.167 [1438.000, 5283.000],  loss: 48.940525, mae: 15.157245, mean_q: 35.666424\n",
            " 10574/30000: episode: 1764, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3923.000 [1871.000, 5598.000],  loss: 50.419483, mae: 15.157501, mean_q: 35.594822\n",
            " 10580/30000: episode: 1765, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3034.000 [2060.000, 5742.000],  loss: 49.676075, mae: 16.576143, mean_q: 38.019775\n",
            " 10586/30000: episode: 1766, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3410.333 [1901.000, 4918.000],  loss: 24.504105, mae: 15.526328, mean_q: 35.332302\n",
            " 10592/30000: episode: 1767, duration: 0.283s, episode steps:   6, steps per second:  21, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 3644.833 [2555.000, 5640.000],  loss: 32.956680, mae: 15.709441, mean_q: 35.765892\n",
            " 10598/30000: episode: 1768, duration: 0.238s, episode steps:   6, steps per second:  25, episode reward: 30.000, mean reward:  5.000 [ 0.000, 20.000], mean action: 2494.000 [650.000, 5652.000],  loss: 37.312626, mae: 16.263445, mean_q: 36.055168\n",
            " 10604/30000: episode: 1769, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2326.500 [350.000, 5129.000],  loss: 40.874523, mae: 14.880898, mean_q: 34.279484\n",
            " 10610/30000: episode: 1770, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2931.333 [350.000, 5678.000],  loss: 58.520489, mae: 14.250633, mean_q: 32.995945\n",
            " 10616/30000: episode: 1771, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 3010.167 [1530.000, 4220.000],  loss: 33.165600, mae: 13.705231, mean_q: 31.482536\n",
            " 10622/30000: episode: 1772, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2808.167 [650.000, 5232.000],  loss: 34.447979, mae: 17.285469, mean_q: 38.107395\n",
            " 10628/30000: episode: 1773, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3194.333 [554.000, 5610.000],  loss: 48.233288, mae: 15.642644, mean_q: 34.805115\n",
            " 10634/30000: episode: 1774, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: 20.000, mean reward:  3.333 [-5.000, 25.000], mean action: 3167.000 [1523.000, 5698.000],  loss: 55.659729, mae: 15.119176, mean_q: 34.417362\n",
            " 10640/30000: episode: 1775, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2807.000 [1297.000, 4918.000],  loss: 46.858135, mae: 14.027949, mean_q: 32.633099\n",
            " 10646/30000: episode: 1776, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 2710.667 [344.000, 4829.000],  loss: 53.164806, mae: 15.634673, mean_q: 35.770363\n",
            " 10652/30000: episode: 1777, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2946.167 [238.000, 5099.000],  loss: 38.567238, mae: 14.651482, mean_q: 33.350357\n",
            " 10658/30000: episode: 1778, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [-5.000, 30.000], mean action: 3183.500 [915.000, 4829.000],  loss: 32.457546, mae: 15.012265, mean_q: 34.180038\n",
            " 10664/30000: episode: 1779, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3222.500 [1622.000, 5249.000],  loss: 57.123795, mae: 17.155998, mean_q: 38.702148\n",
            " 10670/30000: episode: 1780, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2685.167 [350.000, 5171.000],  loss: 51.619465, mae: 14.983735, mean_q: 34.457684\n",
            " 10676/30000: episode: 1781, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3706.167 [489.000, 5742.000],  loss: 37.683376, mae: 15.540799, mean_q: 35.343548\n",
            " 10682/30000: episode: 1782, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 4292.333 [3221.000, 5348.000],  loss: 53.906876, mae: 15.798611, mean_q: 35.701550\n",
            " 10688/30000: episode: 1783, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2781.667 [1136.000, 4213.000],  loss: 46.924637, mae: 15.538823, mean_q: 36.060715\n",
            " 10694/30000: episode: 1784, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3012.500 [1613.000, 5348.000],  loss: 35.015320, mae: 15.087138, mean_q: 35.151096\n",
            " 10700/30000: episode: 1785, duration: 0.164s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3372.333 [1772.000, 5348.000],  loss: 42.329956, mae: 15.597787, mean_q: 35.785679\n",
            " 10706/30000: episode: 1786, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3525.500 [350.000, 5180.000],  loss: 59.255466, mae: 15.898088, mean_q: 35.546852\n",
            " 10712/30000: episode: 1787, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 2149.333 [345.000, 4632.000],  loss: 31.528032, mae: 15.098258, mean_q: 34.390079\n",
            " 10718/30000: episode: 1788, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 1851.333 [269.000, 3525.000],  loss: 63.426792, mae: 16.451775, mean_q: 37.028706\n",
            " 10724/30000: episode: 1789, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3153.167 [1113.000, 5453.000],  loss: 39.091949, mae: 17.298399, mean_q: 38.841412\n",
            " 10730/30000: episode: 1790, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 65.000, mean reward: 10.833 [ 0.000, 35.000], mean action: 3699.333 [1970.000, 5348.000],  loss: 35.749237, mae: 16.516039, mean_q: 37.132587\n",
            " 10736/30000: episode: 1791, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2737.167 [350.000, 4632.000],  loss: 29.003653, mae: 16.964975, mean_q: 37.533688\n",
            " 10742/30000: episode: 1792, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 3783.167 [602.000, 5118.000],  loss: 28.948547, mae: 14.331875, mean_q: 33.265690\n",
            " 10748/30000: episode: 1793, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2508.500 [350.000, 4632.000],  loss: 34.317329, mae: 15.897206, mean_q: 35.939201\n",
            " 10754/30000: episode: 1794, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2614.500 [602.000, 4185.000],  loss: 48.343075, mae: 15.115193, mean_q: 34.007069\n",
            " 10760/30000: episode: 1795, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3782.167 [2555.000, 5085.000],  loss: 40.276073, mae: 15.506824, mean_q: 35.437489\n",
            " 10766/30000: episode: 1796, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3643.000 [350.000, 5679.000],  loss: 48.233841, mae: 15.110542, mean_q: 35.435207\n",
            " 10772/30000: episode: 1797, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1096.500 [350.000, 4829.000],  loss: 35.481728, mae: 14.948151, mean_q: 35.114655\n",
            " 10778/30000: episode: 1798, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3079.667 [290.000, 5564.000],  loss: 30.653807, mae: 15.543864, mean_q: 35.702972\n",
            " 10784/30000: episode: 1799, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3877.833 [1871.000, 5085.000],  loss: 33.746883, mae: 16.130119, mean_q: 37.573811\n",
            " 10790/30000: episode: 1800, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3794.333 [462.000, 5679.000],  loss: 36.769176, mae: 16.529020, mean_q: 37.453629\n",
            " 10796/30000: episode: 1801, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2866.000 [334.000, 5188.000],  loss: 37.242191, mae: 15.481982, mean_q: 35.741718\n",
            " 10802/30000: episode: 1802, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2920.167 [708.000, 5447.000],  loss: 47.560528, mae: 14.978718, mean_q: 34.403469\n",
            " 10808/30000: episode: 1803, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 1751.000 [547.000, 4840.000],  loss: 22.873449, mae: 14.972844, mean_q: 33.832119\n",
            " 10814/30000: episode: 1804, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 3979.667 [2687.000, 5602.000],  loss: 51.917767, mae: 15.674380, mean_q: 35.420429\n",
            " 10820/30000: episode: 1805, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3962.500 [1958.000, 5656.000],  loss: 50.116764, mae: 17.378876, mean_q: 38.580830\n",
            " 10826/30000: episode: 1806, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2442.000 [1239.000, 4588.000],  loss: 42.156685, mae: 16.746466, mean_q: 37.027328\n",
            " 10832/30000: episode: 1807, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3738.833 [1082.000, 4832.000],  loss: 57.088001, mae: 15.880572, mean_q: 36.483925\n",
            " 10838/30000: episode: 1808, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3734.333 [2607.000, 4632.000],  loss: 39.059792, mae: 15.828980, mean_q: 36.279831\n",
            " 10844/30000: episode: 1809, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3821.833 [1970.000, 5626.000],  loss: 52.586197, mae: 17.840439, mean_q: 40.131092\n",
            " 10850/30000: episode: 1810, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2450.333 [39.000, 4632.000],  loss: 43.382446, mae: 16.087179, mean_q: 36.779873\n",
            " 10856/30000: episode: 1811, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2353.167 [312.000, 5544.000],  loss: 42.861938, mae: 16.163126, mean_q: 35.868603\n",
            " 10862/30000: episode: 1812, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1363.333 [580.000, 2687.000],  loss: 30.107824, mae: 15.388726, mean_q: 35.249928\n",
            " 10868/30000: episode: 1813, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3398.667 [2107.000, 4829.000],  loss: 35.683849, mae: 15.984250, mean_q: 36.129429\n",
            " 10874/30000: episode: 1814, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [-5.000, 20.000], mean action: 903.333 [106.000, 1178.000],  loss: 60.018219, mae: 17.319275, mean_q: 37.954380\n",
            " 10880/30000: episode: 1815, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3124.333 [580.000, 5564.000],  loss: 49.370087, mae: 15.631894, mean_q: 35.455589\n",
            " 10886/30000: episode: 1816, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3044.167 [2058.000, 5249.000],  loss: 42.232227, mae: 15.097885, mean_q: 34.073372\n",
            " 10892/30000: episode: 1817, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2074.500 [1571.000, 3651.000],  loss: 52.949310, mae: 15.084514, mean_q: 34.523773\n",
            " 10898/30000: episode: 1818, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3988.333 [2495.000, 5405.000],  loss: 41.726700, mae: 15.699966, mean_q: 35.782742\n",
            " 10904/30000: episode: 1819, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2037.333 [27.000, 3798.000],  loss: 44.546719, mae: 15.701118, mean_q: 35.250221\n",
            " 10910/30000: episode: 1820, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2644.167 [238.000, 4829.000],  loss: 27.284691, mae: 16.137266, mean_q: 37.110683\n",
            " 10916/30000: episode: 1821, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2952.167 [449.000, 5281.000],  loss: 30.577650, mae: 15.580101, mean_q: 36.378620\n",
            " 10922/30000: episode: 1822, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2389.667 [1094.000, 5085.000],  loss: 20.747810, mae: 16.250952, mean_q: 37.334942\n",
            " 10928/30000: episode: 1823, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2536.833 [1438.000, 4829.000],  loss: 38.556011, mae: 15.075471, mean_q: 35.976971\n",
            " 10934/30000: episode: 1824, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 65.000, mean reward: 10.833 [ 0.000, 30.000], mean action: 2100.167 [27.000, 4239.000],  loss: 46.372837, mae: 16.707718, mean_q: 38.311962\n",
            " 10940/30000: episode: 1825, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 2086.000 [847.000, 3283.000],  loss: 23.543411, mae: 14.964997, mean_q: 35.930332\n",
            " 10946/30000: episode: 1826, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 145.000, mean reward: 24.167 [ 0.000, 35.000], mean action: 4140.000 [2444.000, 5656.000],  loss: 41.759785, mae: 13.484868, mean_q: 32.743664\n",
            " 10952/30000: episode: 1827, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2432.333 [580.000, 4543.000],  loss: 42.145245, mae: 16.365885, mean_q: 37.473270\n",
            " 10958/30000: episode: 1828, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3927.000 [1886.000, 5211.000],  loss: 38.040913, mae: 14.709498, mean_q: 34.196835\n",
            " 10964/30000: episode: 1829, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1754.000 [551.000, 2692.000],  loss: 39.672905, mae: 16.263725, mean_q: 37.408314\n",
            " 10970/30000: episode: 1830, duration: 0.274s, episode steps:   6, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2545.167 [1057.000, 4982.000],  loss: 43.597778, mae: 15.667554, mean_q: 36.679768\n",
            " 10976/30000: episode: 1831, duration: 0.266s, episode steps:   6, steps per second:  23, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3798.333 [2444.000, 5608.000],  loss: 37.544888, mae: 15.630562, mean_q: 35.325802\n",
            " 10982/30000: episode: 1832, duration: 0.452s, episode steps:   6, steps per second:  13, episode reward: 50.000, mean reward:  8.333 [-5.000, 35.000], mean action: 3504.667 [945.000, 4840.000],  loss: 48.280991, mae: 14.398182, mean_q: 33.332253\n",
            " 10988/30000: episode: 1833, duration: 0.465s, episode steps:   6, steps per second:  13, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2719.833 [1724.000, 4829.000],  loss: 48.883793, mae: 14.918061, mean_q: 34.186970\n",
            " 10994/30000: episode: 1834, duration: 0.406s, episode steps:   6, steps per second:  15, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2007.000 [580.000, 4829.000],  loss: 16.939726, mae: 15.118810, mean_q: 34.397675\n",
            " 11000/30000: episode: 1835, duration: 0.311s, episode steps:   6, steps per second:  19, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 3698.500 [886.000, 5736.000],  loss: 36.076748, mae: 16.347006, mean_q: 36.973640\n",
            " 11006/30000: episode: 1836, duration: 0.305s, episode steps:   6, steps per second:  20, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1713.667 [156.000, 3784.000],  loss: 39.936214, mae: 16.244778, mean_q: 36.889553\n",
            " 11012/30000: episode: 1837, duration: 0.366s, episode steps:   6, steps per second:  16, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2162.000 [106.000, 3651.000],  loss: 35.778957, mae: 16.217379, mean_q: 36.446865\n",
            " 11018/30000: episode: 1838, duration: 0.314s, episode steps:   6, steps per second:  19, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2478.667 [946.000, 3826.000],  loss: 39.584049, mae: 16.087778, mean_q: 36.561169\n",
            " 11024/30000: episode: 1839, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3867.333 [224.000, 5626.000],  loss: 45.300823, mae: 14.962842, mean_q: 35.151440\n",
            " 11030/30000: episode: 1840, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward: 60.000, mean reward: 10.000 [ 0.000, 30.000], mean action: 3295.333 [808.000, 5608.000],  loss: 39.836651, mae: 16.047134, mean_q: 36.005886\n",
            " 11036/30000: episode: 1841, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3435.500 [2263.000, 5564.000],  loss: 34.650894, mae: 13.748138, mean_q: 32.144337\n",
            " 11042/30000: episode: 1842, duration: 0.212s, episode steps:   6, steps per second:  28, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4625.500 [2128.000, 5718.000],  loss: 41.080334, mae: 14.370613, mean_q: 32.987339\n",
            " 11048/30000: episode: 1843, duration: 0.257s, episode steps:   6, steps per second:  23, episode reward:  5.000, mean reward:  0.833 [-5.000, 25.000], mean action: 4413.833 [238.000, 5249.000],  loss: 49.288258, mae: 14.334743, mean_q: 33.261158\n",
            " 11054/30000: episode: 1844, duration: 0.284s, episode steps:   6, steps per second:  21, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2584.500 [524.000, 5117.000],  loss: 38.094112, mae: 14.815487, mean_q: 34.216297\n",
            " 11060/30000: episode: 1845, duration: 0.241s, episode steps:   6, steps per second:  25, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1683.500 [1074.000, 3560.000],  loss: 38.439373, mae: 15.924324, mean_q: 36.349697\n",
            " 11066/30000: episode: 1846, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 3095.000 [172.000, 5249.000],  loss: 37.367405, mae: 16.396513, mean_q: 37.004272\n",
            " 11072/30000: episode: 1847, duration: 0.317s, episode steps:   6, steps per second:  19, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2825.333 [350.000, 4185.000],  loss: 27.090960, mae: 15.651002, mean_q: 35.357330\n",
            " 11078/30000: episode: 1848, duration: 0.284s, episode steps:   6, steps per second:  21, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3731.333 [649.000, 5715.000],  loss: 70.243095, mae: 17.178476, mean_q: 37.789326\n",
            " 11084/30000: episode: 1849, duration: 0.264s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2884.667 [1403.000, 4847.000],  loss: 42.199200, mae: 15.669376, mean_q: 35.168434\n",
            " 11090/30000: episode: 1850, duration: 0.268s, episode steps:   6, steps per second:  22, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3359.333 [242.000, 5564.000],  loss: 33.045452, mae: 15.891894, mean_q: 36.282787\n",
            " 11096/30000: episode: 1851, duration: 0.221s, episode steps:   6, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3515.000 [249.000, 5652.000],  loss: 33.078896, mae: 17.833529, mean_q: 39.755184\n",
            " 11102/30000: episode: 1852, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2182.667 [1160.000, 3908.000],  loss: 49.006474, mae: 14.632907, mean_q: 34.633354\n",
            " 11108/30000: episode: 1853, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3084.667 [1191.000, 5545.000],  loss: 51.385548, mae: 15.856139, mean_q: 36.932476\n",
            " 11114/30000: episode: 1854, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3704.167 [3287.000, 5398.000],  loss: 40.484627, mae: 15.113574, mean_q: 35.242947\n",
            " 11120/30000: episode: 1855, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3281.000 [1438.000, 5652.000],  loss: 41.829121, mae: 15.819457, mean_q: 37.361572\n",
            " 11126/30000: episode: 1856, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3081.833 [2698.000, 4412.000],  loss: 57.787415, mae: 15.806197, mean_q: 37.461887\n",
            " 11132/30000: episode: 1857, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2414.167 [1438.000, 3945.000],  loss: 38.626617, mae: 15.272130, mean_q: 36.230740\n",
            " 11138/30000: episode: 1858, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2174.500 [238.000, 4782.000],  loss: 31.709021, mae: 16.212080, mean_q: 37.472534\n",
            " 11144/30000: episode: 1859, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: 130.000, mean reward: 21.667 [ 0.000, 35.000], mean action: 3614.667 [238.000, 5608.000],  loss: 34.208546, mae: 14.446880, mean_q: 34.026260\n",
            " 11150/30000: episode: 1860, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2371.667 [637.000, 4631.000],  loss: 42.167435, mae: 16.542578, mean_q: 37.833714\n",
            " 11156/30000: episode: 1861, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2886.167 [1074.000, 5211.000],  loss: 35.361050, mae: 14.571323, mean_q: 34.150349\n",
            " 11162/30000: episode: 1862, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2246.167 [238.000, 4701.000],  loss: 38.245308, mae: 15.208535, mean_q: 35.044708\n",
            " 11168/30000: episode: 1863, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1289.667 [42.000, 3287.000],  loss: 41.984238, mae: 14.165158, mean_q: 33.612114\n",
            " 11174/30000: episode: 1864, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3002.500 [562.000, 5249.000],  loss: 48.783386, mae: 15.448098, mean_q: 35.241604\n",
            " 11180/30000: episode: 1865, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3830.167 [717.000, 5544.000],  loss: 36.760395, mae: 15.514209, mean_q: 35.641964\n",
            " 11186/30000: episode: 1866, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3690.000 [1439.000, 5199.000],  loss: 39.592224, mae: 14.725132, mean_q: 34.944492\n",
            " 11192/30000: episode: 1867, duration: 0.297s, episode steps:   6, steps per second:  20, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2796.500 [238.000, 4220.000],  loss: 40.877674, mae: 15.799722, mean_q: 36.371639\n",
            " 11198/30000: episode: 1868, duration: 0.259s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2497.667 [156.000, 5096.000],  loss: 64.320663, mae: 15.555367, mean_q: 35.407551\n",
            " 11204/30000: episode: 1869, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2948.333 [1227.000, 5211.000],  loss: 45.115620, mae: 15.799575, mean_q: 36.410599\n",
            " 11210/30000: episode: 1870, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2817.167 [1227.000, 3991.000],  loss: 34.090984, mae: 15.067601, mean_q: 35.252090\n",
            " 11216/30000: episode: 1871, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3598.500 [1520.000, 5754.000],  loss: 40.635189, mae: 15.894001, mean_q: 36.884800\n",
            " 11222/30000: episode: 1872, duration: 0.191s, episode steps:   6, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2187.000 [1227.000, 3447.000],  loss: 39.721455, mae: 16.299707, mean_q: 36.840153\n",
            " 11228/30000: episode: 1873, duration: 0.337s, episode steps:   6, steps per second:  18, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3652.667 [592.000, 5658.000],  loss: 38.785843, mae: 15.048772, mean_q: 35.589291\n",
            " 11234/30000: episode: 1874, duration: 0.262s, episode steps:   6, steps per second:  23, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4456.667 [3136.000, 5742.000],  loss: 29.648268, mae: 14.706760, mean_q: 34.209736\n",
            " 11240/30000: episode: 1875, duration: 0.377s, episode steps:   6, steps per second:  16, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2681.500 [1227.000, 5299.000],  loss: 43.919682, mae: 15.426023, mean_q: 35.597195\n",
            " 11246/30000: episode: 1876, duration: 0.376s, episode steps:   6, steps per second:  16, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3188.000 [238.000, 5204.000],  loss: 29.711885, mae: 16.562502, mean_q: 37.766380\n",
            " 11252/30000: episode: 1877, duration: 0.293s, episode steps:   6, steps per second:  20, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2378.667 [1227.000, 4028.000],  loss: 30.769033, mae: 16.576967, mean_q: 37.933441\n",
            " 11258/30000: episode: 1878, duration: 0.308s, episode steps:   6, steps per second:  20, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2942.833 [1408.000, 4884.000],  loss: 36.378605, mae: 16.567495, mean_q: 38.158428\n",
            " 11264/30000: episode: 1879, duration: 0.294s, episode steps:   6, steps per second:  20, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2603.500 [1167.000, 4884.000],  loss: 43.485962, mae: 15.455993, mean_q: 36.393604\n",
            " 11270/30000: episode: 1880, duration: 0.412s, episode steps:   6, steps per second:  15, episode reward: 100.000, mean reward: 16.667 [ 0.000, 40.000], mean action: 3083.667 [1180.000, 4220.000],  loss: 41.861862, mae: 14.551845, mean_q: 34.431602\n",
            " 11276/30000: episode: 1881, duration: 0.388s, episode steps:   6, steps per second:  15, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1981.833 [1044.000, 3587.000],  loss: 44.567322, mae: 14.618222, mean_q: 33.798138\n",
            " 11282/30000: episode: 1882, duration: 0.570s, episode steps:   6, steps per second:  11, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4069.500 [3693.000, 4884.000],  loss: 49.885468, mae: 14.691726, mean_q: 34.791805\n",
            " 11288/30000: episode: 1883, duration: 0.508s, episode steps:   6, steps per second:  12, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1655.833 [253.000, 2779.000],  loss: 64.625519, mae: 16.012405, mean_q: 36.833973\n",
            " 11294/30000: episode: 1884, duration: 0.447s, episode steps:   6, steps per second:  13, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4007.500 [2495.000, 5496.000],  loss: 30.705721, mae: 17.010475, mean_q: 39.135647\n",
            " 11300/30000: episode: 1885, duration: 0.605s, episode steps:   6, steps per second:  10, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3705.000 [1059.000, 4884.000],  loss: 47.265621, mae: 17.435747, mean_q: 40.141270\n",
            " 11306/30000: episode: 1886, duration: 0.874s, episode steps:   6, steps per second:   7, episode reward: 110.000, mean reward: 18.333 [ 0.000, 35.000], mean action: 3880.833 [238.000, 5608.000],  loss: 55.721386, mae: 16.362654, mean_q: 37.674664\n",
            " 11312/30000: episode: 1887, duration: 0.451s, episode steps:   6, steps per second:  13, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2234.500 [1057.000, 3985.000],  loss: 31.861078, mae: 15.676654, mean_q: 36.094280\n",
            " 11318/30000: episode: 1888, duration: 0.309s, episode steps:   6, steps per second:  19, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2999.833 [590.000, 5448.000],  loss: 42.663757, mae: 15.531314, mean_q: 35.668484\n",
            " 11324/30000: episode: 1889, duration: 0.263s, episode steps:   6, steps per second:  23, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 1548.667 [1227.000, 1613.000],  loss: 31.852518, mae: 15.346097, mean_q: 35.829411\n",
            " 11330/30000: episode: 1890, duration: 0.316s, episode steps:   6, steps per second:  19, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2659.667 [471.000, 4548.000],  loss: 31.733910, mae: 14.747719, mean_q: 34.773651\n",
            " 11336/30000: episode: 1891, duration: 0.309s, episode steps:   6, steps per second:  19, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 1227.000 [1227.000, 1227.000],  loss: 49.797199, mae: 15.159602, mean_q: 35.523464\n",
            " 11342/30000: episode: 1892, duration: 0.326s, episode steps:   6, steps per second:  18, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2501.500 [1227.000, 5240.000],  loss: 42.395687, mae: 16.197714, mean_q: 37.160599\n",
            " 11348/30000: episode: 1893, duration: 0.362s, episode steps:   6, steps per second:  17, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2439.667 [534.000, 5099.000],  loss: 39.184818, mae: 15.978626, mean_q: 36.676170\n",
            " 11354/30000: episode: 1894, duration: 0.337s, episode steps:   6, steps per second:  18, episode reward: 35.000, mean reward:  5.833 [ 0.000, 35.000], mean action: 2438.333 [702.000, 4632.000],  loss: 55.306416, mae: 15.273212, mean_q: 36.334782\n",
            " 11360/30000: episode: 1895, duration: 0.365s, episode steps:   6, steps per second:  16, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1946.833 [1227.000, 4002.000],  loss: 31.585989, mae: 15.484932, mean_q: 36.531780\n",
            " 11366/30000: episode: 1896, duration: 0.316s, episode steps:   6, steps per second:  19, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1660.167 [1227.000, 3826.000],  loss: 43.260387, mae: 16.566891, mean_q: 38.960751\n",
            " 11372/30000: episode: 1897, duration: 0.301s, episode steps:   6, steps per second:  20, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3512.167 [429.000, 5663.000],  loss: 53.983059, mae: 14.974221, mean_q: 35.227917\n",
            " 11378/30000: episode: 1898, duration: 0.345s, episode steps:   6, steps per second:  17, episode reward: 145.000, mean reward: 24.167 [ 0.000, 35.000], mean action: 3127.500 [429.000, 4185.000],  loss: 49.527004, mae: 16.323463, mean_q: 38.069946\n",
            " 11384/30000: episode: 1899, duration: 0.383s, episode steps:   6, steps per second:  16, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3614.000 [1227.000, 4473.000],  loss: 61.179600, mae: 16.056557, mean_q: 36.920284\n",
            " 11390/30000: episode: 1900, duration: 0.305s, episode steps:   6, steps per second:  20, episode reward: -10.000, mean reward: -1.667 [-5.000,  5.000], mean action: 1746.167 [1130.000, 4439.000],  loss: 52.335491, mae: 15.846970, mean_q: 36.908680\n",
            " 11396/30000: episode: 1901, duration: 0.224s, episode steps:   6, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3288.833 [592.000, 5652.000],  loss: 54.612728, mae: 16.038374, mean_q: 37.281979\n",
            " 11402/30000: episode: 1902, duration: 0.190s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2680.500 [347.000, 4562.000],  loss: 35.396893, mae: 15.891826, mean_q: 37.757435\n",
            " 11408/30000: episode: 1903, duration: 0.352s, episode steps:   6, steps per second:  17, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2795.667 [429.000, 4632.000],  loss: 25.256948, mae: 16.035788, mean_q: 37.090252\n",
            " 11414/30000: episode: 1904, duration: 0.314s, episode steps:   6, steps per second:  19, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3418.333 [452.000, 4806.000],  loss: 40.043789, mae: 15.718581, mean_q: 36.217953\n",
            " 11420/30000: episode: 1905, duration: 0.343s, episode steps:   6, steps per second:  18, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 1269.333 [238.000, 2205.000],  loss: 40.232456, mae: 14.899619, mean_q: 34.255302\n",
            " 11426/30000: episode: 1906, duration: 0.325s, episode steps:   6, steps per second:  18, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1905.000 [429.000, 2964.000],  loss: 52.686008, mae: 14.299179, mean_q: 33.791679\n",
            " 11432/30000: episode: 1907, duration: 0.297s, episode steps:   6, steps per second:  20, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2906.333 [663.000, 5703.000],  loss: 35.054150, mae: 14.132182, mean_q: 32.690517\n",
            " 11438/30000: episode: 1908, duration: 0.345s, episode steps:   6, steps per second:  17, episode reward: 30.000, mean reward:  5.000 [ 0.000, 30.000], mean action: 2419.500 [607.000, 4185.000],  loss: 31.591440, mae: 13.725945, mean_q: 32.609756\n",
            " 11444/30000: episode: 1909, duration: 0.361s, episode steps:   6, steps per second:  17, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2445.000 [275.000, 4624.000],  loss: 38.707642, mae: 14.332095, mean_q: 33.078259\n",
            " 11450/30000: episode: 1910, duration: 0.297s, episode steps:   6, steps per second:  20, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3639.500 [429.000, 5742.000],  loss: 45.984180, mae: 15.927940, mean_q: 35.885406\n",
            " 11456/30000: episode: 1911, duration: 0.280s, episode steps:   6, steps per second:  21, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2775.333 [429.000, 5486.000],  loss: 42.004047, mae: 15.978825, mean_q: 36.292759\n",
            " 11462/30000: episode: 1912, duration: 0.237s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3012.667 [1942.000, 5161.000],  loss: 42.399029, mae: 16.284021, mean_q: 37.172131\n",
            " 11468/30000: episode: 1913, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3245.667 [429.000, 5257.000],  loss: 34.339771, mae: 15.582161, mean_q: 36.107456\n",
            " 11474/30000: episode: 1914, duration: 0.187s, episode steps:   6, steps per second:  32, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1963.833 [1057.000, 3051.000],  loss: 48.921291, mae: 14.656898, mean_q: 34.332890\n",
            " 11480/30000: episode: 1915, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3528.500 [1247.000, 5216.000],  loss: 33.955673, mae: 15.661099, mean_q: 36.171513\n",
            " 11486/30000: episode: 1916, duration: 0.179s, episode steps:   6, steps per second:  33, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3926.333 [1253.000, 5249.000],  loss: 41.210171, mae: 15.800583, mean_q: 36.888985\n",
            " 11492/30000: episode: 1917, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3000.833 [1983.000, 5688.000],  loss: 51.299335, mae: 16.061083, mean_q: 36.820450\n",
            " 11498/30000: episode: 1918, duration: 0.193s, episode steps:   6, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3682.667 [1460.000, 5249.000],  loss: 38.606716, mae: 16.778555, mean_q: 38.211582\n",
            " 11504/30000: episode: 1919, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 110.000, mean reward: 18.333 [ 0.000, 35.000], mean action: 3880.833 [238.000, 5608.000],  loss: 31.955740, mae: 15.887868, mean_q: 36.190018\n",
            " 11510/30000: episode: 1920, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2584.000 [229.000, 5564.000],  loss: 41.655830, mae: 16.159641, mean_q: 36.892727\n",
            " 11516/30000: episode: 1921, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3390.667 [238.000, 5002.000],  loss: 52.157558, mae: 16.814440, mean_q: 38.280193\n",
            " 11522/30000: episode: 1922, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 2891.333 [30.000, 5675.000],  loss: 30.981438, mae: 15.674121, mean_q: 36.626862\n",
            " 11528/30000: episode: 1923, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: 35.000, mean reward:  5.833 [-5.000, 40.000], mean action: 3851.500 [2601.000, 5249.000],  loss: 25.201378, mae: 14.677226, mean_q: 34.579350\n",
            " 11534/30000: episode: 1924, duration: 0.269s, episode steps:   6, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3055.667 [229.000, 4884.000],  loss: 40.938679, mae: 17.139917, mean_q: 38.624252\n",
            " 11540/30000: episode: 1925, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2304.000 [183.000, 5532.000],  loss: 41.619663, mae: 15.463348, mean_q: 35.629658\n",
            " 11546/30000: episode: 1926, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3914.500 [1596.000, 5722.000],  loss: 33.394505, mae: 14.747165, mean_q: 34.583679\n",
            " 11552/30000: episode: 1927, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3609.667 [238.000, 4884.000],  loss: 43.675457, mae: 15.172202, mean_q: 35.574730\n",
            " 11558/30000: episode: 1928, duration: 0.269s, episode steps:   6, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2445.167 [441.000, 4884.000],  loss: 48.694218, mae: 15.555507, mean_q: 35.998936\n",
            " 11564/30000: episode: 1929, duration: 0.259s, episode steps:   6, steps per second:  23, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3259.333 [26.000, 4884.000],  loss: 39.647705, mae: 15.181094, mean_q: 34.775532\n",
            " 11570/30000: episode: 1930, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 3534.500 [1059.000, 4884.000],  loss: 28.477911, mae: 14.877365, mean_q: 35.074333\n",
            " 11576/30000: episode: 1931, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3233.500 [229.000, 5538.000],  loss: 38.597496, mae: 15.166520, mean_q: 35.207554\n",
            " 11582/30000: episode: 1932, duration: 0.263s, episode steps:   6, steps per second:  23, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2658.833 [275.000, 5093.000],  loss: 36.139469, mae: 15.330281, mean_q: 35.788410\n",
            " 11588/30000: episode: 1933, duration: 0.263s, episode steps:   6, steps per second:  23, episode reward: 115.000, mean reward: 19.167 [ 0.000, 100.000], mean action: 1954.000 [229.000, 4246.000],  loss: 36.291157, mae: 14.026932, mean_q: 33.291126\n",
            " 11594/30000: episode: 1934, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2858.333 [429.000, 3946.000],  loss: 39.115444, mae: 14.926010, mean_q: 34.593349\n",
            " 11600/30000: episode: 1935, duration: 0.224s, episode steps:   6, steps per second:  27, episode reward: -5.000, mean reward: -0.833 [-5.000,  5.000], mean action: 2928.333 [1438.000, 5736.000],  loss: 21.699348, mae: 15.416266, mean_q: 36.286739\n",
            " 11606/30000: episode: 1936, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3640.167 [1828.000, 4884.000],  loss: 30.383474, mae: 14.815314, mean_q: 34.809017\n",
            " 11612/30000: episode: 1937, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2414.000 [375.000, 3814.000],  loss: 40.416107, mae: 15.093678, mean_q: 35.267868\n",
            " 11618/30000: episode: 1938, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3916.667 [1622.000, 5395.000],  loss: 25.037909, mae: 14.228393, mean_q: 33.703625\n",
            " 11624/30000: episode: 1939, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3241.833 [1670.000, 4148.000],  loss: 64.097412, mae: 14.894317, mean_q: 35.317181\n",
            " 11630/30000: episode: 1940, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2906.167 [808.000, 5658.000],  loss: 49.082745, mae: 16.093674, mean_q: 36.880993\n",
            " 11636/30000: episode: 1941, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3003.500 [542.000, 4763.000],  loss: 51.933125, mae: 16.775537, mean_q: 38.131756\n",
            " 11642/30000: episode: 1942, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3915.000 [1439.000, 5651.000],  loss: 31.627113, mae: 15.728119, mean_q: 36.407337\n",
            " 11648/30000: episode: 1943, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2843.000 [150.000, 4134.000],  loss: 32.595867, mae: 14.105475, mean_q: 33.274426\n",
            " 11654/30000: episode: 1944, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 4428.500 [3069.000, 5715.000],  loss: 37.369919, mae: 16.092829, mean_q: 36.864204\n",
            " 11660/30000: episode: 1945, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3221.000 [562.000, 5539.000],  loss: 38.492802, mae: 15.742841, mean_q: 35.688904\n",
            " 11666/30000: episode: 1946, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2917.500 [106.000, 5151.000],  loss: 35.707291, mae: 15.861491, mean_q: 36.701267\n",
            " 11672/30000: episode: 1947, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4724.000 [2904.000, 5564.000],  loss: 40.242012, mae: 15.352506, mean_q: 35.191837\n",
            " 11678/30000: episode: 1948, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 4143.000 [2544.000, 5689.000],  loss: 50.116699, mae: 15.589748, mean_q: 36.838837\n",
            " 11684/30000: episode: 1949, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1865.000 [72.000, 3207.000],  loss: 33.261509, mae: 15.113458, mean_q: 34.927685\n",
            " 11690/30000: episode: 1950, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 4618.833 [4347.000, 4734.000],  loss: 32.345272, mae: 15.276198, mean_q: 35.572113\n",
            " 11696/30000: episode: 1951, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 40.000, mean reward:  6.667 [ 0.000, 35.000], mean action: 3138.667 [437.000, 5656.000],  loss: 34.242691, mae: 14.866799, mean_q: 34.534657\n",
            " 11702/30000: episode: 1952, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2847.333 [1609.000, 5598.000],  loss: 37.654236, mae: 13.849088, mean_q: 33.157684\n",
            " 11708/30000: episode: 1953, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3586.000 [2277.000, 5362.000],  loss: 25.634638, mae: 15.535220, mean_q: 35.898472\n",
            " 11714/30000: episode: 1954, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3233.167 [238.000, 4134.000],  loss: 53.915325, mae: 16.564367, mean_q: 37.898586\n",
            " 11720/30000: episode: 1955, duration: 0.185s, episode steps:   6, steps per second:  32, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 3712.000 [3712.000, 3712.000],  loss: 41.944992, mae: 15.901984, mean_q: 37.220852\n",
            " 11726/30000: episode: 1956, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3324.833 [1118.000, 5011.000],  loss: 44.629040, mae: 17.745638, mean_q: 40.982864\n",
            " 11732/30000: episode: 1957, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3278.500 [1003.000, 5474.000],  loss: 34.168606, mae: 15.680881, mean_q: 36.641018\n",
            " 11738/30000: episode: 1958, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3209.667 [238.000, 4134.000],  loss: 42.202187, mae: 15.166840, mean_q: 35.034153\n",
            " 11744/30000: episode: 1959, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3959.167 [1908.000, 5626.000],  loss: 23.688154, mae: 16.264074, mean_q: 37.676792\n",
            " 11750/30000: episode: 1960, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 4440.833 [2435.000, 5656.000],  loss: 41.584324, mae: 15.302402, mean_q: 35.822613\n",
            " 11756/30000: episode: 1961, duration: 0.188s, episode steps:   6, steps per second:  32, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1958.167 [350.000, 5400.000],  loss: 55.336227, mae: 16.373487, mean_q: 37.703777\n",
            " 11762/30000: episode: 1962, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3888.333 [2953.000, 4819.000],  loss: 45.533955, mae: 16.128893, mean_q: 37.115330\n",
            " 11768/30000: episode: 1963, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3235.833 [280.000, 4735.000],  loss: 39.502445, mae: 14.616120, mean_q: 33.827763\n",
            " 11774/30000: episode: 1964, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3892.833 [1530.000, 5554.000],  loss: 40.671780, mae: 14.785222, mean_q: 34.502102\n",
            " 11780/30000: episode: 1965, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3699.833 [2437.000, 4832.000],  loss: 51.846836, mae: 15.060651, mean_q: 34.904568\n",
            " 11786/30000: episode: 1966, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3696.167 [1564.000, 5297.000],  loss: 39.118156, mae: 14.521187, mean_q: 34.243519\n",
            " 11792/30000: episode: 1967, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: 20.000, mean reward:  3.333 [ 0.000, 10.000], mean action: 2287.000 [106.000, 4799.000],  loss: 31.325701, mae: 15.112766, mean_q: 34.774456\n",
            " 11798/30000: episode: 1968, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 10.000], mean action: 1648.167 [183.000, 4002.000],  loss: 31.124969, mae: 14.367542, mean_q: 33.651855\n",
            " 11804/30000: episode: 1969, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3560.833 [1951.000, 5628.000],  loss: 27.360003, mae: 14.594372, mean_q: 34.632832\n",
            " 11810/30000: episode: 1970, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3579.167 [1190.000, 5532.000],  loss: 61.351700, mae: 16.449942, mean_q: 37.884251\n",
            " 11816/30000: episode: 1971, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4235.000 [4002.000, 5400.000],  loss: 40.393681, mae: 15.239469, mean_q: 35.356647\n",
            " 11822/30000: episode: 1972, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 2291.500 [946.000, 5316.000],  loss: 45.616917, mae: 15.306989, mean_q: 36.326561\n",
            " 11828/30000: episode: 1973, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2291.000 [12.000, 4982.000],  loss: 32.284912, mae: 14.758906, mean_q: 34.928516\n",
            " 11834/30000: episode: 1974, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 1505.000 [608.000, 2277.000],  loss: 33.525227, mae: 15.265140, mean_q: 36.261379\n",
            " 11840/30000: episode: 1975, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4081.333 [2694.000, 5651.000],  loss: 56.113338, mae: 15.275304, mean_q: 35.989521\n",
            " 11846/30000: episode: 1976, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3612.500 [2149.000, 5721.000],  loss: 37.403175, mae: 16.525675, mean_q: 38.812992\n",
            " 11852/30000: episode: 1977, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 3945.167 [2444.000, 5324.000],  loss: 29.313635, mae: 16.034119, mean_q: 37.293438\n",
            " 11858/30000: episode: 1978, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3063.000 [72.000, 5651.000],  loss: 42.073048, mae: 15.304363, mean_q: 35.983868\n",
            " 11864/30000: episode: 1979, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3623.500 [1501.000, 5400.000],  loss: 51.435516, mae: 16.000410, mean_q: 36.763958\n",
            " 11870/30000: episode: 1980, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2795.667 [106.000, 4552.000],  loss: 35.900990, mae: 15.157258, mean_q: 35.692005\n",
            " 11876/30000: episode: 1981, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2230.167 [437.000, 4819.000],  loss: 38.065033, mae: 14.483044, mean_q: 34.661526\n",
            " 11882/30000: episode: 1982, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1817.167 [107.000, 4109.000],  loss: 42.543461, mae: 14.505413, mean_q: 34.748066\n",
            " 11888/30000: episode: 1983, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3867.500 [1243.000, 5488.000],  loss: 33.083942, mae: 16.593784, mean_q: 38.265873\n",
            " 11894/30000: episode: 1984, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3111.667 [140.000, 5348.000],  loss: 37.266476, mae: 16.544920, mean_q: 38.065250\n",
            " 11900/30000: episode: 1985, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1558.833 [172.000, 4159.000],  loss: 41.380184, mae: 16.014610, mean_q: 37.125431\n",
            " 11906/30000: episode: 1986, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 4485.667 [3469.000, 5708.000],  loss: 44.869446, mae: 15.403720, mean_q: 35.823284\n",
            " 11912/30000: episode: 1987, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3271.667 [229.000, 5193.000],  loss: 53.946789, mae: 16.036205, mean_q: 37.269619\n",
            " 11918/30000: episode: 1988, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3179.333 [685.000, 4184.000],  loss: 30.114899, mae: 14.543674, mean_q: 34.739517\n",
            " 11924/30000: episode: 1989, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4182.500 [3155.000, 5617.000],  loss: 33.722851, mae: 15.404889, mean_q: 36.778103\n",
            " 11930/30000: episode: 1990, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3734.667 [1783.000, 5367.000],  loss: 41.185390, mae: 14.747607, mean_q: 35.627918\n",
            " 11936/30000: episode: 1991, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 1902.833 [452.000, 3570.000],  loss: 44.701794, mae: 15.559514, mean_q: 36.695377\n",
            " 11942/30000: episode: 1992, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2734.833 [1609.000, 4454.000],  loss: 48.431854, mae: 16.394766, mean_q: 38.178242\n",
            " 11948/30000: episode: 1993, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 3016.167 [1509.000, 4702.000],  loss: 50.432865, mae: 14.527484, mean_q: 34.182835\n",
            " 11954/30000: episode: 1994, duration: 0.194s, episode steps:   6, steps per second:  31, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 4036.667 [1509.000, 5598.000],  loss: 43.299545, mae: 15.063413, mean_q: 35.760654\n",
            " 11960/30000: episode: 1995, duration: 0.232s, episode steps:   6, steps per second:  26, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4313.667 [2782.000, 5652.000],  loss: 47.798843, mae: 15.475507, mean_q: 36.400482\n",
            " 11966/30000: episode: 1996, duration: 0.245s, episode steps:   6, steps per second:  25, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3129.167 [580.000, 4637.000],  loss: 33.986023, mae: 15.559383, mean_q: 36.459785\n",
            " 11972/30000: episode: 1997, duration: 0.269s, episode steps:   6, steps per second:  22, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2701.833 [1509.000, 4824.000],  loss: 30.271780, mae: 14.377927, mean_q: 33.710842\n",
            " 11978/30000: episode: 1998, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2301.500 [1243.000, 5129.000],  loss: 24.456543, mae: 15.066737, mean_q: 35.634441\n",
            " 11984/30000: episode: 1999, duration: 0.264s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4004.667 [1665.000, 5689.000],  loss: 45.953182, mae: 15.577660, mean_q: 36.354733\n",
            " 11990/30000: episode: 2000, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2492.667 [262.000, 5373.000],  loss: 42.097256, mae: 14.976251, mean_q: 34.989750\n",
            " 11996/30000: episode: 2001, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3373.667 [1509.000, 5708.000],  loss: 37.146503, mae: 15.296618, mean_q: 35.878330\n",
            " 12002/30000: episode: 2002, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2961.167 [229.000, 4765.000],  loss: 39.901501, mae: 16.065519, mean_q: 36.669083\n",
            " 12008/30000: episode: 2003, duration: 0.257s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4318.167 [3669.000, 5162.000],  loss: 37.523510, mae: 14.385514, mean_q: 34.195110\n",
            " 12014/30000: episode: 2004, duration: 0.261s, episode steps:   6, steps per second:  23, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2661.167 [103.000, 5708.000],  loss: 44.637432, mae: 15.365218, mean_q: 36.295635\n",
            " 12020/30000: episode: 2005, duration: 0.264s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2661.500 [106.000, 5367.000],  loss: 35.243351, mae: 14.432087, mean_q: 34.724758\n",
            " 12026/30000: episode: 2006, duration: 0.179s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2612.000 [1381.000, 5703.000],  loss: 38.748657, mae: 15.057129, mean_q: 35.589607\n",
            " 12032/30000: episode: 2007, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3776.167 [2738.000, 5708.000],  loss: 41.293629, mae: 16.585258, mean_q: 37.607716\n",
            " 12038/30000: episode: 2008, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [-5.000, 15.000], mean action: 1514.833 [816.000, 4159.000],  loss: 34.624775, mae: 15.560006, mean_q: 37.163921\n",
            " 12044/30000: episode: 2009, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2138.333 [452.000, 4819.000],  loss: 33.332684, mae: 15.508914, mean_q: 36.752304\n",
            " 12050/30000: episode: 2010, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3003.000 [1231.000, 4159.000],  loss: 31.900595, mae: 17.170153, mean_q: 38.859653\n",
            " 12056/30000: episode: 2011, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3172.833 [106.000, 5077.000],  loss: 26.254713, mae: 15.836060, mean_q: 36.861591\n",
            " 12062/30000: episode: 2012, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2971.167 [1191.000, 4963.000],  loss: 31.647612, mae: 14.897144, mean_q: 34.847515\n",
            " 12068/30000: episode: 2013, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2201.000 [452.000, 4677.000],  loss: 31.513243, mae: 14.913585, mean_q: 35.215939\n",
            " 12074/30000: episode: 2014, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3559.167 [159.000, 5708.000],  loss: 27.027922, mae: 15.073544, mean_q: 35.856903\n",
            " 12080/30000: episode: 2015, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3408.833 [575.000, 5488.000],  loss: 32.151848, mae: 15.367790, mean_q: 35.988331\n",
            " 12086/30000: episode: 2016, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3341.833 [452.000, 5708.000],  loss: 47.352482, mae: 15.997853, mean_q: 37.673004\n",
            " 12092/30000: episode: 2017, duration: 0.186s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1744.833 [452.000, 4159.000],  loss: 46.501190, mae: 14.845520, mean_q: 34.745861\n",
            " 12098/30000: episode: 2018, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  5.000], mean action: 3615.500 [1560.000, 5400.000],  loss: 64.538307, mae: 15.241696, mean_q: 36.193066\n",
            " 12104/30000: episode: 2019, duration: 0.189s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3630.167 [452.000, 5400.000],  loss: 54.870090, mae: 16.337896, mean_q: 37.731941\n",
            " 12110/30000: episode: 2020, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [-5.000, 20.000], mean action: 1167.833 [946.000, 2277.000],  loss: 50.055859, mae: 17.627165, mean_q: 39.812866\n",
            " 12116/30000: episode: 2021, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3620.167 [1242.000, 5474.000],  loss: 45.071949, mae: 15.729020, mean_q: 36.430431\n",
            " 12122/30000: episode: 2022, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1994.833 [10.000, 4725.000],  loss: 53.465515, mae: 16.771526, mean_q: 38.660454\n",
            " 12128/30000: episode: 2023, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3024.000 [452.000, 5675.000],  loss: 50.442890, mae: 15.263461, mean_q: 35.430843\n",
            " 12134/30000: episode: 2024, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 65.000, mean reward: 10.833 [ 0.000, 20.000], mean action: 2983.167 [1202.000, 5400.000],  loss: 39.723965, mae: 15.259494, mean_q: 36.115383\n",
            " 12140/30000: episode: 2025, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1783.000 [230.000, 4824.000],  loss: 41.285107, mae: 16.142229, mean_q: 36.813877\n",
            " 12146/30000: episode: 2026, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3384.000 [1868.000, 5400.000],  loss: 44.758595, mae: 16.960838, mean_q: 38.975636\n",
            " 12152/30000: episode: 2027, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 35.000, mean reward:  5.833 [ 0.000, 30.000], mean action: 2972.000 [253.000, 5708.000],  loss: 51.785446, mae: 17.518995, mean_q: 39.839977\n",
            " 12158/30000: episode: 2028, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3342.333 [238.000, 5708.000],  loss: 24.155586, mae: 14.398056, mean_q: 34.963764\n",
            " 12164/30000: episode: 2029, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3068.333 [230.000, 5743.000],  loss: 30.743034, mae: 14.887688, mean_q: 35.427250\n",
            " 12170/30000: episode: 2030, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3273.500 [1110.000, 5742.000],  loss: 23.452105, mae: 14.468781, mean_q: 34.610607\n",
            " 12176/30000: episode: 2031, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2986.500 [72.000, 5708.000],  loss: 38.881435, mae: 16.813456, mean_q: 38.929504\n",
            " 12182/30000: episode: 2032, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 45.000, mean reward:  7.500 [ 0.000, 30.000], mean action: 3373.333 [1953.000, 5708.000],  loss: 47.389801, mae: 15.446774, mean_q: 36.825012\n",
            " 12188/30000: episode: 2033, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 4355.333 [1347.000, 5708.000],  loss: 48.761047, mae: 15.066655, mean_q: 35.242565\n",
            " 12194/30000: episode: 2034, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2075.333 [238.000, 5218.000],  loss: 41.942669, mae: 15.866441, mean_q: 36.493546\n",
            " 12200/30000: episode: 2035, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3793.667 [946.000, 5608.000],  loss: 51.759018, mae: 15.453522, mean_q: 35.412159\n",
            " 12206/30000: episode: 2036, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1230.000 [229.000, 2659.000],  loss: 38.343510, mae: 14.715657, mean_q: 34.932949\n",
            " 12212/30000: episode: 2037, duration: 0.187s, episode steps:   6, steps per second:  32, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 3663.667 [1104.000, 5163.000],  loss: 52.403965, mae: 16.438272, mean_q: 37.598080\n",
            " 12218/30000: episode: 2038, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2098.167 [363.000, 4749.000],  loss: 33.583996, mae: 14.910232, mean_q: 34.984859\n",
            " 12224/30000: episode: 2039, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 4398.667 [2966.000, 5588.000],  loss: 49.196583, mae: 15.564056, mean_q: 36.235981\n",
            " 12230/30000: episode: 2040, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3234.000 [1596.000, 5017.000],  loss: 70.328941, mae: 15.053387, mean_q: 35.851181\n",
            " 12236/30000: episode: 2041, duration: 0.185s, episode steps:   6, steps per second:  32, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3260.833 [230.000, 4551.000],  loss: 47.486187, mae: 15.540080, mean_q: 35.956524\n",
            " 12242/30000: episode: 2042, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3553.167 [1191.000, 5707.000],  loss: 65.170563, mae: 15.201655, mean_q: 35.394119\n",
            " 12248/30000: episode: 2043, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [-5.000, 20.000], mean action: 3051.833 [1178.000, 5129.000],  loss: 31.395615, mae: 15.446721, mean_q: 35.285206\n",
            " 12254/30000: episode: 2044, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3006.500 [253.000, 3804.000],  loss: 38.349789, mae: 15.641347, mean_q: 35.960659\n",
            " 12260/30000: episode: 2045, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 4143.333 [2544.000, 5598.000],  loss: 38.818195, mae: 14.641170, mean_q: 34.353874\n",
            " 12266/30000: episode: 2046, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3117.500 [253.000, 5253.000],  loss: 34.229198, mae: 16.184076, mean_q: 37.526924\n",
            " 12272/30000: episode: 2047, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 1353.167 [26.000, 3752.000],  loss: 61.195736, mae: 14.814229, mean_q: 34.726482\n",
            " 12278/30000: episode: 2048, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3507.167 [253.000, 5400.000],  loss: 41.509773, mae: 16.229174, mean_q: 37.726040\n",
            " 12284/30000: episode: 2049, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 1391.333 [183.000, 5400.000],  loss: 46.893951, mae: 14.570953, mean_q: 33.806530\n",
            " 12290/30000: episode: 2050, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1385.333 [98.000, 3937.000],  loss: 37.315861, mae: 15.351936, mean_q: 35.452499\n",
            " 12296/30000: episode: 2051, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2041.667 [253.000, 4346.000],  loss: 43.104935, mae: 15.616982, mean_q: 35.985199\n",
            " 12302/30000: episode: 2052, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 35.000, mean reward:  5.833 [ 0.000, 35.000], mean action: 3140.333 [554.000, 5148.000],  loss: 30.684622, mae: 15.325221, mean_q: 35.547619\n",
            " 12308/30000: episode: 2053, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: 15.000, mean reward:  2.500 [-5.000, 10.000], mean action: 2017.667 [285.000, 4229.000],  loss: 25.806620, mae: 14.904613, mean_q: 35.083866\n",
            " 12314/30000: episode: 2054, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 4569.833 [2898.000, 5658.000],  loss: 35.722790, mae: 15.095979, mean_q: 35.523697\n",
            " 12320/30000: episode: 2055, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 3826.833 [2277.000, 5708.000],  loss: 20.883511, mae: 15.442021, mean_q: 35.150227\n",
            " 12326/30000: episode: 2056, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4216.500 [3167.000, 5148.000],  loss: 43.956203, mae: 14.638519, mean_q: 33.525936\n",
            " 12332/30000: episode: 2057, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2142.167 [353.000, 4337.000],  loss: 49.159241, mae: 16.196455, mean_q: 37.248096\n",
            " 12338/30000: episode: 2058, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1994.667 [350.000, 5104.000],  loss: 32.622929, mae: 14.376590, mean_q: 34.956093\n",
            " 12344/30000: episode: 2059, duration: 0.183s, episode steps:   6, steps per second:  33, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1144.500 [350.000, 1530.000],  loss: 47.678188, mae: 14.859444, mean_q: 35.491261\n",
            " 12350/30000: episode: 2060, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2193.667 [350.000, 3221.000],  loss: 54.726368, mae: 15.589600, mean_q: 36.658291\n",
            " 12356/30000: episode: 2061, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2704.000 [350.000, 4229.000],  loss: 44.356113, mae: 15.774901, mean_q: 37.109543\n",
            " 12362/30000: episode: 2062, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 1439.667 [159.000, 3685.000],  loss: 42.454502, mae: 15.252370, mean_q: 35.945034\n",
            " 12368/30000: episode: 2063, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2551.167 [316.000, 5000.000],  loss: 29.965300, mae: 15.805733, mean_q: 36.949745\n",
            " 12374/30000: episode: 2064, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 20.000, mean reward:  3.333 [ 0.000, 10.000], mean action: 2473.667 [350.000, 3473.000],  loss: 32.983784, mae: 14.754674, mean_q: 35.844540\n",
            " 12380/30000: episode: 2065, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2544.833 [350.000, 5302.000],  loss: 50.744370, mae: 16.218893, mean_q: 38.179249\n",
            " 12386/30000: episode: 2066, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3396.833 [350.000, 5656.000],  loss: 51.651913, mae: 16.075014, mean_q: 38.291805\n",
            " 12392/30000: episode: 2067, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2441.500 [350.000, 4590.000],  loss: 38.817097, mae: 15.725068, mean_q: 37.760010\n",
            " 12398/30000: episode: 2068, duration: 0.245s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1134.167 [316.000, 2640.000],  loss: 37.702099, mae: 14.225299, mean_q: 34.836170\n",
            " 12404/30000: episode: 2069, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1562.333 [350.000, 4838.000],  loss: 38.633686, mae: 15.570544, mean_q: 37.448139\n",
            " 12410/30000: episode: 2070, duration: 0.266s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1988.000 [26.000, 5506.000],  loss: 32.022816, mae: 14.825439, mean_q: 35.422955\n",
            " 12416/30000: episode: 2071, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1903.500 [350.000, 3315.000],  loss: 41.928928, mae: 14.028094, mean_q: 34.256985\n",
            " 12422/30000: episode: 2072, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1366.667 [1178.000, 1958.000],  loss: 40.446320, mae: 14.416332, mean_q: 35.104671\n",
            " 12428/30000: episode: 2073, duration: 0.275s, episode steps:   6, steps per second:  22, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 350.000 [350.000, 350.000],  loss: 32.182030, mae: 14.308274, mean_q: 34.979607\n",
            " 12434/30000: episode: 2074, duration: 0.268s, episode steps:   6, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2846.167 [1627.000, 4838.000],  loss: 80.241135, mae: 15.859475, mean_q: 37.599163\n",
            " 12440/30000: episode: 2075, duration: 0.263s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1602.167 [26.000, 5503.000],  loss: 56.852421, mae: 17.527834, mean_q: 40.968369\n",
            " 12446/30000: episode: 2076, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2946.833 [1127.000, 5656.000],  loss: 44.572052, mae: 14.888149, mean_q: 35.668983\n",
            " 12452/30000: episode: 2077, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [-5.000, 30.000], mean action: 2508.167 [1348.000, 3525.000],  loss: 25.855614, mae: 15.049146, mean_q: 35.420277\n",
            " 12458/30000: episode: 2078, duration: 0.185s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [-5.000, 20.000], mean action: 4554.000 [4244.000, 4616.000],  loss: 38.093678, mae: 15.483485, mean_q: 36.303955\n",
            " 12464/30000: episode: 2079, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2760.667 [542.000, 5163.000],  loss: 56.644489, mae: 14.803975, mean_q: 34.521763\n",
            " 12470/30000: episode: 2080, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 2408.167 [489.000, 4786.000],  loss: 40.086979, mae: 15.409225, mean_q: 36.356144\n",
            " 12476/30000: episode: 2081, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2961.500 [1937.000, 3776.000],  loss: 25.581690, mae: 14.555161, mean_q: 34.696484\n",
            " 12482/30000: episode: 2082, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2275.667 [489.000, 5017.000],  loss: 38.080387, mae: 15.459235, mean_q: 35.999374\n",
            " 12488/30000: episode: 2083, duration: 0.190s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2965.833 [733.000, 4930.000],  loss: 50.544910, mae: 15.598239, mean_q: 36.156738\n",
            " 12494/30000: episode: 2084, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1533.667 [218.000, 4264.000],  loss: 45.377018, mae: 15.343581, mean_q: 35.961178\n",
            " 12500/30000: episode: 2085, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: 45.000, mean reward:  7.500 [ 0.000, 20.000], mean action: 2072.167 [339.000, 4954.000],  loss: 41.462910, mae: 14.467201, mean_q: 33.786476\n",
            " 12506/30000: episode: 2086, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: 20.000, mean reward:  3.333 [ 0.000, 10.000], mean action: 2581.333 [1530.000, 4189.000],  loss: 31.956495, mae: 15.714082, mean_q: 36.698509\n",
            " 12512/30000: episode: 2087, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3636.500 [530.000, 5382.000],  loss: 35.009830, mae: 14.620918, mean_q: 34.319324\n",
            " 12518/30000: episode: 2088, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3001.000 [1530.000, 4152.000],  loss: 49.945557, mae: 16.644588, mean_q: 38.173038\n",
            " 12524/30000: episode: 2089, duration: 0.179s, episode steps:   6, steps per second:  33, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1598.167 [149.000, 3120.000],  loss: 29.105438, mae: 15.861900, mean_q: 36.177456\n",
            " 12530/30000: episode: 2090, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3213.167 [334.000, 5016.000],  loss: 31.187326, mae: 16.683317, mean_q: 38.519917\n",
            " 12536/30000: episode: 2091, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3662.833 [2027.000, 4701.000],  loss: 40.880489, mae: 15.659335, mean_q: 36.865002\n",
            " 12542/30000: episode: 2092, duration: 0.188s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2506.333 [238.000, 5486.000],  loss: 41.366856, mae: 16.080828, mean_q: 37.575535\n",
            " 12548/30000: episode: 2093, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2818.167 [1530.000, 5163.000],  loss: 43.411915, mae: 16.640955, mean_q: 37.856194\n",
            " 12554/30000: episode: 2094, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 3182.667 [1305.000, 5736.000],  loss: 50.986958, mae: 15.593808, mean_q: 36.093166\n",
            " 12560/30000: episode: 2095, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [-5.000, 10.000], mean action: 4061.167 [1792.000, 5148.000],  loss: 27.766253, mae: 14.714958, mean_q: 34.380878\n",
            " 12566/30000: episode: 2096, duration: 0.188s, episode steps:   6, steps per second:  32, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2954.500 [106.000, 5163.000],  loss: 47.202488, mae: 15.704815, mean_q: 35.394749\n",
            " 12572/30000: episode: 2097, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3529.167 [238.000, 5626.000],  loss: 28.827002, mae: 15.968811, mean_q: 36.880016\n",
            " 12578/30000: episode: 2098, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2831.167 [229.000, 4725.000],  loss: 37.529846, mae: 15.344117, mean_q: 36.483532\n",
            " 12584/30000: episode: 2099, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1463.167 [271.000, 2694.000],  loss: 42.778713, mae: 15.112742, mean_q: 35.634029\n",
            " 12590/30000: episode: 2100, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2864.500 [1439.000, 5675.000],  loss: 45.415787, mae: 14.890862, mean_q: 34.836624\n",
            " 12596/30000: episode: 2101, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2110.833 [281.000, 4982.000],  loss: 28.871210, mae: 14.529218, mean_q: 33.847393\n",
            " 12602/30000: episode: 2102, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2040.333 [27.000, 4507.000],  loss: 46.698338, mae: 15.459789, mean_q: 36.684757\n",
            " 12608/30000: episode: 2103, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 4727.333 [3804.000, 5742.000],  loss: 48.093639, mae: 16.253103, mean_q: 37.235809\n",
            " 12614/30000: episode: 2104, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2961.167 [562.000, 4708.000],  loss: 33.952637, mae: 14.996781, mean_q: 35.624401\n",
            " 12620/30000: episode: 2105, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2104.500 [44.000, 5283.000],  loss: 28.716187, mae: 15.632798, mean_q: 37.209476\n",
            " 12626/30000: episode: 2106, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4766.333 [3814.000, 5658.000],  loss: 34.781231, mae: 15.290187, mean_q: 36.239212\n",
            " 12632/30000: episode: 2107, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4357.167 [1596.000, 5656.000],  loss: 82.204109, mae: 15.974774, mean_q: 36.987381\n",
            " 12638/30000: episode: 2108, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3047.833 [889.000, 5163.000],  loss: 30.960577, mae: 16.021732, mean_q: 36.983524\n",
            " 12644/30000: episode: 2109, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 2899.500 [427.000, 4918.000],  loss: 45.787983, mae: 15.587428, mean_q: 37.702824\n",
            " 12650/30000: episode: 2110, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1789.833 [339.000, 4786.000],  loss: 50.293453, mae: 16.842718, mean_q: 38.583847\n",
            " 12656/30000: episode: 2111, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1379.167 [238.000, 3921.000],  loss: 41.455708, mae: 14.702850, mean_q: 33.768341\n",
            " 12662/30000: episode: 2112, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1997.667 [238.000, 3221.000],  loss: 25.503805, mae: 15.062595, mean_q: 35.672077\n",
            " 12668/30000: episode: 2113, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3209.333 [554.000, 4010.000],  loss: 56.434811, mae: 15.760238, mean_q: 36.973461\n",
            " 12674/30000: episode: 2114, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2512.167 [106.000, 4464.000],  loss: 32.884182, mae: 15.324822, mean_q: 35.907253\n",
            " 12680/30000: episode: 2115, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 2548.667 [1530.000, 4918.000],  loss: 36.314880, mae: 14.880477, mean_q: 34.781773\n",
            " 12686/30000: episode: 2116, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1631.833 [183.000, 2936.000],  loss: 45.717590, mae: 14.811391, mean_q: 35.359066\n",
            " 12692/30000: episode: 2117, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2395.333 [178.000, 5340.000],  loss: 35.344543, mae: 15.056296, mean_q: 35.332188\n",
            " 12698/30000: episode: 2118, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: 90.000, mean reward: 15.000 [ 0.000, 30.000], mean action: 2704.500 [1596.000, 5239.000],  loss: 37.496334, mae: 13.739234, mean_q: 33.145519\n",
            " 12704/30000: episode: 2119, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3037.500 [2640.000, 4412.000],  loss: 29.902010, mae: 15.662849, mean_q: 36.110535\n",
            " 12710/30000: episode: 2120, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3128.167 [1596.000, 5283.000],  loss: 47.373272, mae: 15.863678, mean_q: 36.688553\n",
            " 12716/30000: episode: 2121, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3323.667 [489.000, 5049.000],  loss: 46.464962, mae: 14.711661, mean_q: 34.579891\n",
            " 12722/30000: episode: 2122, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 1811.500 [145.000, 4630.000],  loss: 26.307695, mae: 16.316566, mean_q: 36.759716\n",
            " 12728/30000: episode: 2123, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [-5.000, 20.000], mean action: 2268.000 [852.000, 2722.000],  loss: 49.201855, mae: 14.104991, mean_q: 33.397961\n",
            " 12734/30000: episode: 2124, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2370.167 [343.000, 3214.000],  loss: 37.683304, mae: 15.946099, mean_q: 36.087975\n",
            " 12740/30000: episode: 2125, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  5.000], mean action: 2980.333 [343.000, 5447.000],  loss: 41.291523, mae: 15.460155, mean_q: 36.064045\n",
            " 12746/30000: episode: 2126, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1710.833 [343.000, 3211.000],  loss: 33.610729, mae: 15.214561, mean_q: 34.791653\n",
            " 12752/30000: episode: 2127, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: 55.000, mean reward:  9.167 [ 0.000, 25.000], mean action: 2595.500 [238.000, 5239.000],  loss: 61.170425, mae: 15.934380, mean_q: 36.938549\n",
            " 12758/30000: episode: 2128, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1681.833 [212.000, 3315.000],  loss: 31.030375, mae: 17.132624, mean_q: 39.352802\n",
            " 12764/30000: episode: 2129, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3419.500 [1979.000, 5667.000],  loss: 33.991631, mae: 14.927842, mean_q: 35.020245\n",
            " 12770/30000: episode: 2130, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 3655.000 [733.000, 5667.000],  loss: 46.246937, mae: 14.852600, mean_q: 35.285877\n",
            " 12776/30000: episode: 2131, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2774.500 [889.000, 4185.000],  loss: 45.129620, mae: 14.539643, mean_q: 34.766106\n",
            " 12782/30000: episode: 2132, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 1993.667 [459.000, 2601.000],  loss: 54.479427, mae: 13.860268, mean_q: 32.394993\n",
            " 12788/30000: episode: 2133, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3892.333 [537.000, 5667.000],  loss: 29.364166, mae: 16.725969, mean_q: 38.300976\n",
            " 12794/30000: episode: 2134, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2297.333 [281.000, 5651.000],  loss: 30.559204, mae: 15.451940, mean_q: 36.831043\n",
            " 12800/30000: episode: 2135, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3600.333 [238.000, 5608.000],  loss: 34.185261, mae: 16.279778, mean_q: 37.690182\n",
            " 12806/30000: episode: 2136, duration: 0.275s, episode steps:   6, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2216.333 [106.000, 3473.000],  loss: 28.024355, mae: 14.786156, mean_q: 35.318058\n",
            " 12812/30000: episode: 2137, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2644.333 [1218.000, 4783.000],  loss: 46.169556, mae: 15.111753, mean_q: 35.359234\n",
            " 12818/30000: episode: 2138, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3349.167 [1586.000, 5737.000],  loss: 34.021832, mae: 13.798667, mean_q: 33.584370\n",
            " 12824/30000: episode: 2139, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3234.000 [369.000, 5239.000],  loss: 42.997070, mae: 14.632294, mean_q: 35.194454\n",
            " 12830/30000: episode: 2140, duration: 0.264s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2980.000 [608.000, 5449.000],  loss: 31.143867, mae: 14.870830, mean_q: 35.081173\n",
            " 12836/30000: episode: 2141, duration: 0.265s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3424.000 [2244.000, 4616.000],  loss: 35.449451, mae: 15.275907, mean_q: 35.550678\n",
            " 12842/30000: episode: 2142, duration: 0.264s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3696.667 [946.000, 5364.000],  loss: 37.032055, mae: 16.018969, mean_q: 37.796818\n",
            " 12848/30000: episode: 2143, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4367.000 [1520.000, 5680.000],  loss: 33.944729, mae: 15.455956, mean_q: 35.822895\n",
            " 12854/30000: episode: 2144, duration: 0.271s, episode steps:   6, steps per second:  22, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2942.333 [907.000, 5474.000],  loss: 41.179623, mae: 15.712558, mean_q: 36.181843\n",
            " 12860/30000: episode: 2145, duration: 0.265s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3241.667 [914.000, 5109.000],  loss: 34.766697, mae: 15.022151, mean_q: 35.735638\n",
            " 12866/30000: episode: 2146, duration: 0.223s, episode steps:   6, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3747.000 [2184.000, 5750.000],  loss: 47.481907, mae: 14.609039, mean_q: 35.000492\n",
            " 12872/30000: episode: 2147, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3650.500 [375.000, 5667.000],  loss: 35.313946, mae: 15.739673, mean_q: 37.226665\n",
            " 12878/30000: episode: 2148, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2480.000 [106.000, 4552.000],  loss: 27.993538, mae: 15.494983, mean_q: 36.417534\n",
            " 12884/30000: episode: 2149, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3271.167 [140.000, 4918.000],  loss: 54.498642, mae: 15.988538, mean_q: 37.841579\n",
            " 12890/30000: episode: 2150, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward: 30.000, mean reward:  5.000 [ 0.000, 30.000], mean action: 4004.000 [172.000, 5656.000],  loss: 24.089178, mae: 14.113922, mean_q: 33.994236\n",
            " 12896/30000: episode: 2151, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3146.000 [1242.000, 4244.000],  loss: 70.805992, mae: 15.884276, mean_q: 37.210178\n",
            " 12902/30000: episode: 2152, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2482.167 [1686.000, 4244.000],  loss: 39.249786, mae: 15.768159, mean_q: 37.701168\n",
            " 12908/30000: episode: 2153, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 1654.500 [1530.000, 2277.000],  loss: 37.193707, mae: 15.358256, mean_q: 35.707321\n",
            " 12914/30000: episode: 2154, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1844.333 [687.000, 4717.000],  loss: 42.321228, mae: 15.711659, mean_q: 36.478672\n",
            " 12920/30000: episode: 2155, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 4231.833 [3308.000, 5667.000],  loss: 27.599113, mae: 14.051629, mean_q: 34.670673\n",
            " 12926/30000: episode: 2156, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2907.667 [1340.000, 4905.000],  loss: 41.003254, mae: 16.138073, mean_q: 38.355316\n",
            " 12932/30000: episode: 2157, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2763.167 [1197.000, 5367.000],  loss: 29.139334, mae: 14.853512, mean_q: 35.304012\n",
            " 12938/30000: episode: 2158, duration: 0.164s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3698.000 [1136.000, 5656.000],  loss: 48.459270, mae: 15.037170, mean_q: 35.929779\n",
            " 12944/30000: episode: 2159, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2976.500 [1132.000, 4244.000],  loss: 41.012573, mae: 15.860230, mean_q: 37.687115\n",
            " 12950/30000: episode: 2160, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 35.000, mean reward:  5.833 [ 0.000, 15.000], mean action: 2601.167 [1127.000, 5448.000],  loss: 37.215942, mae: 14.731128, mean_q: 34.849621\n",
            " 12956/30000: episode: 2161, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2670.667 [113.000, 5697.000],  loss: 50.667385, mae: 15.624942, mean_q: 37.312592\n",
            " 12962/30000: episode: 2162, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1971.000 [158.000, 4244.000],  loss: 43.136169, mae: 15.515224, mean_q: 36.674763\n",
            " 12968/30000: episode: 2163, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3473.333 [238.000, 5239.000],  loss: 45.047882, mae: 15.595811, mean_q: 36.103062\n",
            " 12974/30000: episode: 2164, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4454.333 [2008.000, 5697.000],  loss: 28.223562, mae: 13.467906, mean_q: 32.776791\n",
            " 12980/30000: episode: 2165, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3917.333 [2679.000, 4312.000],  loss: 36.374912, mae: 13.259048, mean_q: 32.428761\n",
            " 12986/30000: episode: 2166, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 5697.000 [5697.000, 5697.000],  loss: 47.932011, mae: 15.610882, mean_q: 37.503506\n",
            " 12992/30000: episode: 2167, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3338.500 [1038.000, 5697.000],  loss: 23.426081, mae: 14.719565, mean_q: 35.204548\n",
            " 12998/30000: episode: 2168, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3036.500 [607.000, 5689.000],  loss: 28.654951, mae: 16.016056, mean_q: 38.535976\n",
            " 13004/30000: episode: 2169, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3226.833 [148.000, 5697.000],  loss: 29.430716, mae: 15.229892, mean_q: 36.987995\n",
            " 13010/30000: episode: 2170, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1654.667 [238.000, 5283.000],  loss: 31.950287, mae: 15.256413, mean_q: 37.195988\n",
            " 13016/30000: episode: 2171, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3972.167 [3473.000, 5697.000],  loss: 36.635532, mae: 14.230171, mean_q: 34.611000\n",
            " 13022/30000: episode: 2172, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3309.500 [261.000, 5697.000],  loss: 42.151272, mae: 15.508414, mean_q: 37.633389\n",
            " 13028/30000: episode: 2173, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3527.500 [1191.000, 5697.000],  loss: 27.405348, mae: 14.419937, mean_q: 35.784859\n",
            " 13034/30000: episode: 2174, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3562.333 [1533.000, 5697.000],  loss: 25.546228, mae: 15.462388, mean_q: 37.360706\n",
            " 13040/30000: episode: 2175, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 3471.500 [1596.000, 5348.000],  loss: 22.445463, mae: 16.458275, mean_q: 39.448505\n",
            " 13046/30000: episode: 2176, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2696.667 [148.000, 5697.000],  loss: 50.339066, mae: 14.454582, mean_q: 35.834446\n",
            " 13052/30000: episode: 2177, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2299.333 [238.000, 5117.000],  loss: 43.286259, mae: 14.648843, mean_q: 35.189426\n",
            " 13058/30000: episode: 2178, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4697.500 [3921.000, 5697.000],  loss: 31.569040, mae: 14.552495, mean_q: 34.748325\n",
            " 13064/30000: episode: 2179, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3415.167 [781.000, 5697.000],  loss: 31.427818, mae: 16.265966, mean_q: 38.338123\n",
            " 13070/30000: episode: 2180, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3100.833 [537.000, 5697.000],  loss: 43.216846, mae: 15.575885, mean_q: 36.573475\n",
            " 13076/30000: episode: 2181, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3471.333 [303.000, 5129.000],  loss: 26.689310, mae: 14.839478, mean_q: 36.216373\n",
            " 13082/30000: episode: 2182, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2822.333 [607.000, 5697.000],  loss: 44.184464, mae: 14.990810, mean_q: 35.251896\n",
            " 13088/30000: episode: 2183, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3988.500 [966.000, 5697.000],  loss: 32.477711, mae: 14.319686, mean_q: 34.760654\n",
            " 13094/30000: episode: 2184, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3846.000 [1127.000, 5072.000],  loss: 38.585892, mae: 15.052127, mean_q: 35.333523\n",
            " 13100/30000: episode: 2185, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3281.000 [285.000, 5544.000],  loss: 45.607830, mae: 14.613948, mean_q: 34.653313\n",
            " 13106/30000: episode: 2186, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3396.167 [1120.000, 5697.000],  loss: 37.903481, mae: 13.872914, mean_q: 33.386398\n",
            " 13112/30000: episode: 2187, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3257.667 [1461.000, 3958.000],  loss: 36.374664, mae: 15.578114, mean_q: 37.057362\n",
            " 13118/30000: episode: 2188, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2440.167 [721.000, 5689.000],  loss: 50.254669, mae: 15.162910, mean_q: 35.424458\n",
            " 13124/30000: episode: 2189, duration: 0.164s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4719.167 [3525.000, 5715.000],  loss: 26.325731, mae: 15.674812, mean_q: 37.267712\n",
            " 13130/30000: episode: 2190, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2963.167 [238.000, 5532.000],  loss: 46.669144, mae: 16.280867, mean_q: 38.262394\n",
            " 13136/30000: episode: 2191, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1482.167 [106.000, 3265.000],  loss: 23.809427, mae: 15.450809, mean_q: 35.981899\n",
            " 13142/30000: episode: 2192, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 3238.500 [285.000, 4742.000],  loss: 51.225601, mae: 15.299563, mean_q: 36.134384\n",
            " 13148/30000: episode: 2193, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2772.667 [721.000, 5204.000],  loss: 43.336452, mae: 16.436510, mean_q: 37.971889\n",
            " 13154/30000: episode: 2194, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3198.167 [721.000, 5486.000],  loss: 32.527699, mae: 15.857559, mean_q: 37.160065\n",
            " 13160/30000: episode: 2195, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 40.000, mean reward:  6.667 [ 0.000, 25.000], mean action: 4535.000 [3505.000, 5279.000],  loss: 39.978260, mae: 14.626457, mean_q: 35.224697\n",
            " 13166/30000: episode: 2196, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2784.000 [721.000, 5211.000],  loss: 46.480167, mae: 14.504410, mean_q: 34.288937\n",
            " 13172/30000: episode: 2197, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3874.167 [1714.000, 5680.000],  loss: 33.057549, mae: 16.050989, mean_q: 37.126667\n",
            " 13178/30000: episode: 2198, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3375.500 [2095.000, 5161.000],  loss: 49.714081, mae: 17.126188, mean_q: 39.125790\n",
            " 13184/30000: episode: 2199, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 100.000, mean reward: 16.667 [ 0.000, 30.000], mean action: 2396.500 [183.000, 5750.000],  loss: 50.500748, mae: 15.420639, mean_q: 36.712173\n",
            " 13190/30000: episode: 2200, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 3870.667 [1596.000, 5656.000],  loss: 37.003582, mae: 15.187621, mean_q: 35.999645\n",
            " 13196/30000: episode: 2201, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 25.000, mean reward:  4.167 [-5.000, 15.000], mean action: 2182.500 [303.000, 4616.000],  loss: 33.376305, mae: 15.673027, mean_q: 37.306988\n",
            " 13202/30000: episode: 2202, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1530.500 [1137.000, 2027.000],  loss: 52.987019, mae: 14.424855, mean_q: 33.776539\n",
            " 13208/30000: episode: 2203, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3090.833 [42.000, 5715.000],  loss: 50.909760, mae: 15.714284, mean_q: 36.803242\n",
            " 13214/30000: episode: 2204, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  5.000], mean action: 2231.667 [242.000, 5072.000],  loss: 28.037626, mae: 14.465454, mean_q: 34.092373\n",
            " 13220/30000: episode: 2205, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3913.333 [816.000, 5129.000],  loss: 40.855305, mae: 15.869450, mean_q: 37.577732\n",
            " 13226/30000: episode: 2206, duration: 0.217s, episode steps:   6, steps per second:  28, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3271.333 [742.000, 4185.000],  loss: 39.232082, mae: 14.700061, mean_q: 34.513851\n",
            " 13232/30000: episode: 2207, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2403.167 [1260.000, 3901.000],  loss: 38.822197, mae: 14.858045, mean_q: 34.798347\n",
            " 13238/30000: episode: 2208, duration: 0.239s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2547.667 [1260.000, 3542.000],  loss: 65.268196, mae: 15.645576, mean_q: 36.286770\n",
            " 13244/30000: episode: 2209, duration: 0.245s, episode steps:   6, steps per second:  24, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2316.333 [1032.000, 4792.000],  loss: 34.096470, mae: 14.353939, mean_q: 34.336784\n",
            " 13250/30000: episode: 2210, duration: 0.234s, episode steps:   6, steps per second:  26, episode reward: 25.000, mean reward:  4.167 [-5.000, 15.000], mean action: 2790.500 [1393.000, 3985.000],  loss: 38.637432, mae: 17.195763, mean_q: 39.323215\n",
            " 13256/30000: episode: 2211, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward: 150.000, mean reward: 25.000 [ 0.000, 35.000], mean action: 2762.500 [238.000, 4185.000],  loss: 39.777397, mae: 15.383766, mean_q: 36.518330\n",
            " 13262/30000: episode: 2212, duration: 0.240s, episode steps:   6, steps per second:  25, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 2269.167 [1347.000, 3051.000],  loss: 27.641851, mae: 13.552597, mean_q: 32.483868\n",
            " 13268/30000: episode: 2213, duration: 0.240s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3323.333 [1127.000, 5204.000],  loss: 48.637913, mae: 16.029970, mean_q: 37.453754\n",
            " 13274/30000: episode: 2214, duration: 0.261s, episode steps:   6, steps per second:  23, episode reward: 50.000, mean reward:  8.333 [ 0.000, 35.000], mean action: 3358.833 [285.000, 5211.000],  loss: 19.034645, mae: 15.111411, mean_q: 35.806873\n",
            " 13280/30000: episode: 2215, duration: 0.259s, episode steps:   6, steps per second:  23, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 4468.333 [3189.000, 5433.000],  loss: 36.381878, mae: 15.002244, mean_q: 35.270309\n",
            " 13286/30000: episode: 2216, duration: 0.265s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000, 10.000], mean action: 1013.000 [285.000, 2529.000],  loss: 41.656723, mae: 15.014470, mean_q: 34.716763\n",
            " 13292/30000: episode: 2217, duration: 0.255s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2936.500 [423.000, 5147.000],  loss: 27.494699, mae: 15.177091, mean_q: 35.940231\n",
            " 13298/30000: episode: 2218, duration: 0.189s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3895.833 [2578.000, 5579.000],  loss: 66.596405, mae: 14.947175, mean_q: 35.540588\n",
            " 13304/30000: episode: 2219, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3845.667 [1596.000, 5396.000],  loss: 37.878803, mae: 14.351037, mean_q: 33.997406\n",
            " 13310/30000: episode: 2220, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 1290.667 [285.000, 3419.000],  loss: 33.693161, mae: 15.801089, mean_q: 36.329044\n",
            " 13316/30000: episode: 2221, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2046.667 [238.000, 5679.000],  loss: 33.442276, mae: 14.767541, mean_q: 34.388988\n",
            " 13322/30000: episode: 2222, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2939.000 [285.000, 5742.000],  loss: 57.667095, mae: 14.252617, mean_q: 33.550117\n",
            " 13328/30000: episode: 2223, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3070.500 [1714.000, 3748.000],  loss: 42.113941, mae: 14.107928, mean_q: 34.034397\n",
            " 13334/30000: episode: 2224, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2412.000 [303.000, 5486.000],  loss: 61.135456, mae: 15.360299, mean_q: 36.507885\n",
            " 13340/30000: episode: 2225, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3177.500 [1253.000, 5750.000],  loss: 58.104340, mae: 16.587227, mean_q: 39.028160\n",
            " 13346/30000: episode: 2226, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3331.500 [742.000, 4978.000],  loss: 42.390278, mae: 16.080591, mean_q: 37.903122\n",
            " 13352/30000: episode: 2227, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3325.333 [2128.000, 5433.000],  loss: 30.942688, mae: 13.547368, mean_q: 33.263561\n",
            " 13358/30000: episode: 2228, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4188.167 [1530.000, 5750.000],  loss: 37.240917, mae: 14.873740, mean_q: 34.843353\n",
            " 13364/30000: episode: 2229, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 30.000, mean reward:  5.000 [ 0.000, 30.000], mean action: 3926.000 [873.000, 5597.000],  loss: 63.513275, mae: 15.212715, mean_q: 35.290981\n",
            " 13370/30000: episode: 2230, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2432.333 [183.000, 5750.000],  loss: 34.389812, mae: 14.770003, mean_q: 34.963787\n",
            " 13376/30000: episode: 2231, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3401.500 [1714.000, 5597.000],  loss: 38.185741, mae: 14.949002, mean_q: 35.222584\n",
            " 13382/30000: episode: 2232, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3107.667 [1253.000, 5597.000],  loss: 31.163218, mae: 15.694458, mean_q: 37.605976\n",
            " 13388/30000: episode: 2233, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4625.333 [3358.000, 5597.000],  loss: 39.052315, mae: 15.094120, mean_q: 35.524906\n",
            " 13394/30000: episode: 2234, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2903.333 [485.000, 5750.000],  loss: 37.207630, mae: 15.571193, mean_q: 37.555042\n",
            " 13400/30000: episode: 2235, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3173.500 [2444.000, 4152.000],  loss: 40.233349, mae: 14.741825, mean_q: 34.973373\n",
            " 13406/30000: episode: 2236, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2912.000 [183.000, 5597.000],  loss: 38.920498, mae: 15.239589, mean_q: 35.242203\n",
            " 13412/30000: episode: 2237, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4394.333 [1057.000, 5680.000],  loss: 43.542755, mae: 14.195714, mean_q: 33.651745\n",
            " 13418/30000: episode: 2238, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 30.000, mean reward:  5.000 [ 0.000, 20.000], mean action: 2682.500 [271.000, 5597.000],  loss: 27.136660, mae: 16.648708, mean_q: 38.364582\n",
            " 13424/30000: episode: 2239, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2135.000 [720.000, 3358.000],  loss: 46.791168, mae: 15.897369, mean_q: 37.298019\n",
            " 13430/30000: episode: 2240, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3658.333 [1596.000, 5656.000],  loss: 28.925440, mae: 14.584903, mean_q: 34.916130\n",
            " 13436/30000: episode: 2241, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3213.167 [1530.000, 5597.000],  loss: 42.658981, mae: 15.003150, mean_q: 34.843922\n",
            " 13442/30000: episode: 2242, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2870.833 [617.000, 5204.000],  loss: 28.917543, mae: 14.640457, mean_q: 35.013931\n",
            " 13448/30000: episode: 2243, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3466.333 [1683.000, 5373.000],  loss: 31.658266, mae: 13.623895, mean_q: 32.425922\n",
            " 13454/30000: episode: 2244, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2527.500 [1504.000, 4407.000],  loss: 35.779377, mae: 14.141725, mean_q: 33.669434\n",
            " 13460/30000: episode: 2245, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3386.333 [1141.000, 5404.000],  loss: 67.775833, mae: 14.481278, mean_q: 34.884281\n",
            " 13466/30000: episode: 2246, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3475.667 [2329.000, 5597.000],  loss: 44.373425, mae: 14.824243, mean_q: 35.628094\n",
            " 13472/30000: episode: 2247, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3737.833 [1828.000, 5148.000],  loss: 39.653698, mae: 15.449933, mean_q: 35.760326\n",
            " 13478/30000: episode: 2248, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2761.333 [733.000, 5597.000],  loss: 39.750969, mae: 14.605918, mean_q: 34.865582\n",
            " 13484/30000: episode: 2249, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2512.500 [26.000, 5737.000],  loss: 38.756916, mae: 15.701455, mean_q: 36.492519\n",
            " 13490/30000: episode: 2250, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 140.000, mean reward: 23.333 [ 0.000, 35.000], mean action: 3557.333 [1596.000, 5597.000],  loss: 36.793411, mae: 16.144075, mean_q: 37.678822\n",
            " 13496/30000: episode: 2251, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4493.333 [1632.000, 5597.000],  loss: 30.239340, mae: 14.867919, mean_q: 34.845348\n",
            " 13502/30000: episode: 2252, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4564.667 [2800.000, 5597.000],  loss: 38.410553, mae: 16.323980, mean_q: 38.305935\n",
            " 13508/30000: episode: 2253, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3200.500 [2444.000, 5597.000],  loss: 47.411198, mae: 15.078471, mean_q: 36.498310\n",
            " 13514/30000: episode: 2254, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3410.500 [2255.000, 3941.000],  loss: 47.730228, mae: 14.247917, mean_q: 34.391071\n",
            " 13520/30000: episode: 2255, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward: 30.000, mean reward:  5.000 [ 0.000, 25.000], mean action: 2855.167 [1202.000, 4486.000],  loss: 27.469381, mae: 15.437163, mean_q: 35.860584\n",
            " 13526/30000: episode: 2256, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3194.167 [1599.000, 5211.000],  loss: 24.177292, mae: 15.220174, mean_q: 35.122070\n",
            " 13532/30000: episode: 2257, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3134.500 [439.000, 5597.000],  loss: 48.532703, mae: 14.637553, mean_q: 34.669666\n",
            " 13538/30000: episode: 2258, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3376.000 [1886.000, 5486.000],  loss: 31.648941, mae: 15.378360, mean_q: 35.433632\n",
            " 13544/30000: episode: 2259, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 3075.500 [606.000, 5597.000],  loss: 43.572140, mae: 14.649418, mean_q: 34.958473\n",
            " 13550/30000: episode: 2260, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2822.833 [26.000, 5721.000],  loss: 62.501816, mae: 13.909686, mean_q: 33.581799\n",
            " 13556/30000: episode: 2261, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2624.333 [461.000, 4802.000],  loss: 25.177214, mae: 13.643327, mean_q: 32.958729\n",
            " 13562/30000: episode: 2262, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3454.167 [1224.000, 5000.000],  loss: 51.359409, mae: 15.904180, mean_q: 37.369141\n",
            " 13568/30000: episode: 2263, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4154.500 [1084.000, 5597.000],  loss: 41.915447, mae: 14.399289, mean_q: 35.087597\n",
            " 13574/30000: episode: 2264, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3750.167 [1347.000, 5597.000],  loss: 46.259808, mae: 15.409198, mean_q: 36.232044\n",
            " 13580/30000: episode: 2265, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 95.000, mean reward: 15.833 [ 0.000, 35.000], mean action: 4353.667 [3591.000, 5597.000],  loss: 44.345890, mae: 15.679437, mean_q: 37.903366\n",
            " 13586/30000: episode: 2266, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4271.500 [1736.000, 5597.000],  loss: 47.323589, mae: 14.636825, mean_q: 35.608944\n",
            " 13592/30000: episode: 2267, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2940.167 [1404.000, 4464.000],  loss: 32.496323, mae: 15.156138, mean_q: 36.071938\n",
            " 13598/30000: episode: 2268, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: 160.000, mean reward: 26.667 [ 0.000, 35.000], mean action: 3988.833 [1596.000, 5597.000],  loss: 41.010448, mae: 15.357013, mean_q: 36.753193\n",
            " 13604/30000: episode: 2269, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2837.833 [1555.000, 5597.000],  loss: 44.597965, mae: 14.986123, mean_q: 36.279949\n",
            " 13610/30000: episode: 2270, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2689.667 [1439.000, 5597.000],  loss: 47.227139, mae: 15.885101, mean_q: 37.974586\n",
            " 13616/30000: episode: 2271, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward: 40.000, mean reward:  6.667 [ 0.000, 15.000], mean action: 2206.833 [1224.000, 5597.000],  loss: 27.672081, mae: 14.383996, mean_q: 35.000751\n",
            " 13622/30000: episode: 2272, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4570.833 [3062.000, 5597.000],  loss: 53.593212, mae: 15.308816, mean_q: 36.528378\n",
            " 13628/30000: episode: 2273, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3402.667 [596.000, 5597.000],  loss: 45.848423, mae: 15.261661, mean_q: 35.725101\n",
            " 13634/30000: episode: 2274, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2366.500 [648.000, 4638.000],  loss: 39.034473, mae: 15.316237, mean_q: 36.362225\n",
            " 13640/30000: episode: 2275, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2709.833 [721.000, 5597.000],  loss: 47.124950, mae: 13.675067, mean_q: 33.382584\n",
            " 13646/30000: episode: 2276, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3395.333 [1009.000, 5597.000],  loss: 42.997204, mae: 16.591776, mean_q: 38.332123\n",
            " 13652/30000: episode: 2277, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3129.333 [1736.000, 5597.000],  loss: 76.390907, mae: 16.422449, mean_q: 37.989777\n",
            " 13658/30000: episode: 2278, duration: 0.219s, episode steps:   6, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3538.667 [485.000, 5608.000],  loss: 37.780060, mae: 14.463275, mean_q: 34.484013\n",
            " 13664/30000: episode: 2279, duration: 0.263s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2514.000 [1484.000, 5656.000],  loss: 50.876705, mae: 14.215992, mean_q: 33.250076\n",
            " 13670/30000: episode: 2280, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2831.500 [463.000, 4941.000],  loss: 41.223728, mae: 15.652316, mean_q: 37.067913\n",
            " 13676/30000: episode: 2281, duration: 0.237s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2070.333 [140.000, 5620.000],  loss: 37.911667, mae: 14.710751, mean_q: 34.907616\n",
            " 13682/30000: episode: 2282, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2113.167 [126.000, 3921.000],  loss: 38.183453, mae: 16.454359, mean_q: 38.385067\n",
            " 13688/30000: episode: 2283, duration: 0.269s, episode steps:   6, steps per second:  22, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 4123.500 [2420.000, 5656.000],  loss: 41.341690, mae: 14.266097, mean_q: 33.529404\n",
            " 13694/30000: episode: 2284, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2730.667 [670.000, 5044.000],  loss: 27.477922, mae: 14.671725, mean_q: 35.302181\n",
            " 13700/30000: episode: 2285, duration: 0.237s, episode steps:   6, steps per second:  25, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3427.167 [437.000, 5585.000],  loss: 60.495861, mae: 16.542267, mean_q: 37.789776\n",
            " 13706/30000: episode: 2286, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward: 50.000, mean reward:  8.333 [-5.000, 45.000], mean action: 2528.333 [1484.000, 4360.000],  loss: 48.646923, mae: 14.975415, mean_q: 35.673138\n",
            " 13712/30000: episode: 2287, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2352.500 [1530.000, 3625.000],  loss: 33.036545, mae: 15.640274, mean_q: 37.565308\n",
            " 13718/30000: episode: 2288, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4248.667 [1530.000, 5608.000],  loss: 28.124748, mae: 14.330894, mean_q: 34.640778\n",
            " 13724/30000: episode: 2289, duration: 0.262s, episode steps:   6, steps per second:  23, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 2526.000 [1530.000, 3795.000],  loss: 40.415134, mae: 15.217889, mean_q: 35.616131\n",
            " 13730/30000: episode: 2290, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 1509.000 [1404.000, 1530.000],  loss: 34.997471, mae: 13.958844, mean_q: 33.545357\n",
            " 13736/30000: episode: 2291, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3137.333 [1404.000, 4423.000],  loss: 44.139744, mae: 15.682397, mean_q: 36.954319\n",
            " 13742/30000: episode: 2292, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1771.500 [487.000, 3768.000],  loss: 40.582539, mae: 16.670481, mean_q: 39.020367\n",
            " 13748/30000: episode: 2293, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2418.667 [678.000, 3797.000],  loss: 39.272896, mae: 14.172845, mean_q: 35.037632\n",
            " 13754/30000: episode: 2294, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 85.000, mean reward: 14.167 [ 0.000, 30.000], mean action: 1399.667 [1202.000, 1530.000],  loss: 42.767838, mae: 15.720761, mean_q: 37.402664\n",
            " 13760/30000: episode: 2295, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2731.167 [1404.000, 4667.000],  loss: 33.513016, mae: 14.653096, mean_q: 35.309750\n",
            " 13766/30000: episode: 2296, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2941.333 [1404.000, 5532.000],  loss: 59.301914, mae: 13.633091, mean_q: 33.679016\n",
            " 13772/30000: episode: 2297, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2901.500 [1736.000, 5485.000],  loss: 26.562263, mae: 14.556557, mean_q: 35.141216\n",
            " 13778/30000: episode: 2298, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1463.833 [542.000, 3029.000],  loss: 26.505346, mae: 14.549876, mean_q: 35.235565\n",
            " 13784/30000: episode: 2299, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2798.167 [1404.000, 5656.000],  loss: 42.892120, mae: 14.983541, mean_q: 36.007133\n",
            " 13790/30000: episode: 2300, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2258.833 [140.000, 3741.000],  loss: 30.733940, mae: 15.060328, mean_q: 36.241375\n",
            " 13796/30000: episode: 2301, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2315.167 [879.000, 5466.000],  loss: 33.813431, mae: 15.557327, mean_q: 37.157021\n",
            " 13802/30000: episode: 2302, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2358.500 [353.000, 5608.000],  loss: 54.565159, mae: 14.788418, mean_q: 35.865704\n",
            " 13808/30000: episode: 2303, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3196.000 [614.000, 4894.000],  loss: 38.107151, mae: 15.933338, mean_q: 37.353226\n",
            " 13814/30000: episode: 2304, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 1404.000 [1404.000, 1404.000],  loss: 33.044018, mae: 14.826122, mean_q: 35.745224\n",
            " 13820/30000: episode: 2305, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1817.333 [404.000, 4884.000],  loss: 25.376837, mae: 14.606013, mean_q: 35.055576\n",
            " 13826/30000: episode: 2306, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 1671.000 [172.000, 3921.000],  loss: 60.418766, mae: 15.037148, mean_q: 35.732807\n",
            " 13832/30000: episode: 2307, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2476.833 [820.000, 4193.000],  loss: 44.633945, mae: 15.100815, mean_q: 36.332962\n",
            " 13838/30000: episode: 2308, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 1636.500 [69.000, 5689.000],  loss: 30.880547, mae: 15.677188, mean_q: 36.888287\n",
            " 13844/30000: episode: 2309, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1105.667 [381.000, 1504.000],  loss: 35.810532, mae: 16.877821, mean_q: 39.530052\n",
            " 13850/30000: episode: 2310, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1774.333 [61.000, 5329.000],  loss: 34.744141, mae: 13.423581, mean_q: 32.542446\n",
            " 13856/30000: episode: 2311, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3199.667 [1404.000, 4819.000],  loss: 44.031113, mae: 15.558090, mean_q: 36.744095\n",
            " 13862/30000: episode: 2312, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2708.500 [820.000, 5715.000],  loss: 28.134970, mae: 13.596059, mean_q: 33.019409\n",
            " 13868/30000: episode: 2313, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3420.333 [1530.000, 4749.000],  loss: 32.539547, mae: 13.699737, mean_q: 33.288513\n",
            " 13874/30000: episode: 2314, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1606.500 [1404.000, 1952.000],  loss: 35.443825, mae: 15.707978, mean_q: 36.971558\n",
            " 13880/30000: episode: 2315, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1805.000 [26.000, 3019.000],  loss: 30.816597, mae: 16.022280, mean_q: 37.450867\n",
            " 13886/30000: episode: 2316, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2668.833 [4.000, 4749.000],  loss: 39.007923, mae: 16.299242, mean_q: 37.932682\n",
            " 13892/30000: episode: 2317, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [-5.000,  5.000], mean action: 4019.667 [3625.000, 4491.000],  loss: 47.892994, mae: 15.171199, mean_q: 35.720455\n",
            " 13898/30000: episode: 2318, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3058.000 [537.000, 4783.000],  loss: 33.125778, mae: 14.353726, mean_q: 34.280838\n",
            " 13904/30000: episode: 2319, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 55.000, mean reward:  9.167 [ 0.000, 25.000], mean action: 2964.833 [1202.000, 4913.000],  loss: 28.763342, mae: 15.029828, mean_q: 35.047684\n",
            " 13910/30000: episode: 2320, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000, 10.000], mean action: 2696.500 [2489.000, 2738.000],  loss: 59.446651, mae: 15.026829, mean_q: 35.550838\n",
            " 13916/30000: episode: 2321, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 30.000, mean reward:  5.000 [-5.000, 35.000], mean action: 2335.333 [1404.000, 4898.000],  loss: 25.043070, mae: 14.100559, mean_q: 33.469639\n",
            " 13922/30000: episode: 2322, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3031.333 [1141.000, 5608.000],  loss: 58.612000, mae: 16.050432, mean_q: 37.925629\n",
            " 13928/30000: episode: 2323, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2916.833 [537.000, 5715.000],  loss: 48.145443, mae: 13.993012, mean_q: 33.462177\n",
            " 13934/30000: episode: 2324, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2097.333 [1404.000, 2444.000],  loss: 19.046709, mae: 16.212723, mean_q: 37.666988\n",
            " 13940/30000: episode: 2325, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3545.667 [1883.000, 5626.000],  loss: 27.967291, mae: 15.252370, mean_q: 36.064915\n",
            " 13946/30000: episode: 2326, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3259.000 [1596.000, 5717.000],  loss: 58.348896, mae: 15.600922, mean_q: 37.450829\n",
            " 13952/30000: episode: 2327, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1616.500 [751.000, 4251.000],  loss: 35.174709, mae: 15.048016, mean_q: 35.536320\n",
            " 13958/30000: episode: 2328, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3058.000 [1926.000, 4796.000],  loss: 48.155441, mae: 15.247647, mean_q: 36.289143\n",
            " 13964/30000: episode: 2329, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2998.667 [1901.000, 5535.000],  loss: 35.095963, mae: 15.865940, mean_q: 37.513546\n",
            " 13970/30000: episode: 2330, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2361.167 [1175.000, 4273.000],  loss: 38.085674, mae: 14.806042, mean_q: 34.733364\n",
            " 13976/30000: episode: 2331, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3871.667 [3625.000, 3921.000],  loss: 34.608253, mae: 16.453913, mean_q: 38.598591\n",
            " 13982/30000: episode: 2332, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2012.833 [62.000, 5487.000],  loss: 52.174992, mae: 15.904725, mean_q: 36.739288\n",
            " 13988/30000: episode: 2333, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2881.833 [2139.000, 3839.000],  loss: 56.403919, mae: 15.042084, mean_q: 35.579910\n",
            " 13994/30000: episode: 2334, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2009.833 [590.000, 5296.000],  loss: 35.425793, mae: 15.865280, mean_q: 37.513958\n",
            " 14000/30000: episode: 2335, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1874.500 [1151.000, 3921.000],  loss: 45.047394, mae: 15.621872, mean_q: 37.459003\n",
            " 14006/30000: episode: 2336, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3333.000 [733.000, 5410.000],  loss: 59.821365, mae: 15.552438, mean_q: 36.592426\n",
            " 14012/30000: episode: 2337, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2530.333 [826.000, 5427.000],  loss: 34.135746, mae: 15.602725, mean_q: 36.374241\n",
            " 14018/30000: episode: 2338, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2768.667 [818.000, 5703.000],  loss: 27.145508, mae: 15.317635, mean_q: 36.123634\n",
            " 14024/30000: episode: 2339, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3139.667 [1398.000, 5575.000],  loss: 34.281910, mae: 14.909953, mean_q: 35.046535\n",
            " 14030/30000: episode: 2340, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1777.000 [449.000, 2667.000],  loss: 33.249603, mae: 15.522689, mean_q: 36.448399\n",
            " 14036/30000: episode: 2341, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2386.000 [423.000, 5426.000],  loss: 34.938400, mae: 14.871831, mean_q: 34.794399\n",
            " 14042/30000: episode: 2342, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 50.000, mean reward:  8.333 [ 0.000, 30.000], mean action: 3723.833 [1127.000, 5656.000],  loss: 45.586777, mae: 14.862862, mean_q: 34.700203\n",
            " 14048/30000: episode: 2343, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3819.500 [537.000, 5466.000],  loss: 28.530724, mae: 15.417058, mean_q: 35.939201\n",
            " 14054/30000: episode: 2344, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2942.167 [26.000, 5395.000],  loss: 30.927361, mae: 14.932229, mean_q: 34.693394\n",
            " 14060/30000: episode: 2345, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4014.667 [2355.000, 5640.000],  loss: 35.495182, mae: 15.275760, mean_q: 36.153046\n",
            " 14066/30000: episode: 2346, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3328.333 [79.000, 5575.000],  loss: 48.174595, mae: 14.974816, mean_q: 36.230953\n",
            " 14072/30000: episode: 2347, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3850.667 [1868.000, 4695.000],  loss: 50.581799, mae: 15.472064, mean_q: 35.907085\n",
            " 14078/30000: episode: 2348, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 3344.333 [1473.000, 5628.000],  loss: 24.538267, mae: 13.637547, mean_q: 32.340042\n",
            " 14084/30000: episode: 2349, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3233.000 [1202.000, 5447.000],  loss: 35.988155, mae: 15.573987, mean_q: 36.083778\n",
            " 14090/30000: episode: 2350, duration: 0.222s, episode steps:   6, steps per second:  27, episode reward: 50.000, mean reward:  8.333 [ 0.000, 25.000], mean action: 2811.167 [238.000, 5426.000],  loss: 42.255360, mae: 15.654784, mean_q: 36.804459\n",
            " 14096/30000: episode: 2351, duration: 0.266s, episode steps:   6, steps per second:  23, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3033.167 [1661.000, 4208.000],  loss: 37.251343, mae: 15.281871, mean_q: 36.168652\n",
            " 14102/30000: episode: 2352, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1436.500 [162.000, 3625.000],  loss: 35.067535, mae: 15.989207, mean_q: 37.407322\n",
            " 14108/30000: episode: 2353, duration: 0.260s, episode steps:   6, steps per second:  23, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 2536.167 [138.000, 4138.000],  loss: 35.564816, mae: 14.890260, mean_q: 35.621510\n",
            " 14114/30000: episode: 2354, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3129.667 [238.000, 5703.000],  loss: 38.695469, mae: 15.006908, mean_q: 35.578995\n",
            " 14120/30000: episode: 2355, duration: 0.259s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2074.667 [91.000, 3625.000],  loss: 33.539280, mae: 15.745484, mean_q: 37.568142\n",
            " 14126/30000: episode: 2356, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1576.333 [172.000, 3230.000],  loss: 28.002304, mae: 14.665658, mean_q: 35.935211\n",
            " 14132/30000: episode: 2357, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2987.500 [271.000, 5148.000],  loss: 46.377319, mae: 15.459786, mean_q: 36.985382\n",
            " 14138/30000: episode: 2358, duration: 0.266s, episode steps:   6, steps per second:  23, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 3625.000 [3625.000, 3625.000],  loss: 26.783041, mae: 15.476338, mean_q: 36.365940\n",
            " 14144/30000: episode: 2359, duration: 0.257s, episode steps:   6, steps per second:  23, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2503.333 [590.000, 4497.000],  loss: 27.187599, mae: 14.396183, mean_q: 34.103233\n",
            " 14150/30000: episode: 2360, duration: 0.269s, episode steps:   6, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4505.500 [2928.000, 5722.000],  loss: 49.580227, mae: 15.102966, mean_q: 35.495621\n",
            " 14156/30000: episode: 2361, duration: 0.214s, episode steps:   6, steps per second:  28, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2063.167 [1151.000, 3625.000],  loss: 50.000874, mae: 16.357504, mean_q: 38.430264\n",
            " 14162/30000: episode: 2362, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2316.500 [1530.000, 5424.000],  loss: 37.662354, mae: 15.038513, mean_q: 36.680111\n",
            " 14168/30000: episode: 2363, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3282.167 [1596.000, 5734.000],  loss: 50.048374, mae: 14.545409, mean_q: 34.854431\n",
            " 14174/30000: episode: 2364, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2838.833 [525.000, 5582.000],  loss: 40.048183, mae: 14.944965, mean_q: 36.083817\n",
            " 14180/30000: episode: 2365, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3102.167 [1029.000, 5433.000],  loss: 44.219025, mae: 14.585362, mean_q: 35.450489\n",
            " 14186/30000: episode: 2366, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3032.167 [1257.000, 5708.000],  loss: 48.823349, mae: 14.871449, mean_q: 35.883331\n",
            " 14192/30000: episode: 2367, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 1286.000 [895.000, 1665.000],  loss: 40.485016, mae: 14.568255, mean_q: 35.159492\n",
            " 14198/30000: episode: 2368, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2245.000 [224.000, 3701.000],  loss: 62.183498, mae: 15.001809, mean_q: 35.656223\n",
            " 14204/30000: episode: 2369, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3850.833 [720.000, 5708.000],  loss: 31.086945, mae: 15.170642, mean_q: 35.784729\n",
            " 14210/30000: episode: 2370, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3499.500 [238.000, 5626.000],  loss: 18.954422, mae: 15.900588, mean_q: 37.408077\n",
            " 14216/30000: episode: 2371, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2059.333 [353.000, 4954.000],  loss: 44.218746, mae: 14.708099, mean_q: 35.169788\n",
            " 14222/30000: episode: 2372, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1925.167 [383.000, 2861.000],  loss: 52.530670, mae: 15.309752, mean_q: 35.805546\n",
            " 14228/30000: episode: 2373, duration: 0.185s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2989.000 [353.000, 5656.000],  loss: 41.050716, mae: 14.990211, mean_q: 35.494965\n",
            " 14234/30000: episode: 2374, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2424.833 [428.000, 5703.000],  loss: 38.890392, mae: 14.764783, mean_q: 35.384708\n",
            " 14240/30000: episode: 2375, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2861.167 [238.000, 5570.000],  loss: 48.175247, mae: 16.240801, mean_q: 37.608982\n",
            " 14246/30000: episode: 2376, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3346.667 [1484.000, 5259.000],  loss: 54.647152, mae: 15.465454, mean_q: 36.363220\n",
            " 14252/30000: episode: 2377, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3158.333 [2245.000, 4407.000],  loss: 29.482452, mae: 15.204959, mean_q: 35.626835\n",
            " 14258/30000: episode: 2378, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3124.000 [423.000, 5498.000],  loss: 36.250763, mae: 15.294799, mean_q: 36.557987\n",
            " 14264/30000: episode: 2379, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2959.167 [238.000, 5680.000],  loss: 39.602356, mae: 16.100929, mean_q: 37.339294\n",
            " 14270/30000: episode: 2380, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3279.667 [869.000, 5471.000],  loss: 25.547483, mae: 14.312406, mean_q: 33.985081\n",
            " 14276/30000: episode: 2381, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2502.000 [1243.000, 5411.000],  loss: 49.419941, mae: 13.597496, mean_q: 32.723679\n",
            " 14282/30000: episode: 2382, duration: 0.164s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3755.500 [1473.000, 5427.000],  loss: 36.129025, mae: 15.531136, mean_q: 36.659679\n",
            " 14288/30000: episode: 2383, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3018.000 [238.000, 4982.000],  loss: 40.083088, mae: 15.516900, mean_q: 36.994984\n",
            " 14294/30000: episode: 2384, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [-5.000,  5.000], mean action: 3372.667 [1202.000, 4491.000],  loss: 34.152164, mae: 14.766046, mean_q: 35.100731\n",
            " 14300/30000: episode: 2385, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3726.333 [1042.000, 5602.000],  loss: 50.856419, mae: 15.345917, mean_q: 36.709438\n",
            " 14306/30000: episode: 2386, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 130.000, mean reward: 21.667 [ 0.000, 35.000], mean action: 3101.000 [158.000, 4185.000],  loss: 44.521729, mae: 16.310408, mean_q: 39.469238\n",
            " 14312/30000: episode: 2387, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 1503.667 [518.000, 3183.000],  loss: 49.231903, mae: 15.542542, mean_q: 36.961712\n",
            " 14318/30000: episode: 2388, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 2799.000 [2799.000, 2799.000],  loss: 41.525913, mae: 14.086904, mean_q: 34.479584\n",
            " 14324/30000: episode: 2389, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  5.000], mean action: 3401.333 [1649.000, 5575.000],  loss: 41.750839, mae: 15.346835, mean_q: 36.875397\n",
            " 14330/30000: episode: 2390, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3078.667 [1440.000, 5151.000],  loss: 43.702438, mae: 14.554099, mean_q: 34.821491\n",
            " 14336/30000: episode: 2391, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2121.667 [224.000, 5147.000],  loss: 35.040798, mae: 15.749400, mean_q: 37.469479\n",
            " 14342/30000: episode: 2392, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2230.333 [238.000, 4522.000],  loss: 23.876068, mae: 15.287125, mean_q: 36.503555\n",
            " 14348/30000: episode: 2393, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2280.667 [1197.000, 4500.000],  loss: 27.242548, mae: 14.911445, mean_q: 35.882145\n",
            " 14354/30000: episode: 2394, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 35.000, mean reward:  5.833 [ 0.000, 35.000], mean action: 2861.333 [1247.000, 4391.000],  loss: 29.475199, mae: 15.167041, mean_q: 36.664333\n",
            " 14360/30000: episode: 2395, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3577.000 [140.000, 5651.000],  loss: 36.985714, mae: 14.899989, mean_q: 36.376980\n",
            " 14366/30000: episode: 2396, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 4106.500 [1838.000, 5575.000],  loss: 30.042303, mae: 14.041966, mean_q: 34.096607\n",
            " 14372/30000: episode: 2397, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 3693.167 [928.000, 5575.000],  loss: 42.249599, mae: 15.930702, mean_q: 37.419662\n",
            " 14378/30000: episode: 2398, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2584.000 [580.000, 4134.000],  loss: 31.757765, mae: 14.677814, mean_q: 35.664490\n",
            " 14384/30000: episode: 2399, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3883.500 [814.000, 5658.000],  loss: 38.127323, mae: 14.837959, mean_q: 35.537907\n",
            " 14390/30000: episode: 2400, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: 95.000, mean reward: 15.833 [ 0.000, 35.000], mean action: 3109.167 [1596.000, 5608.000],  loss: 43.638805, mae: 15.533039, mean_q: 36.767838\n",
            " 14396/30000: episode: 2401, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2329.500 [461.000, 4332.000],  loss: 26.330751, mae: 15.094666, mean_q: 36.225132\n",
            " 14402/30000: episode: 2402, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2698.667 [316.000, 4203.000],  loss: 32.955265, mae: 15.218801, mean_q: 36.590714\n",
            " 14408/30000: episode: 2403, duration: 0.186s, episode steps:   6, steps per second:  32, episode reward: 40.000, mean reward:  6.667 [ 0.000, 40.000], mean action: 3625.500 [1596.000, 5597.000],  loss: 30.763817, mae: 14.920461, mean_q: 36.063869\n",
            " 14414/30000: episode: 2404, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3636.667 [1560.000, 5575.000],  loss: 38.971279, mae: 14.510574, mean_q: 35.373512\n",
            " 14420/30000: episode: 2405, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2761.500 [71.000, 5582.000],  loss: 47.815807, mae: 15.568439, mean_q: 36.860149\n",
            " 14426/30000: episode: 2406, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2784.333 [1178.000, 5575.000],  loss: 33.184101, mae: 16.501692, mean_q: 38.066387\n",
            " 14432/30000: episode: 2407, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3751.333 [542.000, 5575.000],  loss: 26.165110, mae: 14.323018, mean_q: 34.651505\n",
            " 14438/30000: episode: 2408, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3435.333 [1398.000, 5575.000],  loss: 29.898668, mae: 14.865856, mean_q: 35.557575\n",
            " 14444/30000: episode: 2409, duration: 0.187s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1678.333 [238.000, 4015.000],  loss: 44.671051, mae: 15.513246, mean_q: 37.461910\n",
            " 14450/30000: episode: 2410, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 5159.500 [3082.000, 5575.000],  loss: 35.047424, mae: 14.475777, mean_q: 34.704823\n",
            " 14456/30000: episode: 2411, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3376.167 [946.000, 5575.000],  loss: 45.887875, mae: 16.449457, mean_q: 39.309746\n",
            " 14462/30000: episode: 2412, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3785.000 [1212.000, 5703.000],  loss: 46.433361, mae: 14.894586, mean_q: 36.256161\n",
            " 14468/30000: episode: 2413, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2535.333 [463.000, 5575.000],  loss: 33.708534, mae: 14.309959, mean_q: 35.248272\n",
            " 14474/30000: episode: 2414, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3629.333 [1693.000, 5575.000],  loss: 40.345383, mae: 13.207028, mean_q: 32.714954\n",
            " 14480/30000: episode: 2415, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3312.333 [755.000, 5140.000],  loss: 32.230381, mae: 14.909436, mean_q: 36.494839\n",
            " 14486/30000: episode: 2416, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4789.167 [4632.000, 5575.000],  loss: 25.335943, mae: 15.058945, mean_q: 36.893337\n",
            " 14492/30000: episode: 2417, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4082.500 [1937.000, 5626.000],  loss: 27.600740, mae: 14.504247, mean_q: 36.174488\n",
            " 14498/30000: episode: 2418, duration: 0.164s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2966.500 [1059.000, 4438.000],  loss: 52.089962, mae: 15.148064, mean_q: 36.807514\n",
            " 14504/30000: episode: 2419, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 5550.167 [5426.000, 5575.000],  loss: 38.412678, mae: 16.551056, mean_q: 38.869884\n",
            " 14510/30000: episode: 2420, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2949.667 [1141.000, 5575.000],  loss: 59.131153, mae: 16.352570, mean_q: 38.720718\n",
            " 14516/30000: episode: 2421, duration: 0.193s, episode steps:   6, steps per second:  31, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3877.000 [1125.000, 5575.000],  loss: 45.817379, mae: 16.078075, mean_q: 38.360348\n",
            " 14522/30000: episode: 2422, duration: 0.262s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3722.833 [85.000, 5575.000],  loss: 24.122934, mae: 14.908336, mean_q: 36.576797\n",
            " 14528/30000: episode: 2423, duration: 0.260s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3830.167 [1901.000, 5575.000],  loss: 31.784037, mae: 15.240089, mean_q: 37.456303\n",
            " 14534/30000: episode: 2424, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward: 130.000, mean reward: 21.667 [ 0.000, 35.000], mean action: 2759.000 [1596.000, 4185.000],  loss: 41.825317, mae: 13.808410, mean_q: 34.349995\n",
            " 14540/30000: episode: 2425, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward: 30.000, mean reward:  5.000 [ 0.000, 25.000], mean action: 2453.167 [943.000, 5575.000],  loss: 27.354441, mae: 13.508431, mean_q: 33.700626\n",
            " 14546/30000: episode: 2426, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2243.167 [537.000, 5742.000],  loss: 30.104708, mae: 13.695390, mean_q: 34.095753\n",
            " 14552/30000: episode: 2427, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3326.833 [1145.000, 5575.000],  loss: 30.325262, mae: 15.106992, mean_q: 37.779873\n",
            " 14558/30000: episode: 2428, duration: 0.245s, episode steps:   6, steps per second:  24, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 5315.000 [4015.000, 5575.000],  loss: 33.920055, mae: 15.055077, mean_q: 37.205780\n",
            " 14564/30000: episode: 2429, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward: 60.000, mean reward: 10.000 [ 0.000, 30.000], mean action: 3738.333 [946.000, 5575.000],  loss: 38.579060, mae: 14.632840, mean_q: 36.060318\n",
            " 14570/30000: episode: 2430, duration: 0.265s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3401.667 [427.000, 5575.000],  loss: 37.023640, mae: 14.162632, mean_q: 34.864849\n",
            " 14576/30000: episode: 2431, duration: 0.269s, episode steps:   6, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3842.333 [1799.000, 5588.000],  loss: 39.051254, mae: 14.928834, mean_q: 36.209148\n",
            " 14582/30000: episode: 2432, duration: 0.269s, episode steps:   6, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3488.833 [2412.000, 5575.000],  loss: 55.004444, mae: 14.651245, mean_q: 35.326691\n",
            " 14588/30000: episode: 2433, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 5315.000 [4015.000, 5575.000],  loss: 45.734333, mae: 13.906777, mean_q: 34.164379\n",
            " 14594/30000: episode: 2434, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4173.500 [1091.000, 5575.000],  loss: 56.213787, mae: 14.494756, mean_q: 35.349834\n",
            " 14600/30000: episode: 2435, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3624.000 [1575.000, 5575.000],  loss: 52.381283, mae: 15.178475, mean_q: 36.305599\n",
            " 14606/30000: episode: 2436, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2808.500 [238.000, 5716.000],  loss: 48.237930, mae: 14.575363, mean_q: 34.976952\n",
            " 14612/30000: episode: 2437, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2462.667 [504.000, 4616.000],  loss: 44.534729, mae: 14.777161, mean_q: 35.179813\n",
            " 14618/30000: episode: 2438, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2791.333 [1596.000, 3447.000],  loss: 56.940258, mae: 15.358887, mean_q: 35.581448\n",
            " 14624/30000: episode: 2439, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3994.500 [2390.000, 5658.000],  loss: 27.578035, mae: 13.812261, mean_q: 32.688629\n",
            " 14630/30000: episode: 2440, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 140.000, mean reward: 23.333 [ 0.000, 35.000], mean action: 3500.167 [238.000, 5640.000],  loss: 54.684540, mae: 14.607677, mean_q: 35.198624\n",
            " 14636/30000: episode: 2441, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2567.167 [238.000, 4246.000],  loss: 41.648930, mae: 13.959942, mean_q: 33.769192\n",
            " 14642/30000: episode: 2442, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2351.333 [261.000, 3921.000],  loss: 53.317181, mae: 16.028316, mean_q: 37.626358\n",
            " 14648/30000: episode: 2443, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3728.667 [1530.000, 5652.000],  loss: 28.331915, mae: 15.562940, mean_q: 37.141327\n",
            " 14654/30000: episode: 2444, duration: 0.374s, episode steps:   6, steps per second:  16, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4612.667 [1283.000, 5715.000],  loss: 41.399300, mae: 14.296105, mean_q: 34.326221\n",
            " 14660/30000: episode: 2445, duration: 0.225s, episode steps:   6, steps per second:  27, episode reward: 85.000, mean reward: 14.167 [ 0.000, 30.000], mean action: 2044.667 [711.000, 3094.000],  loss: 40.886444, mae: 15.784137, mean_q: 37.230885\n",
            " 14666/30000: episode: 2446, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3824.833 [1530.000, 5703.000],  loss: 38.971767, mae: 17.342495, mean_q: 41.057728\n",
            " 14672/30000: episode: 2447, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 75.000, mean reward: 12.500 [ 0.000, 25.000], mean action: 2325.333 [1202.000, 3640.000],  loss: 32.414356, mae: 14.200906, mean_q: 34.431229\n",
            " 14678/30000: episode: 2448, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2353.167 [1530.000, 5146.000],  loss: 56.203014, mae: 15.283080, mean_q: 36.865417\n",
            " 14684/30000: episode: 2449, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2967.167 [1634.000, 4522.000],  loss: 23.303843, mae: 14.687859, mean_q: 35.339268\n",
            " 14690/30000: episode: 2450, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1417.333 [608.000, 2128.000],  loss: 43.382050, mae: 14.021625, mean_q: 34.370342\n",
            " 14696/30000: episode: 2451, duration: 0.188s, episode steps:   6, steps per second:  32, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3433.000 [1530.000, 5656.000],  loss: 47.823330, mae: 15.632725, mean_q: 37.079174\n",
            " 14702/30000: episode: 2452, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2989.000 [1530.000, 4153.000],  loss: 44.358837, mae: 15.818024, mean_q: 38.629135\n",
            " 14708/30000: episode: 2453, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2918.833 [1530.000, 5204.000],  loss: 45.850281, mae: 16.340866, mean_q: 38.804626\n",
            " 14714/30000: episode: 2454, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: 35.000, mean reward:  5.833 [-5.000, 40.000], mean action: 1818.167 [733.000, 3332.000],  loss: 40.400055, mae: 15.087349, mean_q: 36.533550\n",
            " 14720/30000: episode: 2455, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2540.500 [459.000, 5545.000],  loss: 25.496567, mae: 12.586837, mean_q: 31.847372\n",
            " 14726/30000: episode: 2456, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2256.833 [487.000, 5147.000],  loss: 40.969589, mae: 14.133136, mean_q: 34.450062\n",
            " 14732/30000: episode: 2457, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2886.333 [907.000, 4616.000],  loss: 35.435070, mae: 14.959006, mean_q: 36.473186\n",
            " 14738/30000: episode: 2458, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2583.000 [733.000, 5199.000],  loss: 35.219780, mae: 15.914624, mean_q: 37.641445\n",
            " 14744/30000: episode: 2459, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 1530.000 [1530.000, 1530.000],  loss: 65.858940, mae: 16.367968, mean_q: 38.362225\n",
            " 14750/30000: episode: 2460, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 1558.167 [751.000, 2766.000],  loss: 24.947409, mae: 14.088744, mean_q: 34.470562\n",
            " 14756/30000: episode: 2461, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3083.333 [2455.000, 3209.000],  loss: 27.437073, mae: 14.743148, mean_q: 35.782909\n",
            " 14762/30000: episode: 2462, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2633.500 [587.000, 5120.000],  loss: 57.026672, mae: 15.494548, mean_q: 37.254993\n",
            " 14768/30000: episode: 2463, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: 50.000, mean reward:  8.333 [ 0.000, 20.000], mean action: 2884.000 [1059.000, 5402.000],  loss: 36.411804, mae: 16.432245, mean_q: 38.502457\n",
            " 14774/30000: episode: 2464, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2942.833 [1041.000, 5597.000],  loss: 36.874699, mae: 14.762881, mean_q: 35.699478\n",
            " 14780/30000: episode: 2465, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: 15.000, mean reward:  2.500 [-5.000, 25.000], mean action: 3530.667 [1901.000, 5640.000],  loss: 48.994251, mae: 13.765228, mean_q: 33.807480\n",
            " 14786/30000: episode: 2466, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1970.167 [394.000, 5649.000],  loss: 33.790016, mae: 14.771016, mean_q: 35.329403\n",
            " 14792/30000: episode: 2467, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2664.833 [503.000, 5723.000],  loss: 37.058079, mae: 15.150323, mean_q: 35.304794\n",
            " 14798/30000: episode: 2468, duration: 0.405s, episode steps:   6, steps per second:  15, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2963.667 [178.000, 4703.000],  loss: 53.884907, mae: 14.650635, mean_q: 35.576519\n",
            " 14804/30000: episode: 2469, duration: 0.224s, episode steps:   6, steps per second:  27, episode reward: 55.000, mean reward:  9.167 [ 0.000, 15.000], mean action: 2055.000 [1530.000, 3712.000],  loss: 35.011936, mae: 15.728337, mean_q: 37.616318\n",
            " 14810/30000: episode: 2470, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2160.000 [12.000, 4203.000],  loss: 34.364414, mae: 14.207973, mean_q: 34.517433\n",
            " 14816/30000: episode: 2471, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3305.167 [2063.000, 5316.000],  loss: 36.694530, mae: 15.354960, mean_q: 36.532032\n",
            " 14822/30000: episode: 2472, duration: 0.283s, episode steps:   6, steps per second:  21, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3394.833 [1531.000, 5654.000],  loss: 37.042507, mae: 14.215424, mean_q: 34.989506\n",
            " 14828/30000: episode: 2473, duration: 0.342s, episode steps:   6, steps per second:  18, episode reward: 40.000, mean reward:  6.667 [ 0.000, 25.000], mean action: 1648.667 [10.000, 2529.000],  loss: 30.285727, mae: 13.920081, mean_q: 33.940800\n",
            " 14834/30000: episode: 2474, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2851.833 [27.000, 4391.000],  loss: 27.961157, mae: 14.778003, mean_q: 35.294086\n",
            " 14840/30000: episode: 2475, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1433.333 [149.000, 4522.000],  loss: 45.837143, mae: 16.232859, mean_q: 38.171131\n",
            " 14846/30000: episode: 2476, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3258.000 [1530.000, 4812.000],  loss: 39.623844, mae: 15.610600, mean_q: 37.751778\n",
            " 14852/30000: episode: 2477, duration: 0.411s, episode steps:   6, steps per second:  15, episode reward:  5.000, mean reward:  0.833 [-5.000, 15.000], mean action: 4091.833 [2448.000, 5622.000],  loss: 51.665066, mae: 15.167027, mean_q: 37.068863\n",
            " 14858/30000: episode: 2478, duration: 0.209s, episode steps:   6, steps per second:  29, episode reward: -5.000, mean reward: -0.833 [-5.000,  5.000], mean action: 4033.833 [242.000, 5742.000],  loss: 58.583637, mae: 14.625222, mean_q: 36.092537\n",
            " 14864/30000: episode: 2479, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3221.167 [534.000, 5676.000],  loss: 51.271931, mae: 15.399844, mean_q: 37.740631\n",
            " 14870/30000: episode: 2480, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2572.667 [889.000, 4438.000],  loss: 45.841518, mae: 15.264145, mean_q: 37.785809\n",
            " 14876/30000: episode: 2481, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1919.167 [534.000, 4819.000],  loss: 50.032074, mae: 13.710200, mean_q: 35.146290\n",
            " 14882/30000: episode: 2482, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3294.333 [2489.000, 5127.000],  loss: 42.886868, mae: 15.716054, mean_q: 38.947964\n",
            " 14888/30000: episode: 2483, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3267.500 [1276.000, 5467.000],  loss: 51.905964, mae: 15.643495, mean_q: 38.196697\n",
            " 14894/30000: episode: 2484, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2365.167 [339.000, 3552.000],  loss: 42.994373, mae: 15.213352, mean_q: 38.051605\n",
            " 14900/30000: episode: 2485, duration: 0.187s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4625.833 [2489.000, 5668.000],  loss: 38.755871, mae: 15.732259, mean_q: 38.583744\n",
            " 14906/30000: episode: 2486, duration: 0.245s, episode steps:   6, steps per second:  24, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 2842.000 [1560.000, 3672.000],  loss: 37.348980, mae: 15.551627, mean_q: 38.690922\n",
            " 14912/30000: episode: 2487, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3940.667 [2489.000, 5579.000],  loss: 49.812145, mae: 14.690261, mean_q: 36.951912\n",
            " 14918/30000: episode: 2488, duration: 0.237s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3107.667 [437.000, 4972.000],  loss: 44.505703, mae: 14.730614, mean_q: 36.372913\n",
            " 14924/30000: episode: 2489, duration: 0.236s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2737.333 [1257.000, 4598.000],  loss: 39.938789, mae: 15.264809, mean_q: 37.732105\n",
            " 14930/30000: episode: 2490, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3111.833 [1536.000, 5395.000],  loss: 41.713352, mae: 15.316770, mean_q: 38.198658\n",
            " 14936/30000: episode: 2491, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2520.667 [534.000, 4677.000],  loss: 23.484663, mae: 15.662726, mean_q: 39.135628\n",
            " 14942/30000: episode: 2492, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3501.667 [2489.000, 4939.000],  loss: 41.332039, mae: 15.174438, mean_q: 37.022366\n",
            " 14948/30000: episode: 2493, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward: 55.000, mean reward:  9.167 [ 0.000, 30.000], mean action: 2667.833 [733.000, 5364.000],  loss: 37.536228, mae: 14.057155, mean_q: 35.082325\n",
            " 14954/30000: episode: 2494, duration: 0.259s, episode steps:   6, steps per second:  23, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 2030.167 [1202.000, 3167.000],  loss: 47.596394, mae: 14.913365, mean_q: 36.606167\n",
            " 14960/30000: episode: 2495, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3342.167 [2489.000, 5062.000],  loss: 40.322678, mae: 14.816028, mean_q: 37.009037\n",
            " 14966/30000: episode: 2496, duration: 0.264s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3212.000 [2489.000, 5447.000],  loss: 28.740774, mae: 14.732444, mean_q: 36.145195\n",
            " 14972/30000: episode: 2497, duration: 0.270s, episode steps:   6, steps per second:  22, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3498.667 [1799.000, 4840.000],  loss: 31.057581, mae: 15.017857, mean_q: 36.518372\n",
            " 14978/30000: episode: 2498, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2984.333 [238.000, 5466.000],  loss: 62.387463, mae: 15.526367, mean_q: 37.026920\n",
            " 14984/30000: episode: 2499, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2505.167 [358.000, 3954.000],  loss: 28.604698, mae: 15.183937, mean_q: 37.476978\n",
            " 14990/30000: episode: 2500, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2028.333 [26.000, 4819.000],  loss: 41.128933, mae: 15.914937, mean_q: 38.306591\n",
            " 14996/30000: episode: 2501, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3190.833 [1124.000, 5395.000],  loss: 36.436020, mae: 15.924603, mean_q: 38.368526\n",
            " 15002/30000: episode: 2502, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3441.667 [3051.000, 5395.000],  loss: 32.247211, mae: 14.044510, mean_q: 35.137356\n",
            " 15008/30000: episode: 2503, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1780.833 [617.000, 4488.000],  loss: 31.551798, mae: 14.344494, mean_q: 35.082787\n",
            " 15014/30000: episode: 2504, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2136.833 [1857.000, 2904.000],  loss: 45.385273, mae: 14.742816, mean_q: 35.963470\n",
            " 15020/30000: episode: 2505, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3503.667 [962.000, 5742.000],  loss: 40.624744, mae: 15.807456, mean_q: 37.991741\n",
            " 15026/30000: episode: 2506, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4314.667 [2950.000, 5447.000],  loss: 40.417393, mae: 15.410869, mean_q: 37.826538\n",
            " 15032/30000: episode: 2507, duration: 0.183s, episode steps:   6, steps per second:  33, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2925.833 [1160.000, 5508.000],  loss: 33.506268, mae: 16.022219, mean_q: 38.934849\n",
            " 15038/30000: episode: 2508, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3005.833 [303.000, 5395.000],  loss: 34.497906, mae: 15.131808, mean_q: 37.368305\n",
            " 15044/30000: episode: 2509, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 30.000, mean reward:  5.000 [-5.000, 35.000], mean action: 5030.667 [4185.000, 5608.000],  loss: 30.145174, mae: 15.172222, mean_q: 36.538052\n",
            " 15050/30000: episode: 2510, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3035.667 [1232.000, 4884.000],  loss: 23.213022, mae: 15.234397, mean_q: 36.408398\n",
            " 15056/30000: episode: 2511, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4358.500 [2448.000, 5545.000],  loss: 61.783100, mae: 15.390160, mean_q: 37.824612\n",
            " 15062/30000: episode: 2512, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3424.667 [14.000, 4884.000],  loss: 26.288956, mae: 14.122355, mean_q: 34.902431\n",
            " 15068/30000: episode: 2513, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2658.000 [602.000, 5395.000],  loss: 31.499628, mae: 13.925675, mean_q: 33.596687\n",
            " 15074/30000: episode: 2514, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3003.000 [2640.000, 4244.000],  loss: 32.202896, mae: 15.547828, mean_q: 36.900585\n",
            " 15080/30000: episode: 2515, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3199.667 [1779.000, 4894.000],  loss: 35.928909, mae: 14.512292, mean_q: 35.082897\n",
            " 15086/30000: episode: 2516, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2758.500 [1479.000, 5360.000],  loss: 85.417419, mae: 15.628133, mean_q: 37.260941\n",
            " 15092/30000: episode: 2517, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2492.000 [425.000, 5736.000],  loss: 37.819153, mae: 15.961571, mean_q: 37.898121\n",
            " 15098/30000: episode: 2518, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -10.000, mean reward: -1.667 [-5.000,  5.000], mean action: 5065.833 [4884.000, 5447.000],  loss: 33.569977, mae: 15.620091, mean_q: 36.426556\n",
            " 15104/30000: episode: 2519, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2640.333 [733.000, 5390.000],  loss: 75.459045, mae: 15.084167, mean_q: 36.006886\n",
            " 15110/30000: episode: 2520, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2638.500 [617.000, 4185.000],  loss: 26.911230, mae: 14.824502, mean_q: 35.911621\n",
            " 15116/30000: episode: 2521, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4225.333 [1059.000, 5654.000],  loss: 45.132385, mae: 15.004285, mean_q: 35.616127\n",
            " 15122/30000: episode: 2522, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2199.167 [742.000, 2497.000],  loss: 49.080051, mae: 14.424999, mean_q: 35.409515\n",
            " 15128/30000: episode: 2523, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2264.000 [461.000, 4220.000],  loss: 32.278912, mae: 14.737226, mean_q: 36.162716\n",
            " 15134/30000: episode: 2524, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2682.167 [425.000, 4783.000],  loss: 51.296070, mae: 15.444152, mean_q: 37.787506\n",
            " 15140/30000: episode: 2525, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3251.167 [2357.000, 4244.000],  loss: 28.623909, mae: 15.038575, mean_q: 36.136242\n",
            " 15146/30000: episode: 2526, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2320.000 [742.000, 3475.000],  loss: 30.635880, mae: 15.821362, mean_q: 37.264671\n",
            " 15152/30000: episode: 2527, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2853.000 [733.000, 5742.000],  loss: 39.076427, mae: 14.525627, mean_q: 35.565716\n",
            " 15158/30000: episode: 2528, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2645.000 [945.000, 4244.000],  loss: 47.681900, mae: 14.023361, mean_q: 34.460316\n",
            " 15164/30000: episode: 2529, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2647.000 [303.000, 4765.000],  loss: 59.153622, mae: 15.149643, mean_q: 36.262115\n",
            " 15170/30000: episode: 2530, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2741.333 [678.000, 5561.000],  loss: 36.093658, mae: 15.191032, mean_q: 36.780533\n",
            " 15176/30000: episode: 2531, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2871.667 [1296.000, 5146.000],  loss: 19.970770, mae: 14.769591, mean_q: 35.808170\n",
            " 15182/30000: episode: 2532, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [-5.000, 20.000], mean action: 3999.167 [1530.000, 4819.000],  loss: 37.947739, mae: 15.948734, mean_q: 38.081039\n",
            " 15188/30000: episode: 2533, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3225.333 [733.000, 5654.000],  loss: 34.061214, mae: 14.563770, mean_q: 34.852795\n",
            " 15194/30000: episode: 2534, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3241.167 [1530.000, 5283.000],  loss: 35.016068, mae: 14.468646, mean_q: 35.287853\n",
            " 15200/30000: episode: 2535, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4185.333 [1596.000, 5736.000],  loss: 37.791359, mae: 14.305931, mean_q: 34.942463\n",
            " 15206/30000: episode: 2536, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3780.333 [1575.000, 5395.000],  loss: 28.377470, mae: 16.145998, mean_q: 38.521679\n",
            " 15212/30000: episode: 2537, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1987.333 [140.000, 3932.000],  loss: 49.856968, mae: 15.461212, mean_q: 37.702023\n",
            " 15218/30000: episode: 2538, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2381.167 [518.000, 4508.000],  loss: 50.837677, mae: 15.395995, mean_q: 36.439396\n",
            " 15224/30000: episode: 2539, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3404.500 [742.000, 5395.000],  loss: 26.528198, mae: 14.130998, mean_q: 34.696583\n",
            " 15230/30000: episode: 2540, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2501.833 [742.000, 5243.000],  loss: 62.403355, mae: 14.555218, mean_q: 35.548141\n",
            " 15236/30000: episode: 2541, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 2318.333 [770.000, 4486.000],  loss: 46.164829, mae: 14.865121, mean_q: 36.405552\n",
            " 15242/30000: episode: 2542, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3491.833 [1029.000, 5129.000],  loss: 41.991238, mae: 15.972646, mean_q: 39.005249\n",
            " 15248/30000: episode: 2543, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3137.667 [1660.000, 4940.000],  loss: 46.882111, mae: 15.248784, mean_q: 36.183517\n",
            " 15254/30000: episode: 2544, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3274.667 [1554.000, 5736.000],  loss: 39.996731, mae: 14.410863, mean_q: 34.923668\n",
            " 15260/30000: episode: 2545, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3733.167 [419.000, 4412.000],  loss: 40.131310, mae: 14.738856, mean_q: 35.491955\n",
            " 15266/30000: episode: 2546, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1457.000 [238.000, 2201.000],  loss: 22.900816, mae: 14.450199, mean_q: 34.428730\n",
            " 15272/30000: episode: 2547, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2834.833 [547.000, 4824.000],  loss: 33.211620, mae: 14.424492, mean_q: 34.388287\n",
            " 15278/30000: episode: 2548, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 3171.000 [1530.000, 5433.000],  loss: 38.438488, mae: 14.841897, mean_q: 36.506458\n",
            " 15284/30000: episode: 2549, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4239.667 [2640.000, 5727.000],  loss: 42.976940, mae: 15.754359, mean_q: 36.875935\n",
            " 15290/30000: episode: 2550, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2204.000 [1554.000, 3922.000],  loss: 42.391338, mae: 14.545230, mean_q: 35.511993\n",
            " 15296/30000: episode: 2551, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 1306.000 [27.000, 2435.000],  loss: 34.571060, mae: 14.643758, mean_q: 35.779591\n",
            " 15302/30000: episode: 2552, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2625.000 [309.000, 4164.000],  loss: 43.730499, mae: 13.891459, mean_q: 33.475136\n",
            " 15308/30000: episode: 2553, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3400.333 [1029.000, 5715.000],  loss: 51.331760, mae: 15.130391, mean_q: 35.327892\n",
            " 15314/30000: episode: 2554, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3169.500 [1260.000, 4152.000],  loss: 32.572559, mae: 15.855313, mean_q: 37.342407\n",
            " 15320/30000: episode: 2555, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1421.000 [711.000, 3419.000],  loss: 35.583626, mae: 14.338787, mean_q: 34.670158\n",
            " 15326/30000: episode: 2556, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1892.500 [452.000, 4275.000],  loss: 27.392843, mae: 16.166801, mean_q: 37.560963\n",
            " 15332/30000: episode: 2557, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 30.000, mean reward:  5.000 [ 0.000, 30.000], mean action: 2934.333 [1109.000, 4717.000],  loss: 34.183151, mae: 15.978547, mean_q: 37.986275\n",
            " 15338/30000: episode: 2558, duration: 0.210s, episode steps:   6, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3762.667 [1540.000, 5386.000],  loss: 50.750137, mae: 15.438722, mean_q: 36.416145\n",
            " 15344/30000: episode: 2559, duration: 0.266s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3019.000 [663.000, 5249.000],  loss: 71.757973, mae: 14.420856, mean_q: 35.000614\n",
            " 15350/30000: episode: 2560, duration: 0.264s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3395.167 [2134.000, 5750.000],  loss: 59.444790, mae: 14.171994, mean_q: 33.606342\n",
            " 15356/30000: episode: 2561, duration: 0.257s, episode steps:   6, steps per second:  23, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2254.167 [1097.000, 3494.000],  loss: 46.825871, mae: 14.120820, mean_q: 35.135624\n",
            " 15362/30000: episode: 2562, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3306.167 [317.000, 5118.000],  loss: 44.676655, mae: 14.696391, mean_q: 35.870617\n",
            " 15368/30000: episode: 2563, duration: 0.259s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3775.000 [2100.000, 5722.000],  loss: 36.706570, mae: 15.328621, mean_q: 37.537476\n",
            " 15374/30000: episode: 2564, duration: 0.241s, episode steps:   6, steps per second:  25, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2953.667 [2277.000, 3089.000],  loss: 28.183310, mae: 14.394973, mean_q: 35.255543\n",
            " 15380/30000: episode: 2565, duration: 0.257s, episode steps:   6, steps per second:  23, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 1372.333 [1029.000, 3089.000],  loss: 46.404850, mae: 15.319760, mean_q: 37.866135\n",
            " 15386/30000: episode: 2566, duration: 0.272s, episode steps:   6, steps per second:  22, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2638.667 [238.000, 5654.000],  loss: 26.397470, mae: 13.863397, mean_q: 34.093952\n",
            " 15392/30000: episode: 2567, duration: 0.275s, episode steps:   6, steps per second:  22, episode reward: 135.000, mean reward: 22.500 [ 0.000, 35.000], mean action: 3617.333 [1596.000, 4464.000],  loss: 37.318172, mae: 14.126719, mean_q: 35.650814\n",
            " 15398/30000: episode: 2568, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 3089.000 [3089.000, 3089.000],  loss: 37.790775, mae: 15.717369, mean_q: 38.164654\n",
            " 15404/30000: episode: 2569, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 3089.000 [3089.000, 3089.000],  loss: 29.091749, mae: 14.000876, mean_q: 34.203823\n",
            " 15410/30000: episode: 2570, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2099.000 [1901.000, 3089.000],  loss: 50.898388, mae: 13.375168, mean_q: 33.261288\n",
            " 15416/30000: episode: 2571, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3188.667 [990.000, 5129.000],  loss: 44.120289, mae: 14.762630, mean_q: 36.493053\n",
            " 15422/30000: episode: 2572, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2353.000 [238.000, 4246.000],  loss: 35.313202, mae: 14.427052, mean_q: 35.007046\n",
            " 15428/30000: episode: 2573, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1606.667 [183.000, 2217.000],  loss: 37.752903, mae: 15.306873, mean_q: 36.605289\n",
            " 15434/30000: episode: 2574, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3275.833 [2420.000, 3447.000],  loss: 37.170460, mae: 14.170574, mean_q: 34.629295\n",
            " 15440/30000: episode: 2575, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3486.333 [2146.000, 4464.000],  loss: 30.395910, mae: 15.241600, mean_q: 36.664181\n",
            " 15446/30000: episode: 2576, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3531.833 [819.000, 5602.000],  loss: 22.473730, mae: 15.158520, mean_q: 35.754780\n",
            " 15452/30000: episode: 2577, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3019.500 [452.000, 5413.000],  loss: 37.026752, mae: 14.590179, mean_q: 35.630829\n",
            " 15458/30000: episode: 2578, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: 30.000, mean reward:  5.000 [ 0.000, 25.000], mean action: 2517.167 [663.000, 3512.000],  loss: 53.272625, mae: 14.736157, mean_q: 35.574032\n",
            " 15464/30000: episode: 2579, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3057.333 [590.000, 5722.000],  loss: 45.292416, mae: 14.023374, mean_q: 33.939526\n",
            " 15470/30000: episode: 2580, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3683.333 [1102.000, 5218.000],  loss: 57.247387, mae: 14.024582, mean_q: 33.996635\n",
            " 15476/30000: episode: 2581, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2016.833 [14.000, 4824.000],  loss: 42.738346, mae: 14.510533, mean_q: 35.202961\n",
            " 15482/30000: episode: 2582, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3736.000 [3512.000, 4164.000],  loss: 40.005806, mae: 15.165985, mean_q: 36.420506\n",
            " 15488/30000: episode: 2583, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2097.333 [363.000, 4164.000],  loss: 49.351395, mae: 15.730732, mean_q: 37.182011\n",
            " 15494/30000: episode: 2584, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3106.667 [1076.000, 5582.000],  loss: 48.496613, mae: 14.488960, mean_q: 35.118160\n",
            " 15500/30000: episode: 2585, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3949.000 [2994.000, 4750.000],  loss: 43.781509, mae: 14.527400, mean_q: 34.582485\n",
            " 15506/30000: episode: 2586, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3713.833 [1383.000, 5654.000],  loss: 31.221367, mae: 14.669827, mean_q: 35.703274\n",
            " 15512/30000: episode: 2587, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3213.500 [2170.000, 4164.000],  loss: 40.958935, mae: 14.672909, mean_q: 35.185940\n",
            " 15518/30000: episode: 2588, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3210.500 [1372.000, 5654.000],  loss: 27.773996, mae: 15.123639, mean_q: 35.906796\n",
            " 15524/30000: episode: 2589, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2539.167 [770.000, 5118.000],  loss: 57.761944, mae: 14.531737, mean_q: 35.299103\n",
            " 15530/30000: episode: 2590, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3024.833 [1091.000, 5209.000],  loss: 49.477093, mae: 13.913994, mean_q: 34.278393\n",
            " 15536/30000: episode: 2591, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 2751.667 [245.000, 5640.000],  loss: 30.124601, mae: 15.583470, mean_q: 36.307774\n",
            " 15542/30000: episode: 2592, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3405.333 [1109.000, 5390.000],  loss: 40.014301, mae: 15.176976, mean_q: 36.238651\n",
            " 15548/30000: episode: 2593, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3094.667 [1224.000, 5259.000],  loss: 45.602219, mae: 15.763481, mean_q: 37.645580\n",
            " 15554/30000: episode: 2594, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3169.000 [1029.000, 4573.000],  loss: 34.984329, mae: 15.203041, mean_q: 35.946201\n",
            " 15560/30000: episode: 2595, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3098.167 [770.000, 5658.000],  loss: 29.283356, mae: 14.079175, mean_q: 34.049274\n",
            " 15566/30000: episode: 2596, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -10.000, mean reward: -1.667 [-5.000, 10.000], mean action: 2048.667 [2003.000, 2277.000],  loss: 42.185123, mae: 13.877366, mean_q: 33.025845\n",
            " 15572/30000: episode: 2597, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 3111.167 [1257.000, 4333.000],  loss: 30.425842, mae: 13.798360, mean_q: 34.160656\n",
            " 15578/30000: episode: 2598, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3034.333 [303.000, 4595.000],  loss: 43.886585, mae: 15.103620, mean_q: 35.714237\n",
            " 15584/30000: episode: 2599, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3662.833 [711.000, 4989.000],  loss: 37.433823, mae: 15.240186, mean_q: 35.542633\n",
            " 15590/30000: episode: 2600, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 35.000, mean reward:  5.833 [ 0.000, 30.000], mean action: 3964.167 [309.000, 5447.000],  loss: 41.374569, mae: 14.908890, mean_q: 35.604034\n",
            " 15596/30000: episode: 2601, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2603.667 [708.000, 4454.000],  loss: 43.624104, mae: 15.554657, mean_q: 36.623951\n",
            " 15602/30000: episode: 2602, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3507.833 [966.000, 5747.000],  loss: 46.061127, mae: 16.169195, mean_q: 38.345184\n",
            " 15608/30000: episode: 2603, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 4195.667 [2652.000, 5456.000],  loss: 33.030598, mae: 15.202306, mean_q: 36.604946\n",
            " 15614/30000: episode: 2604, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1297.000 [1032.000, 1833.000],  loss: 53.808674, mae: 14.402303, mean_q: 35.763691\n",
            " 15620/30000: episode: 2605, duration: 0.164s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3304.833 [1409.000, 4979.000],  loss: 15.880761, mae: 13.978497, mean_q: 34.337948\n",
            " 15626/30000: episode: 2606, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2564.667 [411.000, 5593.000],  loss: 36.593723, mae: 14.387413, mean_q: 35.158115\n",
            " 15632/30000: episode: 2607, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 4330.833 [1530.000, 5742.000],  loss: 31.138224, mae: 15.177380, mean_q: 36.684654\n",
            " 15638/30000: episode: 2608, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3042.667 [1520.000, 4945.000],  loss: 78.506081, mae: 15.516991, mean_q: 37.386723\n",
            " 15644/30000: episode: 2609, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2419.167 [72.000, 4588.000],  loss: 39.918270, mae: 15.873248, mean_q: 37.033863\n",
            " 15650/30000: episode: 2610, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3163.833 [453.000, 4894.000],  loss: 45.903931, mae: 14.234513, mean_q: 33.952736\n",
            " 15656/30000: episode: 2611, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2830.667 [1109.000, 5740.000],  loss: 43.799770, mae: 16.181026, mean_q: 38.373493\n",
            " 15662/30000: episode: 2612, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2209.667 [411.000, 4422.000],  loss: 40.378239, mae: 13.681922, mean_q: 33.592999\n",
            " 15668/30000: episode: 2613, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1830.833 [72.000, 4123.000],  loss: 50.475552, mae: 15.594953, mean_q: 38.063274\n",
            " 15674/30000: episode: 2614, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3290.833 [1325.000, 5523.000],  loss: 27.747740, mae: 14.694161, mean_q: 35.415386\n",
            " 15680/30000: episode: 2615, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3060.833 [678.000, 5348.000],  loss: 34.088680, mae: 15.409680, mean_q: 36.868961\n",
            " 15686/30000: episode: 2616, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1635.000 [72.000, 4868.000],  loss: 34.707088, mae: 14.943710, mean_q: 36.080063\n",
            " 15692/30000: episode: 2617, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2673.000 [1225.000, 4761.000],  loss: 37.877651, mae: 13.733581, mean_q: 33.954700\n",
            " 15698/30000: episode: 2618, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 2206.500 [72.000, 5608.000],  loss: 31.555044, mae: 16.094646, mean_q: 38.179726\n",
            " 15704/30000: episode: 2619, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3181.667 [1509.000, 5062.000],  loss: 40.202938, mae: 13.507623, mean_q: 33.423199\n",
            " 15710/30000: episode: 2620, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 40.000, mean reward:  6.667 [ 0.000, 20.000], mean action: 2876.167 [1338.000, 4336.000],  loss: 47.308380, mae: 15.567818, mean_q: 37.161865\n",
            " 15716/30000: episode: 2621, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2327.000 [1225.000, 4109.000],  loss: 42.035950, mae: 15.641514, mean_q: 37.592350\n",
            " 15722/30000: episode: 2622, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2276.167 [883.000, 4826.000],  loss: 45.565258, mae: 15.686795, mean_q: 36.533325\n",
            " 15728/30000: episode: 2623, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4036.667 [1901.000, 5703.000],  loss: 29.249151, mae: 14.611023, mean_q: 35.058121\n",
            " 15734/30000: episode: 2624, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 3081.167 [1530.000, 5742.000],  loss: 29.041998, mae: 13.566550, mean_q: 34.101273\n",
            " 15740/30000: episode: 2625, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 2353.333 [158.000, 4819.000],  loss: 43.503437, mae: 15.142929, mean_q: 37.293858\n",
            " 15746/30000: episode: 2626, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 135.000, mean reward: 22.500 [ 0.000, 35.000], mean action: 3617.333 [1596.000, 4464.000],  loss: 39.419598, mae: 15.245205, mean_q: 37.189404\n",
            " 15752/30000: episode: 2627, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1710.667 [72.000, 5282.000],  loss: 54.770214, mae: 14.840698, mean_q: 36.716839\n",
            " 15758/30000: episode: 2628, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2656.333 [132.000, 5129.000],  loss: 32.998798, mae: 14.742417, mean_q: 35.866764\n",
            " 15764/30000: episode: 2629, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 75.000, mean reward: 12.500 [ 0.000, 35.000], mean action: 2571.000 [562.000, 4982.000],  loss: 35.621292, mae: 13.976928, mean_q: 34.078674\n",
            " 15770/30000: episode: 2630, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward: 25.000, mean reward:  4.167 [ 0.000, 10.000], mean action: 2643.500 [587.000, 3908.000],  loss: 45.846119, mae: 14.432655, mean_q: 35.674995\n",
            " 15776/30000: episode: 2631, duration: 0.222s, episode steps:   6, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2369.500 [150.000, 5011.000],  loss: 31.551294, mae: 14.354024, mean_q: 35.734875\n",
            " 15782/30000: episode: 2632, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3746.167 [3072.000, 5488.000],  loss: 46.271450, mae: 15.157296, mean_q: 37.471008\n",
            " 15788/30000: episode: 2633, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward: 40.000, mean reward:  6.667 [ 0.000, 40.000], mean action: 2461.833 [150.000, 4840.000],  loss: 36.694946, mae: 14.405228, mean_q: 35.566483\n",
            " 15794/30000: episode: 2634, duration: 0.240s, episode steps:   6, steps per second:  25, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3888.000 [550.000, 5466.000],  loss: 23.204958, mae: 14.424518, mean_q: 35.379448\n",
            " 15800/30000: episode: 2635, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 2510.833 [150.000, 4681.000],  loss: 37.491108, mae: 14.830884, mean_q: 35.729141\n",
            " 15806/30000: episode: 2636, duration: 0.257s, episode steps:   6, steps per second:  23, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3496.333 [758.000, 5447.000],  loss: 38.427525, mae: 13.723801, mean_q: 33.086510\n",
            " 15812/30000: episode: 2637, duration: 0.241s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2897.500 [238.000, 4939.000],  loss: 58.248188, mae: 14.333293, mean_q: 35.175053\n",
            " 15818/30000: episode: 2638, duration: 0.239s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3305.833 [150.000, 5737.000],  loss: 36.915375, mae: 15.537094, mean_q: 37.243534\n",
            " 15824/30000: episode: 2639, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3749.333 [150.000, 4894.000],  loss: 47.675110, mae: 15.726159, mean_q: 37.060459\n",
            " 15830/30000: episode: 2640, duration: 0.264s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4087.500 [3230.000, 5348.000],  loss: 34.850254, mae: 14.550869, mean_q: 35.496803\n",
            " 15836/30000: episode: 2641, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward: 45.000, mean reward:  7.500 [ 0.000, 25.000], mean action: 4985.833 [3669.000, 5529.000],  loss: 48.692814, mae: 15.365575, mean_q: 36.317200\n",
            " 15842/30000: episode: 2642, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3133.667 [150.000, 5212.000],  loss: 43.248474, mae: 15.142537, mean_q: 36.471500\n",
            " 15848/30000: episode: 2643, duration: 0.207s, episode steps:   6, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3216.500 [1218.000, 5447.000],  loss: 45.466675, mae: 15.817946, mean_q: 39.077473\n",
            " 15854/30000: episode: 2644, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3683.000 [150.000, 5687.000],  loss: 42.605640, mae: 15.687304, mean_q: 38.200382\n",
            " 15860/30000: episode: 2645, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 3434.833 [150.000, 5348.000],  loss: 42.454296, mae: 15.559651, mean_q: 37.763142\n",
            " 15866/30000: episode: 2646, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2193.667 [150.000, 5447.000],  loss: 64.048363, mae: 15.481355, mean_q: 38.020947\n",
            " 15872/30000: episode: 2647, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3572.333 [3089.000, 3669.000],  loss: 56.830185, mae: 13.561368, mean_q: 33.786098\n",
            " 15878/30000: episode: 2648, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1743.500 [493.000, 4840.000],  loss: 30.851151, mae: 13.411564, mean_q: 32.834629\n",
            " 15884/30000: episode: 2649, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3215.500 [150.000, 5561.000],  loss: 33.279213, mae: 14.358463, mean_q: 34.839897\n",
            " 15890/30000: episode: 2650, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 2018.833 [150.000, 4595.000],  loss: 39.881008, mae: 13.850038, mean_q: 33.909374\n",
            " 15896/30000: episode: 2651, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1097.333 [15.000, 2444.000],  loss: 41.088318, mae: 14.948227, mean_q: 36.557236\n",
            " 15902/30000: episode: 2652, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2520.667 [150.000, 4185.000],  loss: 43.197235, mae: 14.227551, mean_q: 35.317932\n",
            " 15908/30000: episode: 2653, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2667.500 [846.000, 3797.000],  loss: 23.583925, mae: 14.591225, mean_q: 34.917934\n",
            " 15914/30000: episode: 2654, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2517.167 [1504.000, 3921.000],  loss: 43.053608, mae: 14.855157, mean_q: 35.833652\n",
            " 15920/30000: episode: 2655, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2400.667 [150.000, 4911.000],  loss: 29.766912, mae: 14.629585, mean_q: 35.599880\n",
            " 15926/30000: episode: 2656, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2316.667 [150.000, 5656.000],  loss: 27.277121, mae: 15.346145, mean_q: 36.740616\n",
            " 15932/30000: episode: 2657, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2053.833 [10.000, 3961.000],  loss: 35.518555, mae: 15.497867, mean_q: 37.181660\n",
            " 15938/30000: episode: 2658, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2702.167 [150.000, 5204.000],  loss: 47.885593, mae: 15.107390, mean_q: 36.565651\n",
            " 15944/30000: episode: 2659, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [-5.000, 15.000], mean action: 2765.000 [961.000, 3921.000],  loss: 40.586666, mae: 15.897054, mean_q: 37.961994\n",
            " 15950/30000: episode: 2660, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 20.000, mean reward:  3.333 [-5.000, 15.000], mean action: 3334.000 [1596.000, 5193.000],  loss: 40.755913, mae: 14.284049, mean_q: 34.593884\n",
            " 15956/30000: episode: 2661, duration: 0.148s, episode steps:   6, steps per second:  40, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 1788.500 [294.000, 3089.000],  loss: 28.722038, mae: 13.974640, mean_q: 33.993755\n",
            " 15962/30000: episode: 2662, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3256.500 [982.000, 5703.000],  loss: 26.871466, mae: 14.767968, mean_q: 36.398094\n",
            " 15968/30000: episode: 2663, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 2906.000 [1081.000, 5447.000],  loss: 35.069065, mae: 14.483136, mean_q: 35.419575\n",
            " 15974/30000: episode: 2664, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2900.167 [1044.000, 5433.000],  loss: 33.636112, mae: 15.150897, mean_q: 35.804699\n",
            " 15980/30000: episode: 2665, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward: 30.000, mean reward:  5.000 [ 0.000, 30.000], mean action: 2670.167 [1102.000, 4454.000],  loss: 43.752213, mae: 14.997815, mean_q: 35.913189\n",
            " 15986/30000: episode: 2666, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2418.833 [1206.000, 4381.000],  loss: 55.468922, mae: 13.963985, mean_q: 33.548527\n",
            " 15992/30000: episode: 2667, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 1948.333 [1029.000, 3036.000],  loss: 30.667456, mae: 14.678856, mean_q: 35.260891\n",
            " 15998/30000: episode: 2668, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2527.000 [542.000, 5743.000],  loss: 39.089184, mae: 14.900368, mean_q: 36.236328\n",
            " 16004/30000: episode: 2669, duration: 0.219s, episode steps:   6, steps per second:  27, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 2840.667 [1202.000, 5433.000],  loss: 41.179775, mae: 14.649693, mean_q: 35.639370\n",
            " 16010/30000: episode: 2670, duration: 0.371s, episode steps:   6, steps per second:  16, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 3058.667 [1202.000, 5395.000],  loss: 40.136303, mae: 16.159147, mean_q: 38.314800\n",
            " 16016/30000: episode: 2671, duration: 0.204s, episode steps:   6, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1541.500 [163.000, 3763.000],  loss: 22.238066, mae: 14.852791, mean_q: 36.101517\n",
            " 16022/30000: episode: 2672, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2849.000 [587.000, 5623.000],  loss: 21.637247, mae: 15.553103, mean_q: 37.722309\n",
            " 16028/30000: episode: 2673, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 4035.833 [770.000, 5652.000],  loss: 42.824215, mae: 13.773726, mean_q: 34.171719\n",
            " 16034/30000: episode: 2674, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1994.500 [150.000, 3922.000],  loss: 52.501934, mae: 14.410190, mean_q: 34.472286\n",
            " 16040/30000: episode: 2675, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3473.500 [1076.000, 5395.000],  loss: 31.278784, mae: 15.770946, mean_q: 37.856304\n",
            " 16046/30000: episode: 2676, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2993.167 [721.000, 3921.000],  loss: 37.767750, mae: 16.101562, mean_q: 37.702301\n",
            " 16052/30000: episode: 2677, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3602.500 [895.000, 5622.000],  loss: 30.721869, mae: 13.514735, mean_q: 33.472614\n",
            " 16058/30000: episode: 2678, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2638.833 [590.000, 4677.000],  loss: 43.056919, mae: 14.391051, mean_q: 34.946239\n",
            " 16064/30000: episode: 2679, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2799.000 [34.000, 5680.000],  loss: 36.166256, mae: 14.715873, mean_q: 34.627872\n",
            " 16070/30000: episode: 2680, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2605.500 [238.000, 5109.000],  loss: 28.067993, mae: 15.047414, mean_q: 35.256870\n",
            " 16076/30000: episode: 2681, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2910.667 [1251.000, 4750.000],  loss: 30.270584, mae: 15.116649, mean_q: 35.632397\n",
            " 16082/30000: episode: 2682, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2606.667 [1779.000, 4911.000],  loss: 30.933016, mae: 14.432662, mean_q: 35.608952\n",
            " 16088/30000: episode: 2683, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 1558.833 [27.000, 5066.000],  loss: 30.751310, mae: 15.295974, mean_q: 36.164623\n",
            " 16094/30000: episode: 2684, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3383.333 [1779.000, 5070.000],  loss: 25.940191, mae: 13.251504, mean_q: 32.722748\n",
            " 16100/30000: episode: 2685, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2356.167 [608.000, 4619.000],  loss: 55.107685, mae: 14.796216, mean_q: 35.955799\n",
            " 16106/30000: episode: 2686, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [-5.000, 15.000], mean action: 2337.500 [711.000, 5269.000],  loss: 35.071762, mae: 12.911290, mean_q: 32.086121\n",
            " 16112/30000: episode: 2687, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3234.333 [769.000, 5602.000],  loss: 51.740040, mae: 14.922120, mean_q: 36.403347\n",
            " 16118/30000: episode: 2688, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1537.167 [138.000, 3274.000],  loss: 41.927845, mae: 14.535413, mean_q: 35.410885\n",
            " 16124/30000: episode: 2689, duration: 0.164s, episode steps:   6, steps per second:  36, episode reward: 150.000, mean reward: 25.000 [ 0.000, 40.000], mean action: 3225.333 [238.000, 4963.000],  loss: 33.769501, mae: 15.014520, mean_q: 36.789860\n",
            " 16130/30000: episode: 2690, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2353.167 [711.000, 5722.000],  loss: 40.079540, mae: 15.596008, mean_q: 38.002254\n",
            " 16136/30000: episode: 2691, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1952.833 [590.000, 3069.000],  loss: 39.103184, mae: 15.326709, mean_q: 37.039452\n",
            " 16142/30000: episode: 2692, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3685.000 [1042.000, 5651.000],  loss: 36.775360, mae: 14.194225, mean_q: 34.426819\n",
            " 16148/30000: episode: 2693, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4063.667 [1985.000, 5717.000],  loss: 38.560390, mae: 13.231660, mean_q: 32.893764\n",
            " 16154/30000: episode: 2694, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2339.167 [790.000, 3951.000],  loss: 29.329329, mae: 13.653674, mean_q: 33.961056\n",
            " 16160/30000: episode: 2695, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2480.833 [587.000, 4795.000],  loss: 37.492855, mae: 14.508995, mean_q: 35.307640\n",
            " 16166/30000: episode: 2696, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2259.333 [245.000, 5395.000],  loss: 40.762268, mae: 14.306315, mean_q: 35.453842\n",
            " 16172/30000: episode: 2697, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1865.500 [316.000, 4244.000],  loss: 50.031635, mae: 14.584023, mean_q: 35.366009\n",
            " 16178/30000: episode: 2698, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2091.833 [758.000, 4677.000],  loss: 40.360004, mae: 14.660316, mean_q: 35.318577\n",
            " 16184/30000: episode: 2699, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1681.500 [770.000, 3077.000],  loss: 60.351273, mae: 14.529907, mean_q: 36.387508\n",
            " 16190/30000: episode: 2700, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [-5.000, 10.000], mean action: 3389.333 [946.000, 5488.000],  loss: 30.192749, mae: 14.340262, mean_q: 35.738483\n",
            " 16196/30000: episode: 2701, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1691.833 [507.000, 3922.000],  loss: 49.546680, mae: 14.431946, mean_q: 35.458096\n",
            " 16202/30000: episode: 2702, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4099.000 [1779.000, 5386.000],  loss: 20.701611, mae: 15.493858, mean_q: 37.048523\n",
            " 16208/30000: episode: 2703, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3765.167 [1596.000, 5433.000],  loss: 26.125967, mae: 14.391288, mean_q: 35.865322\n",
            " 16214/30000: episode: 2704, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2896.167 [150.000, 5570.000],  loss: 52.172024, mae: 14.575612, mean_q: 34.660587\n",
            " 16220/30000: episode: 2705, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 3160.000 [1596.000, 5447.000],  loss: 51.002151, mae: 14.282596, mean_q: 34.900059\n",
            " 16226/30000: episode: 2706, duration: 0.241s, episode steps:   6, steps per second:  25, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 2885.333 [471.000, 4474.000],  loss: 33.018871, mae: 14.934806, mean_q: 35.599136\n",
            " 16232/30000: episode: 2707, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2800.000 [467.000, 5433.000],  loss: 28.438635, mae: 14.514791, mean_q: 34.650307\n",
            " 16238/30000: episode: 2708, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3437.333 [1183.000, 4439.000],  loss: 28.321098, mae: 14.748099, mean_q: 36.179806\n",
            " 16244/30000: episode: 2709, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1693.167 [842.000, 3041.000],  loss: 43.921612, mae: 15.297030, mean_q: 36.352345\n",
            " 16250/30000: episode: 2710, duration: 0.230s, episode steps:   6, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2301.000 [994.000, 4802.000],  loss: 39.500992, mae: 16.372787, mean_q: 39.199123\n",
            " 16256/30000: episode: 2711, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1858.500 [238.000, 5188.000],  loss: 46.599628, mae: 16.178236, mean_q: 38.159145\n",
            " 16262/30000: episode: 2712, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3320.000 [880.000, 5717.000],  loss: 59.991089, mae: 14.974564, mean_q: 35.666611\n",
            " 16268/30000: episode: 2713, duration: 0.273s, episode steps:   6, steps per second:  22, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 1257.500 [163.000, 3589.000],  loss: 35.470798, mae: 14.697253, mean_q: 35.145153\n",
            " 16274/30000: episode: 2714, duration: 0.231s, episode steps:   6, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2156.500 [459.000, 5390.000],  loss: 51.952015, mae: 14.447362, mean_q: 34.815556\n",
            " 16280/30000: episode: 2715, duration: 0.148s, episode steps:   6, steps per second:  40, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2218.333 [425.000, 4868.000],  loss: 32.740120, mae: 14.831612, mean_q: 35.652370\n",
            " 16286/30000: episode: 2716, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3365.000 [271.000, 5443.000],  loss: 26.962698, mae: 14.594585, mean_q: 35.346012\n",
            " 16292/30000: episode: 2717, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3642.833 [1617.000, 5717.000],  loss: 43.931580, mae: 14.227764, mean_q: 34.805325\n",
            " 16298/30000: episode: 2718, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2901.000 [1202.000, 4415.000],  loss: 54.837322, mae: 15.773677, mean_q: 38.091953\n",
            " 16304/30000: episode: 2719, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3625.833 [1639.000, 5680.000],  loss: 29.019730, mae: 14.441040, mean_q: 35.756710\n",
            " 16310/30000: episode: 2720, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3737.000 [1554.000, 5716.000],  loss: 30.715393, mae: 15.214916, mean_q: 36.238720\n",
            " 16316/30000: episode: 2721, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3913.167 [608.000, 5612.000],  loss: 39.357334, mae: 14.913342, mean_q: 35.688614\n",
            " 16322/30000: episode: 2722, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2417.833 [1237.000, 5736.000],  loss: 30.658157, mae: 13.500272, mean_q: 33.338146\n",
            " 16328/30000: episode: 2723, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2056.167 [17.000, 4288.000],  loss: 70.346199, mae: 15.655169, mean_q: 36.953091\n",
            " 16334/30000: episode: 2724, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2592.500 [1231.000, 4185.000],  loss: 33.120342, mae: 13.871844, mean_q: 34.434673\n",
            " 16340/30000: episode: 2725, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward: 30.000, mean reward:  5.000 [ 0.000, 30.000], mean action: 3635.167 [794.000, 5434.000],  loss: 47.339397, mae: 15.485157, mean_q: 36.843586\n",
            " 16346/30000: episode: 2726, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3741.333 [1554.000, 5395.000],  loss: 45.314606, mae: 14.973382, mean_q: 36.132153\n",
            " 16352/30000: episode: 2727, duration: 0.147s, episode steps:   6, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2440.833 [27.000, 5736.000],  loss: 33.839165, mae: 15.027343, mean_q: 35.956432\n",
            " 16358/30000: episode: 2728, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1635.167 [545.000, 3569.000],  loss: 28.461603, mae: 13.492898, mean_q: 32.827873\n",
            " 16364/30000: episode: 2729, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3107.667 [107.000, 5742.000],  loss: 38.873871, mae: 14.376563, mean_q: 34.890305\n",
            " 16370/30000: episode: 2730, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2291.000 [605.000, 3525.000],  loss: 39.165863, mae: 14.531047, mean_q: 35.340862\n",
            " 16376/30000: episode: 2731, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2466.167 [618.000, 5736.000],  loss: 52.326721, mae: 15.068431, mean_q: 36.204113\n",
            " 16382/30000: episode: 2732, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2635.000 [689.000, 4412.000],  loss: 31.047684, mae: 13.624191, mean_q: 33.292065\n",
            " 16388/30000: episode: 2733, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3541.833 [1521.000, 5717.000],  loss: 43.569687, mae: 14.414796, mean_q: 35.647205\n",
            " 16394/30000: episode: 2734, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3150.833 [52.000, 5736.000],  loss: 43.529690, mae: 14.263222, mean_q: 34.897827\n",
            " 16400/30000: episode: 2735, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 85.000, mean reward: 14.167 [ 0.000, 35.000], mean action: 3778.333 [1596.000, 5640.000],  loss: 39.717693, mae: 15.493561, mean_q: 37.125340\n",
            " 16406/30000: episode: 2736, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3407.500 [590.000, 5629.000],  loss: 31.776079, mae: 12.835900, mean_q: 32.350170\n",
            " 16412/30000: episode: 2737, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2235.833 [961.000, 3274.000],  loss: 56.665005, mae: 15.707329, mean_q: 37.319229\n",
            " 16418/30000: episode: 2738, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3660.000 [2245.000, 5736.000],  loss: 60.020813, mae: 14.735893, mean_q: 35.598858\n",
            " 16424/30000: episode: 2739, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2787.167 [26.000, 5129.000],  loss: 27.954123, mae: 15.273517, mean_q: 37.306698\n",
            " 16430/30000: episode: 2740, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3138.167 [712.000, 3921.000],  loss: 52.285492, mae: 14.178108, mean_q: 34.010960\n",
            " 16436/30000: episode: 2741, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3600.667 [852.000, 5620.000],  loss: 35.292614, mae: 16.109468, mean_q: 37.915958\n",
            " 16442/30000: episode: 2742, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2345.500 [770.000, 5717.000],  loss: 32.813972, mae: 14.022752, mean_q: 34.429905\n",
            " 16448/30000: episode: 2743, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3718.000 [1087.000, 5736.000],  loss: 38.661518, mae: 15.514725, mean_q: 37.358292\n",
            " 16454/30000: episode: 2744, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3838.167 [2188.000, 5736.000],  loss: 40.536430, mae: 15.279757, mean_q: 37.272476\n",
            " 16460/30000: episode: 2745, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [-5.000, 10.000], mean action: 2583.167 [990.000, 5553.000],  loss: 39.559772, mae: 14.649719, mean_q: 35.770863\n",
            " 16466/30000: episode: 2746, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3071.000 [1560.000, 5447.000],  loss: 47.845036, mae: 14.372388, mean_q: 35.730026\n",
            " 16472/30000: episode: 2747, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2268.833 [350.000, 5608.000],  loss: 24.512688, mae: 13.823319, mean_q: 34.818615\n",
            " 16478/30000: episode: 2748, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3440.833 [1596.000, 4203.000],  loss: 29.136826, mae: 15.094392, mean_q: 37.152473\n",
            " 16484/30000: episode: 2749, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 10.000, mean reward:  1.667 [-5.000, 20.000], mean action: 3721.667 [1042.000, 4838.000],  loss: 50.788960, mae: 14.969516, mean_q: 36.394184\n",
            " 16490/30000: episode: 2750, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3522.500 [961.000, 4824.000],  loss: 57.359211, mae: 14.878142, mean_q: 35.799965\n",
            " 16496/30000: episode: 2751, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2265.500 [607.000, 5682.000],  loss: 49.228794, mae: 14.453575, mean_q: 35.360600\n",
            " 16502/30000: episode: 2752, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3595.833 [238.000, 5329.000],  loss: 32.499928, mae: 14.485600, mean_q: 35.032143\n",
            " 16508/30000: episode: 2753, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: 40.000, mean reward:  6.667 [ 0.000, 30.000], mean action: 2614.500 [512.000, 5329.000],  loss: 48.166061, mae: 15.229215, mean_q: 36.949986\n",
            " 16514/30000: episode: 2754, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3402.833 [946.000, 4870.000],  loss: 38.990555, mae: 15.201983, mean_q: 36.857548\n",
            " 16520/30000: episode: 2755, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2480.833 [770.000, 4378.000],  loss: 27.290033, mae: 14.252114, mean_q: 34.978889\n",
            " 16526/30000: episode: 2756, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3371.167 [2263.000, 5370.000],  loss: 37.584690, mae: 15.964538, mean_q: 37.568146\n",
            " 16532/30000: episode: 2757, duration: 0.148s, episode steps:   6, steps per second:  41, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2266.667 [238.000, 4783.000],  loss: 31.939505, mae: 14.073842, mean_q: 34.238705\n",
            " 16538/30000: episode: 2758, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 110.000, mean reward: 18.333 [ 0.000, 35.000], mean action: 3601.167 [238.000, 5608.000],  loss: 36.238194, mae: 15.019423, mean_q: 35.505539\n",
            " 16544/30000: episode: 2759, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3397.500 [1484.000, 4900.000],  loss: 36.584332, mae: 13.789810, mean_q: 34.107742\n",
            " 16550/30000: episode: 2760, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2297.833 [587.000, 2640.000],  loss: 42.362228, mae: 14.808024, mean_q: 35.770821\n",
            " 16556/30000: episode: 2761, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4276.500 [3388.000, 5233.000],  loss: 31.099455, mae: 15.187049, mean_q: 36.702499\n",
            " 16562/30000: episode: 2762, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3399.500 [1888.000, 5120.000],  loss: 40.621433, mae: 15.348189, mean_q: 37.085464\n",
            " 16568/30000: episode: 2763, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2225.833 [1779.000, 3506.000],  loss: 46.846272, mae: 15.495338, mean_q: 37.234634\n",
            " 16574/30000: episode: 2764, duration: 0.179s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2709.167 [1153.000, 5188.000],  loss: 41.763889, mae: 15.522534, mean_q: 37.391201\n",
            " 16580/30000: episode: 2765, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3859.000 [1153.000, 4939.000],  loss: 32.913059, mae: 14.115456, mean_q: 35.275749\n",
            " 16586/30000: episode: 2766, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3606.833 [1153.000, 5629.000],  loss: 29.936201, mae: 14.315582, mean_q: 35.467106\n",
            " 16592/30000: episode: 2767, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2350.833 [678.000, 3669.000],  loss: 30.582575, mae: 14.093700, mean_q: 34.787338\n",
            " 16598/30000: episode: 2768, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3040.167 [958.000, 5348.000],  loss: 41.131550, mae: 13.169876, mean_q: 32.823101\n",
            " 16604/30000: episode: 2769, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2462.500 [52.000, 4939.000],  loss: 38.196808, mae: 15.133442, mean_q: 36.219547\n",
            " 16610/30000: episode: 2770, duration: 0.179s, episode steps:   6, steps per second:  33, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 3723.833 [580.000, 5129.000],  loss: 44.539532, mae: 14.026497, mean_q: 35.381371\n",
            " 16616/30000: episode: 2771, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2709.000 [958.000, 4228.000],  loss: 32.365299, mae: 14.387840, mean_q: 34.130604\n",
            " 16622/30000: episode: 2772, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2038.833 [958.000, 2255.000],  loss: 49.840786, mae: 14.738986, mean_q: 35.487282\n",
            " 16628/30000: episode: 2773, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1596.833 [820.000, 3366.000],  loss: 35.099854, mae: 13.932114, mean_q: 34.057308\n",
            " 16634/30000: episode: 2774, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1721.167 [958.000, 3136.000],  loss: 44.653931, mae: 14.513070, mean_q: 35.129406\n",
            " 16640/30000: episode: 2775, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2696.000 [990.000, 3534.000],  loss: 41.604286, mae: 13.743930, mean_q: 34.062748\n",
            " 16646/30000: episode: 2776, duration: 0.202s, episode steps:   6, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4131.167 [2111.000, 5703.000],  loss: 53.776318, mae: 14.996979, mean_q: 36.571697\n",
            " 16652/30000: episode: 2777, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2733.667 [958.000, 4185.000],  loss: 43.611328, mae: 14.459767, mean_q: 35.698788\n",
            " 16658/30000: episode: 2778, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2486.167 [1153.000, 2829.000],  loss: 34.965019, mae: 14.266858, mean_q: 35.117615\n",
            " 16664/30000: episode: 2779, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2927.333 [238.000, 5467.000],  loss: 20.443687, mae: 13.079698, mean_q: 32.341854\n",
            " 16670/30000: episode: 2780, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 2694.333 [820.000, 5256.000],  loss: 28.116938, mae: 14.832587, mean_q: 36.288513\n",
            " 16676/30000: episode: 2781, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2242.667 [26.000, 5212.000],  loss: 25.802994, mae: 14.395183, mean_q: 35.736256\n",
            " 16682/30000: episode: 2782, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2716.667 [590.000, 5624.000],  loss: 53.273434, mae: 14.579667, mean_q: 36.021221\n",
            " 16688/30000: episode: 2783, duration: 0.236s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2271.167 [436.000, 5448.000],  loss: 47.118465, mae: 14.733144, mean_q: 35.789803\n",
            " 16694/30000: episode: 2784, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward: 35.000, mean reward:  5.833 [-5.000, 20.000], mean action: 3313.500 [1596.000, 4939.000],  loss: 33.429325, mae: 14.395683, mean_q: 35.541332\n",
            " 16700/30000: episode: 2785, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3389.500 [945.000, 4852.000],  loss: 43.758442, mae: 15.156047, mean_q: 37.199177\n",
            " 16706/30000: episode: 2786, duration: 0.265s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3994.167 [2829.000, 5477.000],  loss: 30.369766, mae: 14.044000, mean_q: 34.708164\n",
            " 16712/30000: episode: 2787, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2126.500 [958.000, 4305.000],  loss: 41.683167, mae: 14.775355, mean_q: 36.141968\n",
            " 16718/30000: episode: 2788, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2374.667 [958.000, 3782.000],  loss: 36.289898, mae: 15.256625, mean_q: 37.354832\n",
            " 16724/30000: episode: 2789, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3223.167 [449.000, 5433.000],  loss: 56.681885, mae: 14.478000, mean_q: 35.335285\n",
            " 16730/30000: episode: 2790, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2340.000 [238.000, 4940.000],  loss: 35.770763, mae: 13.423836, mean_q: 34.507263\n",
            " 16736/30000: episode: 2791, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2819.333 [2454.000, 3542.000],  loss: 27.419767, mae: 14.367223, mean_q: 34.910748\n",
            " 16742/30000: episode: 2792, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2887.000 [162.000, 5017.000],  loss: 39.714939, mae: 13.322951, mean_q: 32.766300\n",
            " 16748/30000: episode: 2793, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2269.833 [27.000, 3945.000],  loss: 44.691406, mae: 15.586022, mean_q: 37.425083\n",
            " 16754/30000: episode: 2794, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3011.667 [1218.000, 5608.000],  loss: 34.497303, mae: 14.232095, mean_q: 35.524506\n",
            " 16760/30000: episode: 2795, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2891.500 [238.000, 4819.000],  loss: 32.818485, mae: 13.441730, mean_q: 33.507664\n",
            " 16766/30000: episode: 2796, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2124.000 [199.000, 5675.000],  loss: 43.377731, mae: 15.471408, mean_q: 37.435383\n",
            " 16772/30000: episode: 2797, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3670.000 [1888.000, 4911.000],  loss: 36.784035, mae: 15.078789, mean_q: 36.546558\n",
            " 16778/30000: episode: 2798, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3348.667 [1484.000, 5747.000],  loss: 40.426418, mae: 13.288994, mean_q: 33.286423\n",
            " 16784/30000: episode: 2799, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2853.167 [770.000, 5582.000],  loss: 38.491024, mae: 13.982399, mean_q: 34.083752\n",
            " 16790/30000: episode: 2800, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3978.333 [1634.000, 5742.000],  loss: 50.106674, mae: 13.910062, mean_q: 34.512463\n",
            " 16796/30000: episode: 2801, duration: 0.164s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3471.333 [1596.000, 5100.000],  loss: 34.974762, mae: 14.509087, mean_q: 35.086563\n",
            " 16802/30000: episode: 2802, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward: 80.000, mean reward: 13.333 [ 0.000, 35.000], mean action: 3259.000 [907.000, 4412.000],  loss: 53.215443, mae: 15.408452, mean_q: 37.237545\n",
            " 16808/30000: episode: 2803, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 40.000, mean reward:  6.667 [ 0.000, 40.000], mean action: 2020.833 [733.000, 4185.000],  loss: 44.552181, mae: 14.242064, mean_q: 34.926441\n",
            " 16814/30000: episode: 2804, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2881.667 [138.000, 4203.000],  loss: 39.393787, mae: 13.480209, mean_q: 34.518124\n",
            " 16820/30000: episode: 2805, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3062.000 [678.000, 5011.000],  loss: 40.959312, mae: 13.986619, mean_q: 34.979862\n",
            " 16826/30000: episode: 2806, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1587.000 [138.000, 2930.000],  loss: 40.379097, mae: 15.328027, mean_q: 37.436852\n",
            " 16832/30000: episode: 2807, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2960.500 [282.000, 5212.000],  loss: 26.386969, mae: 14.125430, mean_q: 35.302750\n",
            " 16838/30000: episode: 2808, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [-5.000, 10.000], mean action: 2160.000 [238.000, 3663.000],  loss: 29.749886, mae: 14.304668, mean_q: 35.546387\n",
            " 16844/30000: episode: 2809, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2330.833 [17.000, 5036.000],  loss: 49.465801, mae: 15.456417, mean_q: 37.450909\n",
            " 16850/30000: episode: 2810, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3050.500 [803.000, 5729.000],  loss: 26.205835, mae: 13.753002, mean_q: 34.234600\n",
            " 16856/30000: episode: 2811, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2093.833 [26.000, 5164.000],  loss: 31.426765, mae: 15.386207, mean_q: 38.143040\n",
            " 16862/30000: episode: 2812, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2602.000 [946.000, 5077.000],  loss: 40.951572, mae: 14.827304, mean_q: 36.467316\n",
            " 16868/30000: episode: 2813, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [-5.000, 10.000], mean action: 2401.667 [889.000, 4189.000],  loss: 30.263357, mae: 14.085706, mean_q: 35.085102\n",
            " 16874/30000: episode: 2814, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2990.000 [158.000, 5602.000],  loss: 35.482571, mae: 13.756027, mean_q: 34.112946\n",
            " 16880/30000: episode: 2815, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2525.833 [273.000, 4244.000],  loss: 27.397278, mae: 13.449947, mean_q: 33.330318\n",
            " 16886/30000: episode: 2816, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2278.667 [52.000, 4163.000],  loss: 39.473415, mae: 14.654248, mean_q: 36.316952\n",
            " 16892/30000: episode: 2817, duration: 0.164s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3164.833 [607.000, 5405.000],  loss: 39.581760, mae: 15.165036, mean_q: 36.121094\n",
            " 16898/30000: episode: 2818, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [-5.000, 10.000], mean action: 2160.000 [870.000, 5046.000],  loss: 39.061798, mae: 14.400291, mean_q: 35.413197\n",
            " 16904/30000: episode: 2819, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: 30.000, mean reward:  5.000 [ 0.000, 30.000], mean action: 3635.500 [2114.000, 5608.000],  loss: 42.528187, mae: 14.869681, mean_q: 36.031445\n",
            " 16910/30000: episode: 2820, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [-5.000, 20.000], mean action: 4227.167 [2698.000, 5656.000],  loss: 80.468941, mae: 15.511673, mean_q: 37.835678\n",
            " 16916/30000: episode: 2821, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 1954.500 [238.000, 4717.000],  loss: 30.291656, mae: 13.952136, mean_q: 33.933117\n",
            " 16922/30000: episode: 2822, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3789.500 [1202.000, 5597.000],  loss: 34.459179, mae: 13.917397, mean_q: 34.586651\n",
            " 16928/30000: episode: 2823, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3334.167 [1260.000, 4954.000],  loss: 49.970398, mae: 14.586735, mean_q: 35.414761\n",
            " 16934/30000: episode: 2824, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3937.667 [471.000, 5658.000],  loss: 28.187551, mae: 14.119588, mean_q: 34.456257\n",
            " 16940/30000: episode: 2825, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 40.000, mean reward:  6.667 [ 0.000, 25.000], mean action: 3939.833 [1555.000, 5656.000],  loss: 24.122625, mae: 14.499524, mean_q: 35.819637\n",
            " 16946/30000: episode: 2826, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2255.167 [30.000, 5396.000],  loss: 40.739010, mae: 14.519721, mean_q: 35.480759\n",
            " 16952/30000: episode: 2827, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3645.333 [2263.000, 5718.000],  loss: 41.848030, mae: 14.633815, mean_q: 35.797489\n",
            " 16958/30000: episode: 2828, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2894.667 [663.000, 5582.000],  loss: 38.043606, mae: 14.414659, mean_q: 36.040760\n",
            " 16964/30000: episode: 2829, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3399.333 [1046.000, 4244.000],  loss: 51.515865, mae: 14.705673, mean_q: 36.562256\n",
            " 16970/30000: episode: 2830, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2988.000 [790.000, 4497.000],  loss: 36.628387, mae: 15.639998, mean_q: 37.909252\n",
            " 16976/30000: episode: 2831, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3591.833 [1886.000, 5299.000],  loss: 40.065147, mae: 14.649112, mean_q: 36.384155\n",
            " 16982/30000: episode: 2832, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2935.667 [1961.000, 5729.000],  loss: 38.090080, mae: 15.165565, mean_q: 37.536564\n",
            " 16988/30000: episode: 2833, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 2783.333 [425.000, 5729.000],  loss: 44.512909, mae: 16.936348, mean_q: 40.884270\n",
            " 16994/30000: episode: 2834, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2974.833 [1596.000, 4412.000],  loss: 57.245548, mae: 13.882808, mean_q: 34.479008\n",
            " 17000/30000: episode: 2835, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1542.833 [106.000, 2920.000],  loss: 41.838692, mae: 14.430850, mean_q: 36.051548\n",
            " 17006/30000: episode: 2836, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 30.000, mean reward:  5.000 [ 0.000, 20.000], mean action: 1661.500 [158.000, 3941.000],  loss: 49.670025, mae: 14.537204, mean_q: 35.737690\n",
            " 17012/30000: episode: 2837, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2773.833 [356.000, 4921.000],  loss: 43.561478, mae: 13.628796, mean_q: 33.326221\n",
            " 17018/30000: episode: 2838, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3050.833 [790.000, 5148.000],  loss: 50.086349, mae: 14.691239, mean_q: 36.048466\n",
            " 17024/30000: episode: 2839, duration: 0.183s, episode steps:   6, steps per second:  33, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2506.333 [1886.000, 3393.000],  loss: 25.390129, mae: 13.936764, mean_q: 34.171116\n",
            " 17030/30000: episode: 2840, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2261.667 [778.000, 4588.000],  loss: 32.555660, mae: 13.208491, mean_q: 33.557945\n",
            " 17036/30000: episode: 2841, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 30.000, mean reward:  5.000 [ 0.000, 25.000], mean action: 4259.333 [2521.000, 5362.000],  loss: 36.419872, mae: 14.711513, mean_q: 35.164242\n",
            " 17042/30000: episode: 2842, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1710.833 [790.000, 3355.000],  loss: 48.216183, mae: 14.598182, mean_q: 35.218109\n",
            " 17048/30000: episode: 2843, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3647.500 [747.000, 5161.000],  loss: 71.905273, mae: 14.139262, mean_q: 35.900810\n",
            " 17054/30000: episode: 2844, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3582.667 [2013.000, 5730.000],  loss: 34.770100, mae: 15.264684, mean_q: 36.202518\n",
            " 17060/30000: episode: 2845, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 4111.500 [2521.000, 5212.000],  loss: 46.829514, mae: 14.204936, mean_q: 35.415619\n",
            " 17066/30000: episode: 2846, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3075.000 [790.000, 4649.000],  loss: 25.445129, mae: 13.936376, mean_q: 34.544857\n",
            " 17072/30000: episode: 2847, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 3060.667 [1554.000, 4185.000],  loss: 39.791622, mae: 13.688045, mean_q: 33.395412\n",
            " 17078/30000: episode: 2848, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 55.000, mean reward:  9.167 [ 0.000, 20.000], mean action: 2713.000 [1178.000, 5608.000],  loss: 34.224903, mae: 14.630580, mean_q: 36.114422\n",
            " 17084/30000: episode: 2849, duration: 0.187s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3389.500 [2444.000, 5722.000],  loss: 33.533962, mae: 15.492703, mean_q: 37.029579\n",
            " 17090/30000: episode: 2850, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2609.667 [663.000, 4244.000],  loss: 29.511225, mae: 14.149156, mean_q: 34.607761\n",
            " 17096/30000: episode: 2851, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3051.833 [663.000, 4715.000],  loss: 53.367725, mae: 14.900365, mean_q: 36.362259\n",
            " 17102/30000: episode: 2852, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward: 15.000, mean reward:  2.500 [-5.000, 25.000], mean action: 3822.667 [2318.000, 4616.000],  loss: 38.045803, mae: 15.554714, mean_q: 38.259121\n",
            " 17108/30000: episode: 2853, duration: 0.236s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3846.000 [52.000, 5753.000],  loss: 35.874653, mae: 14.356280, mean_q: 34.688221\n",
            " 17114/30000: episode: 2854, duration: 0.239s, episode steps:   6, steps per second:  25, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 1788.667 [238.000, 4203.000],  loss: 26.644865, mae: 15.760571, mean_q: 38.009853\n",
            " 17120/30000: episode: 2855, duration: 0.238s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3577.333 [1886.000, 5405.000],  loss: 22.581278, mae: 15.005664, mean_q: 36.167690\n",
            " 17126/30000: episode: 2856, duration: 0.239s, episode steps:   6, steps per second:  25, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3546.167 [10.000, 5730.000],  loss: 50.770050, mae: 14.597030, mean_q: 35.512299\n",
            " 17132/30000: episode: 2857, duration: 0.237s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3898.833 [1967.000, 5405.000],  loss: 50.262329, mae: 14.826823, mean_q: 36.544827\n",
            " 17138/30000: episode: 2858, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward: 10.000, mean reward:  1.667 [-5.000, 25.000], mean action: 4834.167 [1961.000, 5658.000],  loss: 54.078899, mae: 14.458771, mean_q: 35.602200\n",
            " 17144/30000: episode: 2859, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward: 55.000, mean reward:  9.167 [ 0.000, 40.000], mean action: 2099.000 [1242.000, 3567.000],  loss: 41.366322, mae: 13.513557, mean_q: 34.197876\n",
            " 17150/30000: episode: 2860, duration: 0.260s, episode steps:   6, steps per second:  23, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2191.000 [1202.000, 3103.000],  loss: 34.083504, mae: 12.856610, mean_q: 32.468781\n",
            " 17156/30000: episode: 2861, duration: 0.261s, episode steps:   6, steps per second:  23, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2472.167 [662.000, 3655.000],  loss: 59.493591, mae: 13.714587, mean_q: 34.381084\n",
            " 17162/30000: episode: 2862, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2779.167 [1961.000, 3537.000],  loss: 71.113228, mae: 14.126095, mean_q: 34.914082\n",
            " 17168/30000: episode: 2863, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 2482.167 [436.000, 4742.000],  loss: 48.487427, mae: 14.714429, mean_q: 36.503448\n",
            " 17174/30000: episode: 2864, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1579.000 [790.000, 1961.000],  loss: 40.837723, mae: 15.244351, mean_q: 38.313805\n",
            " 17180/30000: episode: 2865, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 65.000, mean reward: 10.833 [ 0.000, 30.000], mean action: 2203.500 [889.000, 4185.000],  loss: 69.868538, mae: 13.994770, mean_q: 35.115841\n",
            " 17186/30000: episode: 2866, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3783.500 [1857.000, 5730.000],  loss: 35.169109, mae: 14.199287, mean_q: 34.096912\n",
            " 17192/30000: episode: 2867, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3361.000 [790.000, 5706.000],  loss: 38.164013, mae: 13.991104, mean_q: 34.918964\n",
            " 17198/30000: episode: 2868, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2836.833 [547.000, 5628.000],  loss: 39.133190, mae: 14.744819, mean_q: 35.992496\n",
            " 17204/30000: episode: 2869, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2940.000 [103.000, 5373.000],  loss: 36.556870, mae: 16.298531, mean_q: 38.985416\n",
            " 17210/30000: episode: 2870, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [-5.000, 10.000], mean action: 3384.500 [1202.000, 5352.000],  loss: 38.410393, mae: 15.394927, mean_q: 37.064053\n",
            " 17216/30000: episode: 2871, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1811.667 [542.000, 4934.000],  loss: 30.471292, mae: 14.701365, mean_q: 36.085682\n",
            " 17222/30000: episode: 2872, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2794.500 [873.000, 5553.000],  loss: 55.552551, mae: 15.002553, mean_q: 36.388691\n",
            " 17228/30000: episode: 2873, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3338.333 [534.000, 5597.000],  loss: 26.798676, mae: 14.578209, mean_q: 35.686642\n",
            " 17234/30000: episode: 2874, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2248.167 [425.000, 3812.000],  loss: 22.635994, mae: 12.907826, mean_q: 32.256439\n",
            " 17240/30000: episode: 2875, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 75.000, mean reward: 12.500 [ 0.000, 35.000], mean action: 2957.500 [460.000, 5656.000],  loss: 36.202854, mae: 13.690557, mean_q: 33.885883\n",
            " 17246/30000: episode: 2876, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2460.500 [560.000, 4336.000],  loss: 33.887993, mae: 14.540062, mean_q: 36.796177\n",
            " 17252/30000: episode: 2877, duration: 0.164s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3418.500 [1536.000, 4685.000],  loss: 35.684761, mae: 14.175396, mean_q: 35.788403\n",
            " 17258/30000: episode: 2878, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 3651.000 [1057.000, 5656.000],  loss: 38.425404, mae: 14.530266, mean_q: 36.413223\n",
            " 17264/30000: episode: 2879, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2535.000 [1494.000, 3955.000],  loss: 47.209316, mae: 15.104894, mean_q: 37.941998\n",
            " 17270/30000: episode: 2880, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 4173.167 [2446.000, 4799.000],  loss: 37.925678, mae: 14.727818, mean_q: 36.942719\n",
            " 17276/30000: episode: 2881, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 3827.667 [816.000, 5486.000],  loss: 47.315598, mae: 15.226323, mean_q: 37.838123\n",
            " 17282/30000: episode: 2882, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 4189.000 [4189.000, 4189.000],  loss: 44.300762, mae: 14.601369, mean_q: 36.689121\n",
            " 17288/30000: episode: 2883, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3157.500 [150.000, 5753.000],  loss: 38.889679, mae: 14.351916, mean_q: 35.394131\n",
            " 17294/30000: episode: 2884, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [-5.000, 25.000], mean action: 2341.000 [887.000, 4189.000],  loss: 36.200394, mae: 14.043629, mean_q: 34.611500\n",
            " 17300/30000: episode: 2885, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 4027.667 [3221.000, 4189.000],  loss: 53.091202, mae: 15.538326, mean_q: 38.243000\n",
            " 17306/30000: episode: 2886, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2721.333 [330.000, 5352.000],  loss: 39.972900, mae: 13.934929, mean_q: 35.301991\n",
            " 17312/30000: episode: 2887, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3401.167 [103.000, 5703.000],  loss: 20.231003, mae: 14.345616, mean_q: 36.174397\n",
            " 17318/30000: episode: 2888, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4407.833 [3118.000, 5706.000],  loss: 47.749073, mae: 15.337657, mean_q: 38.095955\n",
            " 17324/30000: episode: 2889, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3829.167 [1981.000, 5602.000],  loss: 48.788105, mae: 14.037202, mean_q: 35.218044\n",
            " 17330/30000: episode: 2890, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2089.833 [27.000, 4189.000],  loss: 35.569813, mae: 15.679760, mean_q: 38.065083\n",
            " 17336/30000: episode: 2891, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 130.000, mean reward: 21.667 [ 0.000, 35.000], mean action: 3960.667 [1596.000, 5269.000],  loss: 46.744129, mae: 13.786732, mean_q: 34.725468\n",
            " 17342/30000: episode: 2892, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3146.167 [550.000, 5212.000],  loss: 36.551327, mae: 14.674255, mean_q: 36.874119\n",
            " 17348/30000: episode: 2893, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 3131.833 [1345.000, 4374.000],  loss: 63.204151, mae: 14.658135, mean_q: 36.302704\n",
            " 17354/30000: episode: 2894, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -15.000, mean reward: -2.500 [-5.000,  5.000], mean action: 1879.833 [1606.000, 3249.000],  loss: 28.682810, mae: 14.756554, mean_q: 36.131927\n",
            " 17360/30000: episode: 2895, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2085.000 [905.000, 4086.000],  loss: 39.798515, mae: 14.901459, mean_q: 37.059418\n",
            " 17366/30000: episode: 2896, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2919.167 [1081.000, 5680.000],  loss: 44.608410, mae: 13.978203, mean_q: 34.555386\n",
            " 17372/30000: episode: 2897, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2481.167 [238.000, 5597.000],  loss: 37.046352, mae: 14.277486, mean_q: 35.414257\n",
            " 17378/30000: episode: 2898, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4317.333 [2950.000, 5753.000],  loss: 48.464874, mae: 14.359191, mean_q: 34.864349\n",
            " 17384/30000: episode: 2899, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2922.500 [183.000, 5539.000],  loss: 29.376993, mae: 15.337955, mean_q: 37.456982\n",
            " 17390/30000: episode: 2900, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3442.000 [649.000, 5376.000],  loss: 42.703552, mae: 15.260772, mean_q: 36.806484\n",
            " 17396/30000: episode: 2901, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3776.000 [889.000, 5628.000],  loss: 41.344017, mae: 14.852580, mean_q: 35.766922\n",
            " 17402/30000: episode: 2902, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3722.833 [1204.000, 5461.000],  loss: 32.209476, mae: 15.648197, mean_q: 36.621674\n",
            " 17408/30000: episode: 2903, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1594.500 [261.000, 5373.000],  loss: 38.988541, mae: 14.347671, mean_q: 34.564480\n",
            " 17414/30000: episode: 2904, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2954.000 [449.000, 5742.000],  loss: 48.621719, mae: 14.739610, mean_q: 34.683414\n",
            " 17420/30000: episode: 2905, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward: 190.000, mean reward: 31.667 [30.000, 35.000], mean action: 2664.167 [238.000, 4185.000],  loss: 61.163960, mae: 15.660751, mean_q: 36.933796\n",
            " 17426/30000: episode: 2906, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2666.333 [808.000, 4749.000],  loss: 35.671623, mae: 14.080849, mean_q: 34.914036\n",
            " 17432/30000: episode: 2907, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3267.500 [1707.000, 4774.000],  loss: 60.652302, mae: 15.856471, mean_q: 38.149998\n",
            " 17438/30000: episode: 2908, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2287.167 [709.000, 4840.000],  loss: 40.016117, mae: 14.683342, mean_q: 35.911457\n",
            " 17444/30000: episode: 2909, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3444.167 [26.000, 5597.000],  loss: 38.279858, mae: 14.472251, mean_q: 35.527374\n",
            " 17450/30000: episode: 2910, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2586.500 [721.000, 4094.000],  loss: 48.698929, mae: 14.143197, mean_q: 35.364182\n",
            " 17456/30000: episode: 2911, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1499.167 [44.000, 3631.000],  loss: 22.937788, mae: 13.837258, mean_q: 33.665318\n",
            " 17462/30000: episode: 2912, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 140.000, mean reward: 23.333 [ 0.000, 35.000], mean action: 2955.833 [1596.000, 4185.000],  loss: 52.879894, mae: 14.617581, mean_q: 35.432354\n",
            " 17468/30000: episode: 2913, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2679.667 [149.000, 5381.000],  loss: 35.981106, mae: 15.381976, mean_q: 36.718445\n",
            " 17474/30000: episode: 2914, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4048.833 [2013.000, 5433.000],  loss: 47.554867, mae: 14.697200, mean_q: 36.465107\n",
            " 17480/30000: episode: 2915, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2032.667 [554.000, 4667.000],  loss: 49.074127, mae: 14.365861, mean_q: 35.397480\n",
            " 17486/30000: episode: 2916, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2826.833 [709.000, 5057.000],  loss: 41.900219, mae: 14.079540, mean_q: 34.460068\n",
            " 17492/30000: episode: 2917, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3118.500 [709.000, 4654.000],  loss: 30.059290, mae: 14.527425, mean_q: 35.309452\n",
            " 17498/30000: episode: 2918, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 2389.000 [709.000, 3170.000],  loss: 40.080925, mae: 14.681385, mean_q: 35.637730\n",
            " 17504/30000: episode: 2919, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2935.500 [1313.000, 3804.000],  loss: 34.094620, mae: 12.939155, mean_q: 31.844244\n",
            " 17510/30000: episode: 2920, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2361.333 [946.000, 4599.000],  loss: 48.090878, mae: 15.307282, mean_q: 37.752640\n",
            " 17516/30000: episode: 2921, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2528.333 [879.000, 5338.000],  loss: 43.717300, mae: 15.105481, mean_q: 37.105900\n",
            " 17522/30000: episode: 2922, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 40.000, mean reward:  6.667 [-5.000, 20.000], mean action: 2302.667 [709.000, 2669.000],  loss: 28.180059, mae: 13.785740, mean_q: 35.120266\n",
            " 17528/30000: episode: 2923, duration: 0.238s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3128.667 [709.000, 5588.000],  loss: 39.142231, mae: 14.109462, mean_q: 35.529186\n",
            " 17534/30000: episode: 2924, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3136.667 [709.000, 4893.000],  loss: 35.239677, mae: 14.579122, mean_q: 36.612804\n",
            " 17540/30000: episode: 2925, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 3118.333 [309.000, 5164.000],  loss: 49.078476, mae: 14.244625, mean_q: 35.962551\n",
            " 17546/30000: episode: 2926, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1799.833 [847.000, 3631.000],  loss: 38.009914, mae: 15.187062, mean_q: 37.768799\n",
            " 17552/30000: episode: 2927, duration: 0.240s, episode steps:   6, steps per second:  25, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1408.500 [366.000, 5148.000],  loss: 36.443432, mae: 14.001587, mean_q: 35.113270\n",
            " 17558/30000: episode: 2928, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2710.000 [709.000, 4918.000],  loss: 33.786942, mae: 14.664624, mean_q: 36.310284\n",
            " 17564/30000: episode: 2929, duration: 0.257s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1601.000 [460.000, 3830.000],  loss: 57.330318, mae: 15.002248, mean_q: 37.609112\n",
            " 17570/30000: episode: 2930, duration: 0.263s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3027.333 [709.000, 5405.000],  loss: 40.981998, mae: 15.640823, mean_q: 38.899792\n",
            " 17576/30000: episode: 2931, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2825.667 [709.000, 4918.000],  loss: 31.053396, mae: 14.163162, mean_q: 36.563526\n",
            " 17582/30000: episode: 2932, duration: 0.239s, episode steps:   6, steps per second:  25, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3146.167 [709.000, 4134.000],  loss: 54.726791, mae: 13.749110, mean_q: 34.997059\n",
            " 17588/30000: episode: 2933, duration: 0.267s, episode steps:   6, steps per second:  22, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1998.833 [709.000, 3249.000],  loss: 32.615105, mae: 15.209708, mean_q: 37.829754\n",
            " 17594/30000: episode: 2934, duration: 0.202s, episode steps:   6, steps per second:  30, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2602.167 [1596.000, 3748.000],  loss: 50.505310, mae: 14.300323, mean_q: 36.141960\n",
            " 17600/30000: episode: 2935, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4005.500 [905.000, 5718.000],  loss: 51.732693, mae: 14.484226, mean_q: 36.258289\n",
            " 17606/30000: episode: 2936, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3341.667 [684.000, 4884.000],  loss: 38.731976, mae: 14.551758, mean_q: 34.682484\n",
            " 17612/30000: episode: 2937, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1570.333 [285.000, 1970.000],  loss: 48.007267, mae: 13.742682, mean_q: 33.911564\n",
            " 17618/30000: episode: 2938, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2939.500 [183.000, 4807.000],  loss: 62.773758, mae: 14.548476, mean_q: 35.658207\n",
            " 17624/30000: episode: 2939, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3385.667 [709.000, 3921.000],  loss: 49.646114, mae: 14.320699, mean_q: 35.693630\n",
            " 17630/30000: episode: 2940, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1774.000 [26.000, 5656.000],  loss: 53.366833, mae: 15.265590, mean_q: 36.783764\n",
            " 17636/30000: episode: 2941, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1879.833 [290.000, 3826.000],  loss: 33.118397, mae: 13.421333, mean_q: 33.216068\n",
            " 17642/30000: episode: 2942, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2351.833 [1239.000, 4884.000],  loss: 49.210728, mae: 13.471394, mean_q: 34.542744\n",
            " 17648/30000: episode: 2943, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 269.667 [238.000, 428.000],  loss: 32.904560, mae: 14.548518, mean_q: 35.908375\n",
            " 17654/30000: episode: 2944, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3187.333 [709.000, 4921.000],  loss: 50.894299, mae: 14.100265, mean_q: 35.513241\n",
            " 17660/30000: episode: 2945, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3103.667 [580.000, 4884.000],  loss: 43.349812, mae: 14.622325, mean_q: 35.623520\n",
            " 17666/30000: episode: 2946, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3219.667 [2652.000, 4884.000],  loss: 27.280167, mae: 15.161735, mean_q: 36.842979\n",
            " 17672/30000: episode: 2947, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2468.000 [709.000, 3054.000],  loss: 52.282787, mae: 14.151497, mean_q: 34.890095\n",
            " 17678/30000: episode: 2948, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4288.167 [3094.000, 5387.000],  loss: 29.361572, mae: 13.520793, mean_q: 34.447388\n",
            " 17684/30000: episode: 2949, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 25.000, mean reward:  4.167 [-5.000, 20.000], mean action: 3182.333 [1632.000, 4884.000],  loss: 43.104797, mae: 13.670959, mean_q: 33.942394\n",
            " 17690/30000: episode: 2950, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3014.833 [1419.000, 5523.000],  loss: 40.797241, mae: 14.649304, mean_q: 36.382938\n",
            " 17696/30000: episode: 2951, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1744.833 [1076.000, 3136.000],  loss: 45.946751, mae: 13.702634, mean_q: 34.266361\n",
            " 17702/30000: episode: 2952, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3083.333 [1654.000, 4902.000],  loss: 26.913050, mae: 14.912736, mean_q: 36.480625\n",
            " 17708/30000: episode: 2953, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3129.500 [1984.000, 4412.000],  loss: 25.711374, mae: 15.219208, mean_q: 36.657349\n",
            " 17714/30000: episode: 2954, duration: 0.179s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3269.333 [309.000, 5466.000],  loss: 48.709812, mae: 14.610343, mean_q: 35.974613\n",
            " 17720/30000: episode: 2955, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2778.667 [755.000, 5532.000],  loss: 55.673038, mae: 13.782079, mean_q: 34.869740\n",
            " 17726/30000: episode: 2956, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 3238.500 [1148.000, 5656.000],  loss: 46.616886, mae: 14.741966, mean_q: 36.378475\n",
            " 17732/30000: episode: 2957, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3137.500 [946.000, 5656.000],  loss: 26.144724, mae: 13.805644, mean_q: 34.676647\n",
            " 17738/30000: episode: 2958, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3642.333 [3249.000, 3721.000],  loss: 28.487776, mae: 13.836812, mean_q: 34.667950\n",
            " 17744/30000: episode: 2959, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3648.333 [1178.000, 4893.000],  loss: 50.134659, mae: 14.707482, mean_q: 36.328495\n",
            " 17750/30000: episode: 2960, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2982.667 [2152.000, 5120.000],  loss: 36.923977, mae: 14.658603, mean_q: 36.649036\n",
            " 17756/30000: episode: 2961, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3765.667 [733.000, 5718.000],  loss: 41.062500, mae: 14.227723, mean_q: 35.527409\n",
            " 17762/30000: episode: 2962, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 3071.167 [442.000, 5703.000],  loss: 40.638416, mae: 12.770313, mean_q: 32.732548\n",
            " 17768/30000: episode: 2963, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2479.500 [309.000, 3721.000],  loss: 38.003300, mae: 14.386615, mean_q: 35.578537\n",
            " 17774/30000: episode: 2964, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [-5.000, 25.000], mean action: 3767.000 [457.000, 5658.000],  loss: 42.742802, mae: 14.139038, mean_q: 35.790112\n",
            " 17780/30000: episode: 2965, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2101.500 [145.000, 3921.000],  loss: 66.521866, mae: 15.487050, mean_q: 37.550816\n",
            " 17786/30000: episode: 2966, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2566.167 [28.000, 4797.000],  loss: 26.938421, mae: 14.677483, mean_q: 36.040634\n",
            " 17792/30000: episode: 2967, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3064.833 [72.000, 4918.000],  loss: 70.895805, mae: 14.317260, mean_q: 35.633224\n",
            " 17798/30000: episode: 2968, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2934.333 [1622.000, 5339.000],  loss: 22.208353, mae: 14.212764, mean_q: 34.667255\n",
            " 17804/30000: episode: 2969, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2627.333 [873.000, 4185.000],  loss: 34.792999, mae: 13.643239, mean_q: 34.588215\n",
            " 17810/30000: episode: 2970, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3206.667 [2252.000, 3856.000],  loss: 41.312977, mae: 14.584251, mean_q: 35.572544\n",
            " 17816/30000: episode: 2971, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 4364.500 [3251.000, 5696.000],  loss: 60.091312, mae: 14.333884, mean_q: 34.847549\n",
            " 17822/30000: episode: 2972, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 2592.667 [580.000, 4765.000],  loss: 32.176662, mae: 14.941292, mean_q: 36.216534\n",
            " 17828/30000: episode: 2973, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2436.167 [1744.000, 3721.000],  loss: 46.848591, mae: 16.375832, mean_q: 39.703129\n",
            " 17834/30000: episode: 2974, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3767.333 [1419.000, 5352.000],  loss: 33.403072, mae: 13.964607, mean_q: 34.561253\n",
            " 17840/30000: episode: 2975, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3653.500 [1866.000, 5608.000],  loss: 36.203865, mae: 13.547627, mean_q: 33.768116\n",
            " 17846/30000: episode: 2976, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3496.167 [1530.000, 4963.000],  loss: 38.546291, mae: 13.641434, mean_q: 33.570965\n",
            " 17852/30000: episode: 2977, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3280.667 [1676.000, 3830.000],  loss: 44.002789, mae: 13.489441, mean_q: 34.085270\n",
            " 17858/30000: episode: 2978, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2876.167 [1358.000, 5597.000],  loss: 38.706760, mae: 15.049145, mean_q: 36.308659\n",
            " 17864/30000: episode: 2979, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4069.833 [3183.000, 4907.000],  loss: 40.402111, mae: 13.465377, mean_q: 33.558556\n",
            " 17870/30000: episode: 2980, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2259.500 [1381.000, 4185.000],  loss: 49.936611, mae: 14.566684, mean_q: 34.903996\n",
            " 17876/30000: episode: 2981, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2712.833 [1059.000, 3985.000],  loss: 29.493677, mae: 14.719518, mean_q: 36.468708\n",
            " 17882/30000: episode: 2982, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  5.000], mean action: 4976.500 [3721.000, 5433.000],  loss: 30.851690, mae: 14.376766, mean_q: 35.373486\n",
            " 17888/30000: episode: 2983, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 4485.500 [2818.000, 5602.000],  loss: 37.503384, mae: 15.442212, mean_q: 37.966091\n",
            " 17894/30000: episode: 2984, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 2805.167 [1338.000, 5281.000],  loss: 85.662987, mae: 15.130589, mean_q: 37.158924\n",
            " 17900/30000: episode: 2985, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3786.000 [216.000, 4500.000],  loss: 34.996510, mae: 13.912272, mean_q: 34.002262\n",
            " 17906/30000: episode: 2986, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2811.167 [1127.000, 4595.000],  loss: 41.194939, mae: 13.943217, mean_q: 35.084438\n",
            " 17912/30000: episode: 2987, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2144.833 [852.000, 3433.000],  loss: 38.879528, mae: 14.069051, mean_q: 34.670288\n",
            " 17918/30000: episode: 2988, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3200.500 [826.000, 4992.000],  loss: 54.663086, mae: 14.982243, mean_q: 36.129482\n",
            " 17924/30000: episode: 2989, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2739.833 [575.000, 4595.000],  loss: 34.954800, mae: 14.296430, mean_q: 34.935062\n",
            " 17930/30000: episode: 2990, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2488.833 [103.000, 3712.000],  loss: 30.520958, mae: 13.688575, mean_q: 33.866512\n",
            " 17936/30000: episode: 2991, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [-5.000, 15.000], mean action: 1444.167 [946.000, 2688.000],  loss: 41.593376, mae: 15.508247, mean_q: 37.681591\n",
            " 17942/30000: episode: 2992, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [-5.000, 20.000], mean action: 3843.667 [2901.000, 5447.000],  loss: 49.058655, mae: 15.126801, mean_q: 36.569450\n",
            " 17948/30000: episode: 2993, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 30.000, mean reward:  5.000 [ 0.000, 20.000], mean action: 3306.000 [915.000, 5508.000],  loss: 39.776104, mae: 14.110019, mean_q: 35.227654\n",
            " 17954/30000: episode: 2994, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2299.333 [224.000, 4963.000],  loss: 50.240265, mae: 13.832474, mean_q: 33.930313\n",
            " 17960/30000: episode: 2995, duration: 0.192s, episode steps:   6, steps per second:  31, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2912.167 [502.000, 5703.000],  loss: 49.081745, mae: 14.623103, mean_q: 35.545990\n",
            " 17966/30000: episode: 2996, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2052.833 [58.000, 4963.000],  loss: 27.515198, mae: 15.219766, mean_q: 37.919846\n",
            " 17972/30000: episode: 2997, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 4489.333 [560.000, 5608.000],  loss: 34.502850, mae: 14.582276, mean_q: 35.988178\n",
            " 17978/30000: episode: 2998, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2894.667 [926.000, 4230.000],  loss: 44.406612, mae: 13.965122, mean_q: 34.721119\n",
            " 17984/30000: episode: 2999, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3851.000 [2008.000, 5626.000],  loss: 23.572510, mae: 14.588936, mean_q: 35.596943\n",
            " 17990/30000: episode: 3000, duration: 0.263s, episode steps:   6, steps per second:  23, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 2804.333 [1744.000, 5447.000],  loss: 30.331522, mae: 13.730785, mean_q: 34.113491\n",
            " 17996/30000: episode: 3001, duration: 0.271s, episode steps:   6, steps per second:  22, episode reward: 20.000, mean reward:  3.333 [ 0.000, 10.000], mean action: 2811.167 [1102.000, 4918.000],  loss: 35.317623, mae: 14.276156, mean_q: 35.068417\n",
            " 18002/30000: episode: 3002, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3344.333 [1244.000, 4963.000],  loss: 36.791950, mae: 14.195212, mean_q: 35.719009\n",
            " 18008/30000: episode: 3003, duration: 0.264s, episode steps:   6, steps per second:  23, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3537.000 [1204.000, 5696.000],  loss: 33.513084, mae: 13.869075, mean_q: 33.996746\n",
            " 18014/30000: episode: 3004, duration: 0.270s, episode steps:   6, steps per second:  22, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2085.833 [852.000, 3871.000],  loss: 38.714920, mae: 13.280835, mean_q: 33.609383\n",
            " 18020/30000: episode: 3005, duration: 0.271s, episode steps:   6, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3844.000 [852.000, 5626.000],  loss: 30.719824, mae: 13.506370, mean_q: 33.335293\n",
            " 18026/30000: episode: 3006, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3820.167 [3655.000, 4415.000],  loss: 41.765495, mae: 14.716544, mean_q: 36.220661\n",
            " 18032/30000: episode: 3007, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3492.833 [303.000, 5656.000],  loss: 30.896704, mae: 14.276703, mean_q: 35.707554\n",
            " 18038/30000: episode: 3008, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2790.667 [275.000, 5447.000],  loss: 27.736984, mae: 13.045934, mean_q: 32.808270\n",
            " 18044/30000: episode: 3009, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3686.667 [959.000, 5717.000],  loss: 44.500732, mae: 14.229538, mean_q: 35.002579\n",
            " 18050/30000: episode: 3010, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3477.000 [2264.000, 5348.000],  loss: 47.784153, mae: 14.924203, mean_q: 36.730289\n",
            " 18056/30000: episode: 3011, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3863.333 [2008.000, 4963.000],  loss: 41.591099, mae: 14.354415, mean_q: 35.214642\n",
            " 18062/30000: episode: 3012, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3231.167 [1057.000, 4439.000],  loss: 33.576725, mae: 14.262826, mean_q: 35.375950\n",
            " 18068/30000: episode: 3013, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [-5.000, 15.000], mean action: 3506.500 [2865.000, 4963.000],  loss: 35.219898, mae: 14.794197, mean_q: 36.389858\n",
            " 18074/30000: episode: 3014, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4057.167 [1910.000, 5602.000],  loss: 48.521866, mae: 14.541608, mean_q: 36.001667\n",
            " 18080/30000: episode: 3015, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4912.333 [2688.000, 5742.000],  loss: 43.940258, mae: 15.546597, mean_q: 37.471073\n",
            " 18086/30000: episode: 3016, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3508.167 [138.000, 5486.000],  loss: 28.668297, mae: 13.595818, mean_q: 34.016773\n",
            " 18092/30000: episode: 3017, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3908.167 [969.000, 4992.000],  loss: 43.411976, mae: 14.815747, mean_q: 36.880810\n",
            " 18098/30000: episode: 3018, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 4392.833 [3204.000, 5745.000],  loss: 36.918217, mae: 15.159387, mean_q: 37.778816\n",
            " 18104/30000: episode: 3019, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3452.000 [285.000, 4749.000],  loss: 45.631271, mae: 13.949722, mean_q: 35.578548\n",
            " 18110/30000: episode: 3020, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2759.000 [372.000, 3769.000],  loss: 27.614405, mae: 15.547025, mean_q: 38.375988\n",
            " 18116/30000: episode: 3021, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2544.000 [309.000, 4586.000],  loss: 31.148691, mae: 14.015811, mean_q: 36.622196\n",
            " 18122/30000: episode: 3022, duration: 0.185s, episode steps:   6, steps per second:  32, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3465.167 [2188.000, 4138.000],  loss: 52.431610, mae: 14.168164, mean_q: 34.978622\n",
            " 18128/30000: episode: 3023, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 4425.500 [1178.000, 5433.000],  loss: 37.202568, mae: 13.681924, mean_q: 34.946903\n",
            " 18134/30000: episode: 3024, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3530.167 [1555.000, 5326.000],  loss: 33.212372, mae: 13.494512, mean_q: 35.336205\n",
            " 18140/30000: episode: 3025, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2362.167 [347.000, 4497.000],  loss: 47.575214, mae: 13.786663, mean_q: 34.659046\n",
            " 18146/30000: episode: 3026, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3511.167 [3094.000, 5597.000],  loss: 29.302595, mae: 14.659168, mean_q: 36.034077\n",
            " 18152/30000: episode: 3027, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 2985.000 [1059.000, 4838.000],  loss: 37.204800, mae: 14.278409, mean_q: 34.549984\n",
            " 18158/30000: episode: 3028, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2197.500 [578.000, 3780.000],  loss: 34.497021, mae: 13.412354, mean_q: 33.706516\n",
            " 18164/30000: episode: 3029, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3222.667 [2264.000, 4339.000],  loss: 30.146227, mae: 14.143102, mean_q: 34.938618\n",
            " 18170/30000: episode: 3030, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2406.667 [1102.000, 3643.000],  loss: 57.369869, mae: 13.389775, mean_q: 33.163296\n",
            " 18176/30000: episode: 3031, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2901.000 [2197.000, 4838.000],  loss: 41.507923, mae: 15.438716, mean_q: 37.626690\n",
            " 18182/30000: episode: 3032, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2894.667 [237.000, 4749.000],  loss: 41.826855, mae: 15.783118, mean_q: 37.828529\n",
            " 18188/30000: episode: 3033, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [-5.000, 20.000], mean action: 4085.000 [238.000, 5640.000],  loss: 58.120388, mae: 12.958823, mean_q: 32.620995\n",
            " 18194/30000: episode: 3034, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 35.000, mean reward:  5.833 [ 0.000, 25.000], mean action: 4257.167 [1596.000, 5699.000],  loss: 45.071274, mae: 14.207062, mean_q: 35.370178\n",
            " 18200/30000: episode: 3035, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2506.833 [238.000, 4374.000],  loss: 46.057156, mae: 13.723470, mean_q: 33.811008\n",
            " 18206/30000: episode: 3036, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  5.000], mean action: 2160.500 [578.000, 4433.000],  loss: 31.711996, mae: 13.534442, mean_q: 33.339985\n",
            " 18212/30000: episode: 3037, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 55.000, mean reward:  9.167 [ 0.000, 25.000], mean action: 3556.667 [946.000, 5656.000],  loss: 27.770056, mae: 14.475581, mean_q: 35.752522\n",
            " 18218/30000: episode: 3038, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 30.000, mean reward:  5.000 [ 0.000, 30.000], mean action: 1939.667 [238.000, 3447.000],  loss: 26.495493, mae: 14.039273, mean_q: 34.996944\n",
            " 18224/30000: episode: 3039, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2833.667 [1790.000, 4390.000],  loss: 38.207546, mae: 13.403394, mean_q: 34.070042\n",
            " 18230/30000: episode: 3040, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 1947.167 [791.000, 3447.000],  loss: 63.594860, mae: 15.943741, mean_q: 38.496548\n",
            " 18236/30000: episode: 3041, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3052.667 [205.000, 5396.000],  loss: 41.725285, mae: 14.495767, mean_q: 35.403347\n",
            " 18242/30000: episode: 3042, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1526.333 [62.000, 4548.000],  loss: 38.115524, mae: 13.911591, mean_q: 34.465202\n",
            " 18248/30000: episode: 3043, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2452.667 [907.000, 4134.000],  loss: 41.503773, mae: 14.960902, mean_q: 36.475136\n",
            " 18254/30000: episode: 3044, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 4060.333 [1634.000, 5656.000],  loss: 38.784199, mae: 14.254295, mean_q: 34.492924\n",
            " 18260/30000: episode: 3045, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1494.167 [145.000, 2967.000],  loss: 51.449081, mae: 14.415874, mean_q: 35.571159\n",
            " 18266/30000: episode: 3046, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 1874.833 [145.000, 4749.000],  loss: 37.121769, mae: 14.485313, mean_q: 35.156952\n",
            " 18272/30000: episode: 3047, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1138.000 [145.000, 5326.000],  loss: 30.996332, mae: 15.733026, mean_q: 38.195721\n",
            " 18278/30000: episode: 3048, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1515.333 [44.000, 3286.000],  loss: 33.957451, mae: 14.890884, mean_q: 36.723495\n",
            " 18284/30000: episode: 3049, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2712.333 [1127.000, 4838.000],  loss: 53.371670, mae: 15.140555, mean_q: 36.224243\n",
            " 18290/30000: episode: 3050, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2391.833 [303.000, 4717.000],  loss: 55.230255, mae: 16.011866, mean_q: 37.814884\n",
            " 18296/30000: episode: 3051, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1927.500 [145.000, 3658.000],  loss: 43.543446, mae: 14.901221, mean_q: 36.153946\n",
            " 18302/30000: episode: 3052, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2163.500 [238.000, 5002.000],  loss: 37.004471, mae: 14.248652, mean_q: 35.269840\n",
            " 18308/30000: episode: 3053, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2791.333 [238.000, 5121.000],  loss: 36.992962, mae: 14.904823, mean_q: 36.778435\n",
            " 18314/30000: episode: 3054, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4136.500 [3054.000, 4614.000],  loss: 40.627174, mae: 13.375389, mean_q: 33.381222\n",
            " 18320/30000: episode: 3055, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 1974.000 [238.000, 4614.000],  loss: 44.280895, mae: 14.212566, mean_q: 34.486149\n",
            " 18326/30000: episode: 3056, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3742.000 [436.000, 5333.000],  loss: 40.596050, mae: 13.967216, mean_q: 34.732960\n",
            " 18332/30000: episode: 3057, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2012.500 [285.000, 4497.000],  loss: 35.096161, mae: 14.610686, mean_q: 36.065083\n",
            " 18338/30000: episode: 3058, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3261.500 [145.000, 5269.000],  loss: 57.993973, mae: 14.542052, mean_q: 36.223583\n",
            " 18344/30000: episode: 3059, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2726.833 [285.000, 4246.000],  loss: 42.740112, mae: 15.324021, mean_q: 37.195423\n",
            " 18350/30000: episode: 3060, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2841.833 [1191.000, 4185.000],  loss: 28.523066, mae: 14.866639, mean_q: 35.878796\n",
            " 18356/30000: episode: 3061, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward: 20.000, mean reward:  3.333 [ 0.000, 10.000], mean action: 2648.500 [1554.000, 3693.000],  loss: 40.896603, mae: 14.750876, mean_q: 35.566235\n",
            " 18362/30000: episode: 3062, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3047.167 [1501.000, 4725.000],  loss: 40.018269, mae: 14.191673, mean_q: 35.730701\n",
            " 18368/30000: episode: 3063, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2221.833 [907.000, 3951.000],  loss: 37.864281, mae: 14.393277, mean_q: 35.475372\n",
            " 18374/30000: episode: 3064, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3040.833 [1102.000, 3712.000],  loss: 37.306389, mae: 14.043260, mean_q: 35.644623\n",
            " 18380/30000: episode: 3065, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward: -5.000, mean reward: -0.833 [-5.000,  5.000], mean action: 2331.000 [285.000, 4802.000],  loss: 29.855570, mae: 14.089973, mean_q: 35.011395\n",
            " 18386/30000: episode: 3066, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2386.833 [294.000, 5717.000],  loss: 33.488739, mae: 15.103442, mean_q: 36.999645\n",
            " 18392/30000: episode: 3067, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4051.167 [1127.000, 5656.000],  loss: 51.433643, mae: 14.443633, mean_q: 36.529846\n",
            " 18398/30000: episode: 3068, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2725.500 [145.000, 5689.000],  loss: 47.886234, mae: 15.904630, mean_q: 39.809326\n",
            " 18404/30000: episode: 3069, duration: 0.262s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [-5.000, 10.000], mean action: 1968.000 [285.000, 4691.000],  loss: 63.477947, mae: 15.296624, mean_q: 37.303375\n",
            " 18410/30000: episode: 3070, duration: 0.257s, episode steps:   6, steps per second:  23, episode reward: 65.000, mean reward: 10.833 [ 0.000, 35.000], mean action: 2917.667 [285.000, 5348.000],  loss: 31.012320, mae: 14.026923, mean_q: 34.309490\n",
            " 18416/30000: episode: 3071, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2369.500 [61.000, 4921.000],  loss: 37.303738, mae: 14.426536, mean_q: 36.234402\n",
            " 18422/30000: episode: 3072, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3510.333 [1094.000, 5602.000],  loss: 25.640810, mae: 12.694297, mean_q: 32.114677\n",
            " 18428/30000: episode: 3073, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1991.167 [328.000, 2609.000],  loss: 23.292261, mae: 13.884762, mean_q: 34.763611\n",
            " 18434/30000: episode: 3074, duration: 0.270s, episode steps:   6, steps per second:  22, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3980.833 [285.000, 5473.000],  loss: 43.564152, mae: 16.215326, mean_q: 38.913342\n",
            " 18440/30000: episode: 3075, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 2514.667 [250.000, 4900.000],  loss: 27.755270, mae: 14.674186, mean_q: 36.190651\n",
            " 18446/30000: episode: 3076, duration: 0.245s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3727.333 [2555.000, 5019.000],  loss: 39.119129, mae: 13.724618, mean_q: 34.567173\n",
            " 18452/30000: episode: 3077, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2642.833 [158.000, 5061.000],  loss: 26.597403, mae: 14.217183, mean_q: 35.204967\n",
            " 18458/30000: episode: 3078, duration: 0.274s, episode steps:   6, steps per second:  22, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 3054.500 [178.000, 5486.000],  loss: 42.299026, mae: 14.409522, mean_q: 36.295471\n",
            " 18464/30000: episode: 3079, duration: 0.197s, episode steps:   6, steps per second:  30, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3806.500 [2599.000, 5576.000],  loss: 43.409595, mae: 14.951066, mean_q: 36.789135\n",
            " 18470/30000: episode: 3080, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1657.833 [238.000, 4091.000],  loss: 46.674622, mae: 14.755840, mean_q: 36.128452\n",
            " 18476/30000: episode: 3081, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3001.000 [1224.000, 4894.000],  loss: 34.949142, mae: 14.445873, mean_q: 35.339371\n",
            " 18482/30000: episode: 3082, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1958.000 [106.000, 4073.000],  loss: 30.033102, mae: 14.290524, mean_q: 35.971653\n",
            " 18488/30000: episode: 3083, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 35.000, mean reward:  5.833 [ 0.000, 25.000], mean action: 2872.500 [1127.000, 5156.000],  loss: 38.985882, mae: 14.414851, mean_q: 35.124611\n",
            " 18494/30000: episode: 3084, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2213.667 [285.000, 4158.000],  loss: 62.440308, mae: 15.102756, mean_q: 37.165241\n",
            " 18500/30000: episode: 3085, duration: 0.145s, episode steps:   6, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 468.167 [28.000, 1576.000],  loss: 63.662369, mae: 14.495358, mean_q: 36.466354\n",
            " 18506/30000: episode: 3086, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2984.000 [285.000, 5211.000],  loss: 31.977873, mae: 15.677709, mean_q: 37.434170\n",
            " 18512/30000: episode: 3087, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2719.667 [285.000, 4412.000],  loss: 30.597204, mae: 13.641320, mean_q: 33.829922\n",
            " 18518/30000: episode: 3088, duration: 0.164s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3490.167 [709.000, 4954.000],  loss: 36.806850, mae: 13.292605, mean_q: 33.212250\n",
            " 18524/30000: episode: 3089, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3818.500 [285.000, 5602.000],  loss: 51.819366, mae: 14.181361, mean_q: 35.752434\n",
            " 18530/30000: episode: 3090, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3795.667 [1110.000, 5597.000],  loss: 26.096558, mae: 14.129704, mean_q: 33.911957\n",
            " 18536/30000: episode: 3091, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3013.333 [575.000, 5171.000],  loss: 48.467823, mae: 14.814841, mean_q: 36.152668\n",
            " 18542/30000: episode: 3092, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2778.667 [1540.000, 3922.000],  loss: 36.971889, mae: 16.268415, mean_q: 38.104519\n",
            " 18548/30000: episode: 3093, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 3184.833 [238.000, 5163.000],  loss: 29.886551, mae: 14.070485, mean_q: 34.663864\n",
            " 18554/30000: episode: 3094, duration: 0.183s, episode steps:   6, steps per second:  33, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 3427.667 [2609.000, 3964.000],  loss: 31.919935, mae: 13.273284, mean_q: 33.446247\n",
            " 18560/30000: episode: 3095, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3569.167 [1094.000, 5689.000],  loss: 55.390583, mae: 13.705788, mean_q: 33.924038\n",
            " 18566/30000: episode: 3096, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1747.667 [429.000, 3156.000],  loss: 33.922916, mae: 13.490059, mean_q: 33.312580\n",
            " 18572/30000: episode: 3097, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: 130.000, mean reward: 21.667 [ 0.000, 35.000], mean action: 2469.667 [238.000, 4185.000],  loss: 45.327526, mae: 14.228711, mean_q: 34.777142\n",
            " 18578/30000: episode: 3098, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [-5.000, 25.000], mean action: 3359.167 [238.000, 5281.000],  loss: 40.766434, mae: 14.195287, mean_q: 34.975082\n",
            " 18584/30000: episode: 3099, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2227.000 [238.000, 5163.000],  loss: 47.385712, mae: 15.649087, mean_q: 37.795807\n",
            " 18590/30000: episode: 3100, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2238.500 [818.000, 3334.000],  loss: 44.372833, mae: 15.648728, mean_q: 38.504269\n",
            " 18596/30000: episode: 3101, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3836.667 [2448.000, 5622.000],  loss: 40.273693, mae: 15.553093, mean_q: 37.856155\n",
            " 18602/30000: episode: 3102, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 75.000, mean reward: 12.500 [ 0.000, 25.000], mean action: 3044.500 [709.000, 5456.000],  loss: 68.811348, mae: 14.809056, mean_q: 36.334789\n",
            " 18608/30000: episode: 3103, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2786.167 [1576.000, 3570.000],  loss: 39.006977, mae: 14.443816, mean_q: 35.580654\n",
            " 18614/30000: episode: 3104, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 2807.000 [869.000, 3913.000],  loss: 36.982334, mae: 14.549488, mean_q: 35.855091\n",
            " 18620/30000: episode: 3105, duration: 0.146s, episode steps:   6, steps per second:  41, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3368.333 [709.000, 4954.000],  loss: 51.526852, mae: 15.643548, mean_q: 38.711857\n",
            " 18626/30000: episode: 3106, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1825.167 [709.000, 2455.000],  loss: 35.705257, mae: 14.852768, mean_q: 37.004227\n",
            " 18632/30000: episode: 3107, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3008.167 [1038.000, 4619.000],  loss: 53.982788, mae: 14.989070, mean_q: 36.749683\n",
            " 18638/30000: episode: 3108, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 45.000, mean reward:  7.500 [ 0.000, 20.000], mean action: 2254.833 [663.000, 3712.000],  loss: 38.486423, mae: 14.043193, mean_q: 35.751019\n",
            " 18644/30000: episode: 3109, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2146.667 [363.000, 4213.000],  loss: 33.564667, mae: 13.934143, mean_q: 34.736614\n",
            " 18650/30000: episode: 3110, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2124.000 [411.000, 3103.000],  loss: 18.475798, mae: 14.738205, mean_q: 36.549538\n",
            " 18656/30000: episode: 3111, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3546.833 [1127.000, 5608.000],  loss: 32.535015, mae: 13.688811, mean_q: 35.249252\n",
            " 18662/30000: episode: 3112, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4259.167 [1127.000, 5640.000],  loss: 28.707010, mae: 15.455922, mean_q: 38.056995\n",
            " 18668/30000: episode: 3113, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 2309.833 [363.000, 3370.000],  loss: 33.513790, mae: 14.334091, mean_q: 35.630154\n",
            " 18674/30000: episode: 3114, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3717.333 [859.000, 5656.000],  loss: 28.609131, mae: 14.350583, mean_q: 35.263752\n",
            " 18680/30000: episode: 3115, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 4177.667 [3083.000, 5447.000],  loss: 27.940371, mae: 13.726398, mean_q: 34.289467\n",
            " 18686/30000: episode: 3116, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 1868.167 [709.000, 5602.000],  loss: 39.454540, mae: 15.438969, mean_q: 37.991577\n",
            " 18692/30000: episode: 3117, duration: 0.148s, episode steps:   6, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3015.667 [309.000, 5721.000],  loss: 42.531384, mae: 15.184117, mean_q: 36.677860\n",
            " 18698/30000: episode: 3118, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2974.333 [2074.000, 4497.000],  loss: 35.914368, mae: 15.161346, mean_q: 36.685658\n",
            " 18704/30000: episode: 3119, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2286.667 [145.000, 4256.000],  loss: 33.606388, mae: 13.457887, mean_q: 33.559353\n",
            " 18710/30000: episode: 3120, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3140.667 [733.000, 4479.000],  loss: 49.594830, mae: 14.455731, mean_q: 35.960506\n",
            " 18716/30000: episode: 3121, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2567.500 [238.000, 5717.000],  loss: 31.644470, mae: 14.495679, mean_q: 35.774971\n",
            " 18722/30000: episode: 3122, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4228.500 [3311.000, 4412.000],  loss: 34.764412, mae: 13.801362, mean_q: 34.378464\n",
            " 18728/30000: episode: 3123, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2231.333 [303.000, 4749.000],  loss: 47.907070, mae: 14.283681, mean_q: 35.086891\n",
            " 18734/30000: episode: 3124, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2559.500 [441.000, 5187.000],  loss: 37.940121, mae: 13.519943, mean_q: 34.722645\n",
            " 18740/30000: episode: 3125, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3274.833 [218.000, 5689.000],  loss: 35.900726, mae: 14.084960, mean_q: 35.788513\n",
            " 18746/30000: episode: 3126, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3620.667 [816.000, 5593.000],  loss: 49.483891, mae: 14.325419, mean_q: 35.895550\n",
            " 18752/30000: episode: 3127, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2632.500 [1152.000, 3284.000],  loss: 62.846722, mae: 15.050143, mean_q: 36.622681\n",
            " 18758/30000: episode: 3128, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4123.167 [3083.000, 5281.000],  loss: 29.937454, mae: 14.411415, mean_q: 35.834801\n",
            " 18764/30000: episode: 3129, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3611.833 [2203.000, 5708.000],  loss: 24.698814, mae: 15.616460, mean_q: 38.804737\n",
            " 18770/30000: episode: 3130, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 3101.667 [316.000, 4905.000],  loss: 37.568577, mae: 14.064219, mean_q: 35.870907\n",
            " 18776/30000: episode: 3131, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4071.000 [2455.000, 4797.000],  loss: 42.655621, mae: 14.098164, mean_q: 36.076477\n",
            " 18782/30000: episode: 3132, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 115.000, mean reward: 19.167 [ 0.000, 35.000], mean action: 2791.500 [238.000, 4185.000],  loss: 44.541088, mae: 15.456665, mean_q: 38.987217\n",
            " 18788/30000: episode: 3133, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2764.000 [1495.000, 5680.000],  loss: 35.877014, mae: 15.462369, mean_q: 38.858051\n",
            " 18794/30000: episode: 3134, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2794.667 [2737.000, 3083.000],  loss: 31.092653, mae: 12.912148, mean_q: 33.964336\n",
            " 18800/30000: episode: 3135, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3339.500 [2277.000, 4958.000],  loss: 42.327530, mae: 14.702194, mean_q: 37.511463\n",
            " 18806/30000: episode: 3136, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 10.000], mean action: 2443.667 [709.000, 5433.000],  loss: 46.842518, mae: 14.629585, mean_q: 37.056282\n",
            " 18812/30000: episode: 3137, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2648.667 [1530.000, 3921.000],  loss: 48.900249, mae: 14.263904, mean_q: 35.815426\n",
            " 18818/30000: episode: 3138, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4184.000 [2522.000, 5249.000],  loss: 33.767136, mae: 13.998741, mean_q: 35.268040\n",
            " 18824/30000: episode: 3139, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3198.167 [1145.000, 4444.000],  loss: 32.399166, mae: 14.723809, mean_q: 36.965702\n",
            " 18830/30000: episode: 3140, duration: 0.236s, episode steps:   6, steps per second:  25, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 1970.333 [1622.000, 3712.000],  loss: 29.446081, mae: 13.903934, mean_q: 35.125462\n",
            " 18836/30000: episode: 3141, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2369.500 [1057.000, 3901.000],  loss: 32.446255, mae: 14.555222, mean_q: 36.607510\n",
            " 18842/30000: episode: 3142, duration: 0.240s, episode steps:   6, steps per second:  25, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3592.333 [2448.000, 4868.000],  loss: 36.381893, mae: 14.066559, mean_q: 35.983601\n",
            " 18848/30000: episode: 3143, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 3487.833 [889.000, 4868.000],  loss: 48.505795, mae: 13.489259, mean_q: 34.493755\n",
            " 18854/30000: episode: 3144, duration: 0.245s, episode steps:   6, steps per second:  24, episode reward: 30.000, mean reward:  5.000 [-5.000, 20.000], mean action: 1837.167 [527.000, 3796.000],  loss: 35.492855, mae: 14.694362, mean_q: 36.247486\n",
            " 18860/30000: episode: 3145, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3761.667 [2201.000, 5721.000],  loss: 25.826637, mae: 13.629837, mean_q: 35.186737\n",
            " 18866/30000: episode: 3146, duration: 0.259s, episode steps:   6, steps per second:  23, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 1871.833 [106.000, 5316.000],  loss: 26.252584, mae: 13.537145, mean_q: 34.542305\n",
            " 18872/30000: episode: 3147, duration: 0.257s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3143.667 [996.000, 4868.000],  loss: 31.349554, mae: 15.206105, mean_q: 37.857479\n",
            " 18878/30000: episode: 3148, duration: 0.276s, episode steps:   6, steps per second:  22, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2952.000 [586.000, 4372.000],  loss: 35.032425, mae: 15.753098, mean_q: 39.248447\n",
            " 18884/30000: episode: 3149, duration: 0.260s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3912.333 [1256.000, 4868.000],  loss: 58.957661, mae: 15.077748, mean_q: 37.652939\n",
            " 18890/30000: episode: 3150, duration: 0.276s, episode steps:   6, steps per second:  22, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 4868.000 [4868.000, 4868.000],  loss: 38.490993, mae: 15.666855, mean_q: 39.801880\n",
            " 18896/30000: episode: 3151, duration: 0.231s, episode steps:   6, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4162.167 [2372.000, 5707.000],  loss: 24.086195, mae: 14.679681, mean_q: 37.565655\n",
            " 18902/30000: episode: 3152, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2717.833 [1564.000, 3951.000],  loss: 39.929794, mae: 13.637286, mean_q: 35.230427\n",
            " 18908/30000: episode: 3153, duration: 0.186s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3605.000 [2547.000, 5047.000],  loss: 40.806965, mae: 14.273026, mean_q: 36.849064\n",
            " 18914/30000: episode: 3154, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2801.167 [889.000, 5658.000],  loss: 46.845898, mae: 13.143628, mean_q: 35.405720\n",
            " 18920/30000: episode: 3155, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [-5.000, 15.000], mean action: 3065.667 [145.000, 4868.000],  loss: 34.813747, mae: 14.363011, mean_q: 36.308979\n",
            " 18926/30000: episode: 3156, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2059.833 [634.000, 3299.000],  loss: 42.589855, mae: 13.682336, mean_q: 34.703075\n",
            " 18932/30000: episode: 3157, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 4133.667 [2107.000, 5721.000],  loss: 37.660614, mae: 15.667714, mean_q: 37.970848\n",
            " 18938/30000: episode: 3158, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3292.833 [2210.000, 3901.000],  loss: 24.592484, mae: 13.777180, mean_q: 34.762592\n",
            " 18944/30000: episode: 3159, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2270.833 [224.000, 3784.000],  loss: 44.317123, mae: 14.915268, mean_q: 36.679173\n",
            " 18950/30000: episode: 3160, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3401.167 [648.000, 5717.000],  loss: 37.994633, mae: 14.695361, mean_q: 36.216427\n",
            " 18956/30000: episode: 3161, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 5266.167 [4412.000, 5717.000],  loss: 42.000435, mae: 14.483251, mean_q: 36.275753\n",
            " 18962/30000: episode: 3162, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3082.500 [634.000, 5717.000],  loss: 27.665056, mae: 14.690789, mean_q: 36.611782\n",
            " 18968/30000: episode: 3163, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2584.000 [602.000, 4799.000],  loss: 30.670404, mae: 14.594453, mean_q: 36.136425\n",
            " 18974/30000: episode: 3164, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2687.833 [907.000, 4894.000],  loss: 35.696430, mae: 14.322090, mean_q: 35.364902\n",
            " 18980/30000: episode: 3165, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 3168.667 [1202.000, 5433.000],  loss: 42.856312, mae: 13.469430, mean_q: 33.778126\n",
            " 18986/30000: episode: 3166, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 592.667 [238.000, 2366.000],  loss: 45.589848, mae: 14.560107, mean_q: 36.576118\n",
            " 18992/30000: episode: 3167, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2684.000 [634.000, 3094.000],  loss: 38.920662, mae: 15.345322, mean_q: 36.642872\n",
            " 18998/30000: episode: 3168, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3978.000 [2072.000, 5649.000],  loss: 41.297085, mae: 14.676895, mean_q: 35.958897\n",
            " 19004/30000: episode: 3169, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3595.667 [1617.000, 5387.000],  loss: 41.577274, mae: 14.117900, mean_q: 35.150684\n",
            " 19010/30000: episode: 3170, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2459.000 [404.000, 4901.000],  loss: 32.912518, mae: 14.209786, mean_q: 35.808800\n",
            " 19016/30000: episode: 3171, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3302.333 [1530.000, 4868.000],  loss: 21.874565, mae: 14.140316, mean_q: 35.300060\n",
            " 19022/30000: episode: 3172, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2653.167 [404.000, 5522.000],  loss: 42.849442, mae: 15.005448, mean_q: 36.932117\n",
            " 19028/30000: episode: 3173, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3705.500 [2205.000, 4344.000],  loss: 39.532959, mae: 14.760146, mean_q: 36.211334\n",
            " 19034/30000: episode: 3174, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4110.167 [605.000, 5593.000],  loss: 40.671303, mae: 13.917052, mean_q: 34.282505\n",
            " 19040/30000: episode: 3175, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3186.000 [1322.000, 5449.000],  loss: 38.866531, mae: 12.790241, mean_q: 32.341385\n",
            " 19046/30000: episode: 3176, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3119.000 [663.000, 5367.000],  loss: 36.118443, mae: 14.440417, mean_q: 35.651134\n",
            " 19052/30000: episode: 3177, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3378.000 [12.000, 5606.000],  loss: 54.966663, mae: 14.980445, mean_q: 37.342449\n",
            " 19058/30000: episode: 3178, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 3174.000 [460.000, 4868.000],  loss: 57.241695, mae: 14.899246, mean_q: 37.517612\n",
            " 19064/30000: episode: 3179, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2540.500 [578.000, 4246.000],  loss: 32.416168, mae: 14.183961, mean_q: 35.957138\n",
            " 19070/30000: episode: 3180, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3652.833 [751.000, 5656.000],  loss: 47.152966, mae: 12.764877, mean_q: 32.772137\n",
            " 19076/30000: episode: 3181, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4306.833 [2125.000, 5602.000],  loss: 37.544922, mae: 14.466861, mean_q: 35.220783\n",
            " 19082/30000: episode: 3182, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3507.333 [1509.000, 5168.000],  loss: 40.646816, mae: 14.203042, mean_q: 35.455318\n",
            " 19088/30000: episode: 3183, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2227.667 [178.000, 4185.000],  loss: 31.853081, mae: 15.117306, mean_q: 36.777660\n",
            " 19094/30000: episode: 3184, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3488.000 [1243.000, 5708.000],  loss: 23.218008, mae: 14.661382, mean_q: 36.192303\n",
            " 19100/30000: episode: 3185, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4161.333 [2437.000, 5597.000],  loss: 33.333752, mae: 15.076987, mean_q: 37.394585\n",
            " 19106/30000: episode: 3186, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4195.500 [3299.000, 4858.000],  loss: 52.354748, mae: 14.432144, mean_q: 36.063042\n",
            " 19112/30000: episode: 3187, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3182.667 [393.000, 5742.000],  loss: 49.040577, mae: 14.610646, mean_q: 36.309528\n",
            " 19118/30000: episode: 3188, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3421.000 [404.000, 4619.000],  loss: 28.380623, mae: 14.107422, mean_q: 35.942410\n",
            " 19124/30000: episode: 3189, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1940.500 [915.000, 3299.000],  loss: 21.387390, mae: 14.050858, mean_q: 35.683575\n",
            " 19130/30000: episode: 3190, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3508.333 [2722.000, 4228.000],  loss: 34.386559, mae: 14.360972, mean_q: 34.763199\n",
            " 19136/30000: episode: 3191, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2527.833 [150.000, 5348.000],  loss: 36.989166, mae: 14.529817, mean_q: 35.994274\n",
            " 19142/30000: episode: 3192, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2161.000 [106.000, 4444.000],  loss: 34.356415, mae: 14.748188, mean_q: 36.346836\n",
            " 19148/30000: episode: 3193, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 150.000, mean reward: 25.000 [ 0.000, 35.000], mean action: 2653.167 [238.000, 4185.000],  loss: 39.717289, mae: 15.109604, mean_q: 37.502987\n",
            " 19154/30000: episode: 3194, duration: 0.147s, episode steps:   6, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3388.500 [1509.000, 5016.000],  loss: 28.097540, mae: 13.882463, mean_q: 35.119720\n",
            " 19160/30000: episode: 3195, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4468.833 [2672.000, 5721.000],  loss: 27.334961, mae: 13.248797, mean_q: 33.614948\n",
            " 19166/30000: episode: 3196, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3457.667 [2205.000, 5576.000],  loss: 44.030197, mae: 13.671538, mean_q: 34.156902\n",
            " 19172/30000: episode: 3197, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  5.000], mean action: 3007.167 [2060.000, 5486.000],  loss: 32.720631, mae: 13.785016, mean_q: 34.966671\n",
            " 19178/30000: episode: 3198, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2424.167 [18.000, 5689.000],  loss: 35.821404, mae: 14.944964, mean_q: 37.053562\n",
            " 19184/30000: episode: 3199, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3556.833 [1940.000, 5351.000],  loss: 50.504040, mae: 14.314702, mean_q: 36.160538\n",
            " 19190/30000: episode: 3200, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2779.000 [238.000, 4954.000],  loss: 31.776712, mae: 13.076565, mean_q: 33.922596\n",
            " 19196/30000: episode: 3201, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3591.500 [43.000, 4900.000],  loss: 45.560078, mae: 14.184815, mean_q: 35.765137\n",
            " 19202/30000: episode: 3202, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4077.000 [328.000, 5486.000],  loss: 62.086411, mae: 15.420970, mean_q: 38.193569\n",
            " 19208/30000: episode: 3203, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3549.167 [1650.000, 5387.000],  loss: 29.067011, mae: 13.551964, mean_q: 34.857853\n",
            " 19214/30000: episode: 3204, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 45.000, mean reward:  7.500 [ 0.000, 25.000], mean action: 2830.000 [1178.000, 5532.000],  loss: 42.551403, mae: 15.008775, mean_q: 37.482159\n",
            " 19220/30000: episode: 3205, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4103.167 [1979.000, 5622.000],  loss: 54.716503, mae: 13.850506, mean_q: 35.260826\n",
            " 19226/30000: episode: 3206, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3147.167 [1833.000, 5395.000],  loss: 26.047653, mae: 13.991360, mean_q: 35.595692\n",
            " 19232/30000: episode: 3207, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 3608.833 [1057.000, 5387.000],  loss: 59.967937, mae: 14.685287, mean_q: 37.147873\n",
            " 19238/30000: episode: 3208, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3165.833 [1202.000, 4191.000],  loss: 49.070774, mae: 15.158283, mean_q: 38.661034\n",
            " 19244/30000: episode: 3209, duration: 0.183s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3226.333 [2133.000, 4228.000],  loss: 38.793945, mae: 13.274579, mean_q: 34.554760\n",
            " 19250/30000: episode: 3210, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2230.333 [106.000, 4717.000],  loss: 53.935658, mae: 14.106748, mean_q: 35.771374\n",
            " 19256/30000: episode: 3211, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3473.500 [1617.000, 4134.000],  loss: 54.180359, mae: 12.434417, mean_q: 32.550877\n",
            " 19262/30000: episode: 3212, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3513.000 [1988.000, 5387.000],  loss: 37.918049, mae: 13.291667, mean_q: 33.624538\n",
            " 19268/30000: episode: 3213, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2762.000 [2558.000, 3393.000],  loss: 27.141977, mae: 12.973522, mean_q: 33.392780\n",
            " 19274/30000: episode: 3214, duration: 0.270s, episode steps:   6, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3695.833 [1617.000, 5736.000],  loss: 40.511505, mae: 14.191869, mean_q: 35.229660\n",
            " 19280/30000: episode: 3215, duration: 0.265s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2948.000 [1020.000, 4834.000],  loss: 31.072607, mae: 14.124931, mean_q: 35.964165\n",
            " 19286/30000: episode: 3216, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2260.333 [75.000, 4717.000],  loss: 46.349903, mae: 13.929879, mean_q: 35.018604\n",
            " 19292/30000: episode: 3217, duration: 0.267s, episode steps:   6, steps per second:  22, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 3170.167 [1523.000, 4804.000],  loss: 17.599995, mae: 13.854347, mean_q: 35.247341\n",
            " 19298/30000: episode: 3218, duration: 0.259s, episode steps:   6, steps per second:  23, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3970.500 [1202.000, 5656.000],  loss: 32.961292, mae: 14.289762, mean_q: 35.927418\n",
            " 19304/30000: episode: 3219, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2642.333 [1076.000, 5411.000],  loss: 39.555035, mae: 14.846503, mean_q: 37.536995\n",
            " 19310/30000: episode: 3220, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward: 15.000, mean reward:  2.500 [-5.000, 15.000], mean action: 2764.167 [1596.000, 4616.000],  loss: 43.258381, mae: 14.261546, mean_q: 35.531002\n",
            " 19316/30000: episode: 3221, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 3162.000 [1495.000, 4808.000],  loss: 41.273266, mae: 14.946288, mean_q: 36.654068\n",
            " 19322/30000: episode: 3222, duration: 0.245s, episode steps:   6, steps per second:  24, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2551.167 [634.000, 3921.000],  loss: 41.820187, mae: 14.471721, mean_q: 37.151230\n",
            " 19328/30000: episode: 3223, duration: 0.229s, episode steps:   6, steps per second:  26, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 4028.500 [1074.000, 5466.000],  loss: 35.933426, mae: 15.252297, mean_q: 37.206951\n",
            " 19334/30000: episode: 3224, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3650.167 [1722.000, 5387.000],  loss: 50.427418, mae: 15.279186, mean_q: 37.955124\n",
            " 19340/30000: episode: 3225, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4099.333 [2485.000, 5620.000],  loss: 47.218170, mae: 15.461553, mean_q: 37.994396\n",
            " 19346/30000: episode: 3226, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3895.000 [2188.000, 5316.000],  loss: 37.632778, mae: 13.704141, mean_q: 34.694092\n",
            " 19352/30000: episode: 3227, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3919.833 [3178.000, 4782.000],  loss: 41.531250, mae: 13.999108, mean_q: 35.042042\n",
            " 19358/30000: episode: 3228, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2781.500 [491.000, 3831.000],  loss: 23.494268, mae: 13.749875, mean_q: 34.244488\n",
            " 19364/30000: episode: 3229, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2728.167 [996.000, 5538.000],  loss: 46.270245, mae: 14.790402, mean_q: 37.146694\n",
            " 19370/30000: episode: 3230, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 2342.833 [72.000, 3221.000],  loss: 42.760311, mae: 14.071246, mean_q: 35.454914\n",
            " 19376/30000: episode: 3231, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 40.000, mean reward:  6.667 [ 0.000, 40.000], mean action: 3733.833 [917.000, 5622.000],  loss: 43.800060, mae: 12.668446, mean_q: 32.138577\n",
            " 19382/30000: episode: 3232, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2156.167 [491.000, 5163.000],  loss: 39.275730, mae: 13.485897, mean_q: 33.116459\n",
            " 19388/30000: episode: 3233, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4262.167 [2560.000, 5538.000],  loss: 39.995445, mae: 14.316544, mean_q: 35.419907\n",
            " 19394/30000: episode: 3234, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2298.833 [1147.000, 3656.000],  loss: 41.890385, mae: 13.848405, mean_q: 34.561180\n",
            " 19400/30000: episode: 3235, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2988.167 [323.000, 5680.000],  loss: 28.166046, mae: 14.320096, mean_q: 35.734570\n",
            " 19406/30000: episode: 3236, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2157.167 [527.000, 5538.000],  loss: 24.765997, mae: 15.001573, mean_q: 37.155106\n",
            " 19412/30000: episode: 3237, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3031.667 [238.000, 5538.000],  loss: 51.488010, mae: 15.539691, mean_q: 39.465939\n",
            " 19418/30000: episode: 3238, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 1735.000 [85.000, 4884.000],  loss: 35.965992, mae: 14.113248, mean_q: 35.997730\n",
            " 19424/30000: episode: 3239, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3670.000 [238.000, 5736.000],  loss: 39.872131, mae: 14.306188, mean_q: 35.571339\n",
            " 19430/30000: episode: 3240, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2271.333 [238.000, 4884.000],  loss: 23.690208, mae: 12.932668, mean_q: 33.114182\n",
            " 19436/30000: episode: 3241, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 3056.167 [238.000, 4884.000],  loss: 43.026615, mae: 13.145593, mean_q: 33.440697\n",
            " 19442/30000: episode: 3242, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3266.167 [1484.000, 4391.000],  loss: 45.286190, mae: 14.967323, mean_q: 37.720642\n",
            " 19448/30000: episode: 3243, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3506.333 [1596.000, 5474.000],  loss: 38.205235, mae: 13.514417, mean_q: 34.317295\n",
            " 19454/30000: episode: 3244, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3207.000 [895.000, 4884.000],  loss: 33.890202, mae: 14.571599, mean_q: 36.194054\n",
            " 19460/30000: episode: 3245, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2411.000 [915.000, 5283.000],  loss: 51.664074, mae: 14.959503, mean_q: 37.279343\n",
            " 19466/30000: episode: 3246, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4338.333 [1260.000, 5367.000],  loss: 41.007915, mae: 15.321308, mean_q: 38.482697\n",
            " 19472/30000: episode: 3247, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3552.167 [1555.000, 5233.000],  loss: 45.069302, mae: 14.093078, mean_q: 35.655552\n",
            " 19478/30000: episode: 3248, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1785.500 [669.000, 2623.000],  loss: 41.037811, mae: 14.158885, mean_q: 36.251949\n",
            " 19484/30000: episode: 3249, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3914.667 [3221.000, 5708.000],  loss: 29.268755, mae: 14.291183, mean_q: 36.826172\n",
            " 19490/30000: episode: 3250, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2938.333 [282.000, 4884.000],  loss: 38.606026, mae: 14.920947, mean_q: 37.563499\n",
            " 19496/30000: episode: 3251, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3609.500 [709.000, 4412.000],  loss: 43.405727, mae: 14.828606, mean_q: 36.980301\n",
            " 19502/30000: episode: 3252, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2910.833 [87.000, 4884.000],  loss: 46.195587, mae: 13.962584, mean_q: 34.473461\n",
            " 19508/30000: episode: 3253, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2781.000 [1253.000, 4612.000],  loss: 55.206882, mae: 13.151683, mean_q: 33.501270\n",
            " 19514/30000: episode: 3254, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward: 140.000, mean reward: 23.333 [ 0.000, 35.000], mean action: 3369.000 [238.000, 4185.000],  loss: 50.544239, mae: 13.956897, mean_q: 34.644230\n",
            " 19520/30000: episode: 3255, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2645.833 [190.000, 5706.000],  loss: 46.458954, mae: 14.838554, mean_q: 36.844875\n",
            " 19526/30000: episode: 3256, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 30.000, mean reward:  5.000 [ 0.000, 20.000], mean action: 3024.833 [709.000, 4819.000],  loss: 37.207077, mae: 14.504055, mean_q: 35.569393\n",
            " 19532/30000: episode: 3257, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3106.333 [331.000, 4884.000],  loss: 40.342148, mae: 14.532914, mean_q: 35.374374\n",
            " 19538/30000: episode: 3258, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3172.167 [2201.000, 5061.000],  loss: 30.577087, mae: 14.299915, mean_q: 36.162296\n",
            " 19544/30000: episode: 3259, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3643.667 [2357.000, 4566.000],  loss: 33.026508, mae: 12.924352, mean_q: 33.325573\n",
            " 19550/30000: episode: 3260, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1936.667 [393.000, 2920.000],  loss: 50.436161, mae: 14.165504, mean_q: 35.703590\n",
            " 19556/30000: episode: 3261, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4022.667 [2989.000, 4884.000],  loss: 44.033054, mae: 13.457901, mean_q: 34.267899\n",
            " 19562/30000: episode: 3262, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3950.500 [2623.000, 5061.000],  loss: 35.575600, mae: 13.622292, mean_q: 33.909901\n",
            " 19568/30000: episode: 3263, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1311.833 [323.000, 3074.000],  loss: 33.422913, mae: 14.304916, mean_q: 35.411243\n",
            " 19574/30000: episode: 3264, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 3563.667 [1693.000, 5750.000],  loss: 36.033417, mae: 14.861458, mean_q: 36.689754\n",
            " 19580/30000: episode: 3265, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1506.833 [525.000, 3770.000],  loss: 27.138977, mae: 13.996391, mean_q: 35.631733\n",
            " 19586/30000: episode: 3266, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3624.500 [2197.000, 5283.000],  loss: 38.344807, mae: 14.853503, mean_q: 36.912624\n",
            " 19592/30000: episode: 3267, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3994.500 [2441.000, 5061.000],  loss: 39.898842, mae: 15.291698, mean_q: 36.922321\n",
            " 19598/30000: episode: 3268, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 60.000, mean reward: 10.000 [ 0.000, 35.000], mean action: 2662.000 [946.000, 5679.000],  loss: 45.712116, mae: 14.722814, mean_q: 36.376686\n",
            " 19604/30000: episode: 3269, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2297.500 [729.000, 3669.000],  loss: 38.997944, mae: 13.926163, mean_q: 34.598530\n",
            " 19610/30000: episode: 3270, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3765.667 [2936.000, 4372.000],  loss: 29.970566, mae: 14.050072, mean_q: 35.001972\n",
            " 19616/30000: episode: 3271, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3833.167 [1981.000, 5163.000],  loss: 53.734509, mae: 14.266658, mean_q: 35.536282\n",
            " 19622/30000: episode: 3272, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3309.667 [1530.000, 4632.000],  loss: 28.897287, mae: 13.107863, mean_q: 33.938660\n",
            " 19628/30000: episode: 3273, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  5.000], mean action: 2076.833 [990.000, 4397.000],  loss: 34.098904, mae: 13.648583, mean_q: 34.370056\n",
            " 19634/30000: episode: 3274, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2447.167 [224.000, 4397.000],  loss: 30.084747, mae: 13.263621, mean_q: 34.269718\n",
            " 19640/30000: episode: 3275, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3323.167 [605.000, 5486.000],  loss: 39.064137, mae: 14.144534, mean_q: 36.077259\n",
            " 19646/30000: episode: 3276, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4328.000 [553.000, 5687.000],  loss: 42.243908, mae: 13.481468, mean_q: 34.588268\n",
            " 19652/30000: episode: 3277, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4544.667 [4397.000, 5283.000],  loss: 40.834995, mae: 14.616361, mean_q: 36.582539\n",
            " 19658/30000: episode: 3278, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 35.000, mean reward:  5.833 [-5.000, 25.000], mean action: 2639.833 [1596.000, 4397.000],  loss: 44.271687, mae: 14.749423, mean_q: 37.431110\n",
            " 19664/30000: episode: 3279, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3460.000 [2390.000, 4464.000],  loss: 27.016912, mae: 13.489579, mean_q: 34.982254\n",
            " 19670/30000: episode: 3280, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 4544.667 [4397.000, 5283.000],  loss: 55.988842, mae: 13.788628, mean_q: 34.730068\n",
            " 19676/30000: episode: 3281, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4353.167 [3712.000, 4819.000],  loss: 45.820053, mae: 14.154747, mean_q: 36.007351\n",
            " 19682/30000: episode: 3282, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2746.667 [527.000, 4372.000],  loss: 51.217133, mae: 14.381917, mean_q: 36.876553\n",
            " 19688/30000: episode: 3283, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4139.167 [1558.000, 5007.000],  loss: 43.705677, mae: 14.434606, mean_q: 36.588184\n",
            " 19694/30000: episode: 3284, duration: 0.206s, episode steps:   6, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3167.833 [328.000, 5283.000],  loss: 43.449162, mae: 14.045058, mean_q: 35.172283\n",
            " 19700/30000: episode: 3285, duration: 0.239s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3874.833 [1924.000, 5736.000],  loss: 42.431492, mae: 13.042239, mean_q: 33.365833\n",
            " 19706/30000: episode: 3286, duration: 0.239s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3248.667 [1305.000, 4816.000],  loss: 54.699680, mae: 13.550411, mean_q: 34.759647\n",
            " 19712/30000: episode: 3287, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4226.000 [3433.000, 5283.000],  loss: 24.446299, mae: 12.408299, mean_q: 32.178009\n",
            " 19718/30000: episode: 3288, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3762.167 [1857.000, 5259.000],  loss: 41.988720, mae: 13.995669, mean_q: 35.762890\n",
            " 19724/30000: episode: 3289, duration: 0.236s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2872.333 [1530.000, 5000.000],  loss: 33.068470, mae: 12.978698, mean_q: 33.970348\n",
            " 19730/30000: episode: 3290, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2291.333 [1057.000, 5387.000],  loss: 34.990742, mae: 14.023524, mean_q: 35.992069\n",
            " 19736/30000: episode: 3291, duration: 0.237s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2895.500 [866.000, 5387.000],  loss: 61.726151, mae: 13.957176, mean_q: 36.236328\n",
            " 19742/30000: episode: 3292, duration: 0.265s, episode steps:   6, steps per second:  23, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 5077.167 [3528.000, 5387.000],  loss: 37.581173, mae: 13.910415, mean_q: 35.947289\n",
            " 19748/30000: episode: 3293, duration: 0.257s, episode steps:   6, steps per second:  23, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1849.833 [1057.000, 5387.000],  loss: 56.649776, mae: 14.608548, mean_q: 37.153522\n",
            " 19754/30000: episode: 3294, duration: 0.238s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3734.333 [648.000, 5554.000],  loss: 36.777267, mae: 14.231075, mean_q: 36.416050\n",
            " 19760/30000: episode: 3295, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3236.167 [553.000, 5387.000],  loss: 47.519970, mae: 14.185084, mean_q: 37.077106\n",
            " 19766/30000: episode: 3296, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2265.333 [42.000, 5387.000],  loss: 48.587872, mae: 13.977445, mean_q: 37.203648\n",
            " 19772/30000: episode: 3297, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2815.000 [663.000, 5387.000],  loss: 40.365429, mae: 14.414723, mean_q: 37.158630\n",
            " 19778/30000: episode: 3298, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3294.833 [1521.000, 5387.000],  loss: 37.435879, mae: 14.217774, mean_q: 36.093933\n",
            " 19784/30000: episode: 3299, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4921.500 [3669.000, 5427.000],  loss: 57.381149, mae: 14.048424, mean_q: 36.916157\n",
            " 19790/30000: episode: 3300, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2739.333 [12.000, 5387.000],  loss: 32.471706, mae: 13.384336, mean_q: 34.776993\n",
            " 19796/30000: episode: 3301, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 5387.000 [5387.000, 5387.000],  loss: 28.980711, mae: 14.860148, mean_q: 37.489285\n",
            " 19802/30000: episode: 3302, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 45.000, mean reward:  7.500 [ 0.000, 25.000], mean action: 3419.333 [1530.000, 5283.000],  loss: 51.970123, mae: 14.092240, mean_q: 36.174194\n",
            " 19808/30000: episode: 3303, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4624.833 [3528.000, 5367.000],  loss: 51.358883, mae: 14.236866, mean_q: 35.526707\n",
            " 19814/30000: episode: 3304, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3696.167 [2312.000, 5488.000],  loss: 38.339310, mae: 14.005090, mean_q: 34.999737\n",
            " 19820/30000: episode: 3305, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 30.000, mean reward:  5.000 [ 0.000, 30.000], mean action: 4042.333 [2698.000, 5750.000],  loss: 28.383425, mae: 12.963832, mean_q: 32.300171\n",
            " 19826/30000: episode: 3306, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2027.333 [158.000, 4316.000],  loss: 46.210201, mae: 14.359676, mean_q: 35.218662\n",
            " 19832/30000: episode: 3307, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3318.333 [590.000, 5252.000],  loss: 35.975063, mae: 15.429469, mean_q: 37.299366\n",
            " 19838/30000: episode: 3308, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3349.500 [852.000, 5007.000],  loss: 34.125172, mae: 15.063884, mean_q: 36.333073\n",
            " 19844/30000: episode: 3309, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3161.000 [140.000, 5005.000],  loss: 46.896214, mae: 15.233689, mean_q: 37.628506\n",
            " 19850/30000: episode: 3310, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3046.833 [1484.000, 5750.000],  loss: 42.514622, mae: 15.104321, mean_q: 37.242229\n",
            " 19856/30000: episode: 3311, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4005.333 [1183.000, 5538.000],  loss: 60.443607, mae: 13.248901, mean_q: 33.523609\n",
            " 19862/30000: episode: 3312, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3589.000 [590.000, 5597.000],  loss: 42.330803, mae: 12.984756, mean_q: 32.531635\n",
            " 19868/30000: episode: 3313, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3532.167 [846.000, 4847.000],  loss: 46.997906, mae: 13.276216, mean_q: 33.349907\n",
            " 19874/30000: episode: 3314, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2449.167 [2448.000, 2455.000],  loss: 24.777222, mae: 15.441426, mean_q: 37.438805\n",
            " 19880/30000: episode: 3315, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4017.167 [2577.000, 5658.000],  loss: 41.718884, mae: 14.606602, mean_q: 36.063599\n",
            " 19886/30000: episode: 3316, duration: 0.186s, episode steps:   6, steps per second:  32, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 3242.333 [329.000, 5561.000],  loss: 49.191116, mae: 14.177105, mean_q: 34.850643\n",
            " 19892/30000: episode: 3317, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3777.833 [2188.000, 5099.000],  loss: 26.304335, mae: 13.059115, mean_q: 32.500484\n",
            " 19898/30000: episode: 3318, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3514.833 [2560.000, 5750.000],  loss: 34.778530, mae: 14.578481, mean_q: 36.595795\n",
            " 19904/30000: episode: 3319, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3632.333 [587.000, 5326.000],  loss: 46.459885, mae: 13.846946, mean_q: 34.981094\n",
            " 19910/30000: episode: 3320, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [-5.000, 15.000], mean action: 3339.500 [756.000, 5722.000],  loss: 40.028683, mae: 14.864608, mean_q: 36.901295\n",
            " 19916/30000: episode: 3321, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3980.667 [756.000, 5597.000],  loss: 29.948547, mae: 14.783577, mean_q: 36.492191\n",
            " 19922/30000: episode: 3322, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3264.500 [1484.000, 5099.000],  loss: 48.441784, mae: 14.735821, mean_q: 36.706028\n",
            " 19928/30000: episode: 3323, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 4156.500 [1530.000, 5532.000],  loss: 24.014570, mae: 12.614934, mean_q: 33.426697\n",
            " 19934/30000: episode: 3324, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2873.000 [329.000, 4782.000],  loss: 47.256199, mae: 13.887978, mean_q: 34.770287\n",
            " 19940/30000: episode: 3325, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3788.167 [607.000, 5605.000],  loss: 43.086853, mae: 14.608922, mean_q: 36.057201\n",
            " 19946/30000: episode: 3326, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 10.000], mean action: 3275.333 [870.000, 4464.000],  loss: 46.924137, mae: 15.173951, mean_q: 37.337925\n",
            " 19952/30000: episode: 3327, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 2311.667 [733.000, 4552.000],  loss: 54.444630, mae: 14.565357, mean_q: 35.995712\n",
            " 19958/30000: episode: 3328, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2475.167 [329.000, 5164.000],  loss: 29.988800, mae: 14.425826, mean_q: 35.103367\n",
            " 19964/30000: episode: 3329, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3084.500 [522.000, 5597.000],  loss: 50.993145, mae: 13.964318, mean_q: 34.307716\n",
            " 19970/30000: episode: 3330, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2478.000 [1484.000, 5099.000],  loss: 28.032982, mae: 13.870999, mean_q: 35.120235\n",
            " 19976/30000: episode: 3331, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 40.000, mean reward:  6.667 [ 0.000, 20.000], mean action: 3177.833 [1202.000, 5072.000],  loss: 45.788830, mae: 14.730493, mean_q: 36.678867\n",
            " 19982/30000: episode: 3332, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3651.000 [2275.000, 5703.000],  loss: 36.714695, mae: 15.229501, mean_q: 36.917088\n",
            " 19988/30000: episode: 3333, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 3078.833 [1596.000, 5099.000],  loss: 33.128086, mae: 14.660752, mean_q: 36.020370\n",
            " 19994/30000: episode: 3334, duration: 0.187s, episode steps:   6, steps per second:  32, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3817.667 [803.000, 5099.000],  loss: 48.507397, mae: 13.604289, mean_q: 34.444962\n",
            " 20000/30000: episode: 3335, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2960.667 [205.000, 5211.000],  loss: 40.378593, mae: 13.645339, mean_q: 34.400429\n",
            " 20006/30000: episode: 3336, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2809.667 [461.000, 5433.000],  loss: 73.887779, mae: 14.259633, mean_q: 34.911327\n",
            " 20012/30000: episode: 3337, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3035.500 [1484.000, 5249.000],  loss: 46.273251, mae: 13.579904, mean_q: 34.408688\n",
            " 20018/30000: episode: 3338, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1474.000 [425.000, 3510.000],  loss: 51.560215, mae: 16.624044, mean_q: 40.036236\n",
            " 20024/30000: episode: 3339, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2935.167 [534.000, 5099.000],  loss: 45.080067, mae: 15.009609, mean_q: 37.595562\n",
            " 20030/30000: episode: 3340, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 3761.500 [756.000, 5259.000],  loss: 43.920460, mae: 14.884181, mean_q: 36.638485\n",
            " 20036/30000: episode: 3341, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3411.500 [590.000, 5099.000],  loss: 71.323677, mae: 13.347251, mean_q: 33.554123\n",
            " 20042/30000: episode: 3342, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2844.167 [312.000, 4681.000],  loss: 37.219913, mae: 14.313519, mean_q: 35.286484\n",
            " 20048/30000: episode: 3343, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2325.000 [854.000, 4392.000],  loss: 52.167736, mae: 12.482285, mean_q: 32.062809\n",
            " 20054/30000: episode: 3344, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3339.667 [756.000, 4412.000],  loss: 45.881855, mae: 13.002315, mean_q: 33.486050\n",
            " 20060/30000: episode: 3345, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3109.333 [2201.000, 4256.000],  loss: 74.202522, mae: 15.180043, mean_q: 37.373905\n",
            " 20066/30000: episode: 3346, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2765.500 [1772.000, 3972.000],  loss: 66.772446, mae: 15.457049, mean_q: 38.745907\n",
            " 20072/30000: episode: 3347, duration: 0.146s, episode steps:   6, steps per second:  41, episode reward: 30.000, mean reward:  5.000 [ 0.000, 25.000], mean action: 2257.833 [756.000, 3587.000],  loss: 49.265911, mae: 15.529500, mean_q: 39.063244\n",
            " 20078/30000: episode: 3348, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2327.500 [1202.000, 3712.000],  loss: 76.140434, mae: 14.952469, mean_q: 36.983822\n",
            " 20084/30000: episode: 3349, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2727.333 [910.000, 3712.000],  loss: 66.437431, mae: 14.578064, mean_q: 36.442829\n",
            " 20090/30000: episode: 3350, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1741.333 [756.000, 3712.000],  loss: 70.890755, mae: 13.707507, mean_q: 36.097088\n",
            " 20096/30000: episode: 3351, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 3198.333 [1596.000, 5395.000],  loss: 47.524372, mae: 16.062599, mean_q: 39.718273\n",
            " 20102/30000: episode: 3352, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: 30.000, mean reward:  5.000 [-5.000, 25.000], mean action: 2063.000 [756.000, 2994.000],  loss: 63.458889, mae: 13.424516, mean_q: 34.988033\n",
            " 20108/30000: episode: 3353, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 3404.667 [159.000, 5062.000],  loss: 43.449768, mae: 14.175790, mean_q: 36.643177\n",
            " 20114/30000: episode: 3354, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3393.833 [756.000, 5652.000],  loss: 64.175568, mae: 14.705776, mean_q: 37.608765\n",
            " 20120/30000: episode: 3355, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3672.833 [1516.000, 5649.000],  loss: 56.170528, mae: 13.480548, mean_q: 33.756878\n",
            " 20126/30000: episode: 3356, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3477.667 [1596.000, 5652.000],  loss: 56.061169, mae: 13.612693, mean_q: 34.577579\n",
            " 20132/30000: episode: 3357, duration: 0.187s, episode steps:   6, steps per second:  32, episode reward: 10.000, mean reward:  1.667 [-5.000, 20.000], mean action: 2181.333 [1202.000, 3712.000],  loss: 58.572449, mae: 15.457044, mean_q: 37.313339\n",
            " 20138/30000: episode: 3358, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3251.333 [756.000, 5703.000],  loss: 31.069399, mae: 14.605498, mean_q: 36.643066\n",
            " 20144/30000: episode: 3359, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3567.833 [1736.000, 5624.000],  loss: 56.457912, mae: 14.928176, mean_q: 36.708721\n",
            " 20150/30000: episode: 3360, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1697.500 [756.000, 2818.000],  loss: 32.189205, mae: 13.688470, mean_q: 34.836761\n",
            " 20156/30000: episode: 3361, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2733.167 [329.000, 4970.000],  loss: 64.014748, mae: 13.972539, mean_q: 35.479591\n",
            " 20162/30000: episode: 3362, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward: 15.000, mean reward:  2.500 [-5.000, 15.000], mean action: 2641.000 [674.000, 4683.000],  loss: 48.009968, mae: 13.442912, mean_q: 33.654835\n",
            " 20168/30000: episode: 3363, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 2642.833 [161.000, 5626.000],  loss: 59.685963, mae: 14.006816, mean_q: 35.917793\n",
            " 20174/30000: episode: 3364, duration: 0.231s, episode steps:   6, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1864.667 [1102.000, 3236.000],  loss: 45.017635, mae: 15.740741, mean_q: 39.647129\n",
            " 20180/30000: episode: 3365, duration: 0.267s, episode steps:   6, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3017.333 [1212.000, 5334.000],  loss: 90.938301, mae: 15.052878, mean_q: 37.659794\n",
            " 20186/30000: episode: 3366, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2314.333 [148.000, 4728.000],  loss: 49.465576, mae: 14.724683, mean_q: 36.603546\n",
            " 20192/30000: episode: 3367, duration: 0.257s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [-5.000, 10.000], mean action: 2054.167 [1484.000, 2900.000],  loss: 53.754169, mae: 15.622624, mean_q: 37.596798\n",
            " 20198/30000: episode: 3368, duration: 0.261s, episode steps:   6, steps per second:  23, episode reward: 20.000, mean reward:  3.333 [ 0.000, 10.000], mean action: 3402.667 [2375.000, 4391.000],  loss: 43.416515, mae: 13.899587, mean_q: 35.415104\n",
            " 20204/30000: episode: 3369, duration: 0.197s, episode steps:   6, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2758.000 [1439.000, 4804.000],  loss: 46.887253, mae: 13.366328, mean_q: 34.625641\n",
            " 20210/30000: episode: 3370, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2642.833 [148.000, 5411.000],  loss: 49.354431, mae: 14.922291, mean_q: 36.961605\n",
            " 20216/30000: episode: 3371, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 4295.000 [2297.000, 5658.000],  loss: 52.278698, mae: 14.809726, mean_q: 37.353283\n",
            " 20222/30000: episode: 3372, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4192.167 [3132.000, 5703.000],  loss: 46.673340, mae: 14.459126, mean_q: 36.847988\n",
            " 20228/30000: episode: 3373, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2264.500 [329.000, 3712.000],  loss: 59.162907, mae: 13.375478, mean_q: 34.208179\n",
            " 20234/30000: episode: 3374, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2436.000 [1202.000, 4721.000],  loss: 46.381695, mae: 14.224094, mean_q: 35.858070\n",
            " 20240/30000: episode: 3375, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2400.667 [230.000, 5652.000],  loss: 43.364147, mae: 14.352474, mean_q: 36.251904\n",
            " 20246/30000: episode: 3376, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3790.667 [2670.000, 5163.000],  loss: 49.220150, mae: 14.385700, mean_q: 36.410976\n",
            " 20252/30000: episode: 3377, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2356.667 [183.000, 4620.000],  loss: 54.862152, mae: 15.523906, mean_q: 39.090801\n",
            " 20258/30000: episode: 3378, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2618.333 [238.000, 5612.000],  loss: 59.909321, mae: 16.101418, mean_q: 39.893837\n",
            " 20264/30000: episode: 3379, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3514.667 [2948.000, 5612.000],  loss: 43.543957, mae: 12.491203, mean_q: 32.637802\n",
            " 20270/30000: episode: 3380, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2177.667 [363.000, 3132.000],  loss: 46.617901, mae: 14.153084, mean_q: 35.782619\n",
            " 20276/30000: episode: 3381, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3467.500 [1710.000, 5005.000],  loss: 36.533283, mae: 14.237107, mean_q: 36.380558\n",
            " 20282/30000: episode: 3382, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3135.167 [569.000, 4852.000],  loss: 62.518375, mae: 14.824653, mean_q: 38.046757\n",
            " 20288/30000: episode: 3383, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2962.500 [2560.000, 4403.000],  loss: 46.892506, mae: 13.646846, mean_q: 35.079971\n",
            " 20294/30000: episode: 3384, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3196.167 [282.000, 5129.000],  loss: 39.752373, mae: 14.704506, mean_q: 36.277378\n",
            " 20300/30000: episode: 3385, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 3562.000 [1596.000, 5612.000],  loss: 37.091702, mae: 13.634509, mean_q: 35.293678\n",
            " 20306/30000: episode: 3386, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3359.667 [1905.000, 4477.000],  loss: 63.892590, mae: 14.001843, mean_q: 36.129147\n",
            " 20312/30000: episode: 3387, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4099.000 [1530.000, 5750.000],  loss: 35.355854, mae: 14.035272, mean_q: 34.959087\n",
            " 20318/30000: episode: 3388, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 2528.167 [436.000, 4632.000],  loss: 53.196453, mae: 13.745938, mean_q: 35.633461\n",
            " 20324/30000: episode: 3389, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2922.000 [1260.000, 4804.000],  loss: 62.798248, mae: 14.434730, mean_q: 36.792328\n",
            " 20330/30000: episode: 3390, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3766.833 [2555.000, 5670.000],  loss: 62.641144, mae: 14.468659, mean_q: 36.430248\n",
            " 20336/30000: episode: 3391, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3695.333 [1035.000, 5602.000],  loss: 62.072453, mae: 15.537219, mean_q: 38.351856\n",
            " 20342/30000: episode: 3392, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3269.333 [2595.000, 4493.000],  loss: 51.058228, mae: 14.129004, mean_q: 35.482426\n",
            " 20348/30000: episode: 3393, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2507.667 [507.000, 4379.000],  loss: 50.704346, mae: 14.290645, mean_q: 35.897339\n",
            " 20354/30000: episode: 3394, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3694.333 [329.000, 5061.000],  loss: 53.011875, mae: 15.423164, mean_q: 38.673840\n",
            " 20360/30000: episode: 3395, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3542.000 [1109.000, 5718.000],  loss: 58.806622, mae: 14.896111, mean_q: 37.716080\n",
            " 20366/30000: episode: 3396, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2809.167 [1059.000, 3696.000],  loss: 41.548706, mae: 15.121384, mean_q: 37.554985\n",
            " 20372/30000: episode: 3397, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: 40.000, mean reward:  6.667 [-5.000, 25.000], mean action: 3227.333 [1596.000, 4795.000],  loss: 73.645386, mae: 15.401405, mean_q: 38.255787\n",
            " 20378/30000: episode: 3398, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3931.833 [946.000, 5718.000],  loss: 44.524845, mae: 14.703037, mean_q: 36.593658\n",
            " 20384/30000: episode: 3399, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2359.167 [634.000, 4632.000],  loss: 28.975784, mae: 14.095464, mean_q: 35.134563\n",
            " 20390/30000: episode: 3400, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2180.333 [791.000, 4464.000],  loss: 45.264668, mae: 14.552152, mean_q: 36.021992\n",
            " 20396/30000: episode: 3401, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 45.000, mean reward:  7.500 [ 0.000, 25.000], mean action: 3060.667 [1596.000, 5188.000],  loss: 46.645206, mae: 14.218682, mean_q: 35.713623\n",
            " 20402/30000: episode: 3402, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4058.167 [2823.000, 4632.000],  loss: 54.097260, mae: 14.024879, mean_q: 35.341656\n",
            " 20408/30000: episode: 3403, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3719.167 [1617.000, 4632.000],  loss: 57.862728, mae: 14.906526, mean_q: 36.921860\n",
            " 20414/30000: episode: 3404, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2686.667 [149.000, 4749.000],  loss: 69.000023, mae: 14.025456, mean_q: 35.610088\n",
            " 20420/30000: episode: 3405, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3040.667 [1305.000, 5360.000],  loss: 49.010914, mae: 13.885762, mean_q: 35.513103\n",
            " 20426/30000: episode: 3406, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3323.167 [1419.000, 4496.000],  loss: 85.810730, mae: 14.157677, mean_q: 35.023891\n",
            " 20432/30000: episode: 3407, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3986.500 [2984.000, 5597.000],  loss: 48.102600, mae: 13.457614, mean_q: 34.343777\n",
            " 20438/30000: episode: 3408, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2469.500 [507.000, 5597.000],  loss: 63.402161, mae: 13.924171, mean_q: 34.811031\n",
            " 20444/30000: episode: 3409, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4446.167 [1985.000, 5737.000],  loss: 41.366444, mae: 13.420924, mean_q: 34.537506\n",
            " 20450/30000: episode: 3410, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1586.167 [485.000, 4464.000],  loss: 38.659088, mae: 12.769841, mean_q: 33.068195\n",
            " 20456/30000: episode: 3411, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2319.667 [425.000, 4725.000],  loss: 74.956238, mae: 14.230466, mean_q: 35.727619\n",
            " 20462/30000: episode: 3412, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2769.500 [363.000, 5597.000],  loss: 52.663311, mae: 14.758354, mean_q: 36.808640\n",
            " 20468/30000: episode: 3413, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 3965.167 [1596.000, 5597.000],  loss: 61.214420, mae: 15.173538, mean_q: 38.221355\n",
            " 20474/30000: episode: 3414, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3417.833 [2904.000, 4496.000],  loss: 36.462936, mae: 13.991946, mean_q: 35.600048\n",
            " 20480/30000: episode: 3415, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 4027.000 [2008.000, 5466.000],  loss: 35.935184, mae: 16.217371, mean_q: 39.778606\n",
            " 20486/30000: episode: 3416, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2994.500 [1523.000, 5467.000],  loss: 74.013939, mae: 14.055252, mean_q: 35.110291\n",
            " 20492/30000: episode: 3417, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2068.333 [1484.000, 4806.000],  loss: 27.003332, mae: 14.275085, mean_q: 35.864090\n",
            " 20498/30000: episode: 3418, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [-5.000, 15.000], mean action: 1421.667 [183.000, 5597.000],  loss: 60.313736, mae: 14.195615, mean_q: 36.001114\n",
            " 20504/30000: episode: 3419, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1998.833 [183.000, 5597.000],  loss: 62.942139, mae: 15.230595, mean_q: 37.711903\n",
            " 20510/30000: episode: 3420, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2077.000 [12.000, 4315.000],  loss: 37.020779, mae: 14.358233, mean_q: 36.746571\n",
            " 20516/30000: episode: 3421, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: 50.000, mean reward:  8.333 [ 0.000, 35.000], mean action: 2681.833 [158.000, 5463.000],  loss: 53.470058, mae: 13.328717, mean_q: 34.171631\n",
            " 20522/30000: episode: 3422, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4078.500 [238.000, 5597.000],  loss: 38.681095, mae: 13.058057, mean_q: 33.818134\n",
            " 20528/30000: episode: 3423, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 30.000, mean reward:  5.000 [ 0.000, 30.000], mean action: 2483.000 [733.000, 5461.000],  loss: 44.450214, mae: 14.915752, mean_q: 37.683872\n",
            " 20534/30000: episode: 3424, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 3492.833 [363.000, 5597.000],  loss: 55.568329, mae: 14.693082, mean_q: 37.355499\n",
            " 20540/30000: episode: 3425, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4261.333 [2187.000, 5283.000],  loss: 52.054485, mae: 14.847581, mean_q: 37.735966\n",
            " 20546/30000: episode: 3426, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3335.000 [1393.000, 5624.000],  loss: 44.528393, mae: 14.387798, mean_q: 36.647709\n",
            " 20552/30000: episode: 3427, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2292.167 [790.000, 5624.000],  loss: 41.296848, mae: 14.319515, mean_q: 35.962803\n",
            " 20558/30000: episode: 3428, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2391.500 [356.000, 5597.000],  loss: 45.915432, mae: 14.167714, mean_q: 36.752979\n",
            " 20564/30000: episode: 3429, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3865.333 [2008.000, 5630.000],  loss: 55.728104, mae: 13.636112, mean_q: 35.423859\n",
            " 20570/30000: episode: 3430, duration: 0.245s, episode steps:   6, steps per second:  24, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3515.167 [946.000, 5703.000],  loss: 41.420467, mae: 13.333206, mean_q: 34.252254\n",
            " 20576/30000: episode: 3431, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3306.333 [791.000, 4482.000],  loss: 66.219597, mae: 13.036485, mean_q: 34.290096\n",
            " 20582/30000: episode: 3432, duration: 0.265s, episode steps:   6, steps per second:  23, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 5072.000 [5072.000, 5072.000],  loss: 68.055046, mae: 14.944160, mean_q: 38.449661\n",
            " 20588/30000: episode: 3433, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2082.000 [1484.000, 5072.000],  loss: 41.677170, mae: 13.881892, mean_q: 36.585976\n",
            " 20594/30000: episode: 3434, duration: 0.241s, episode steps:   6, steps per second:  25, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 5107.167 [5072.000, 5283.000],  loss: 82.258255, mae: 15.140082, mean_q: 38.915417\n",
            " 20600/30000: episode: 3435, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward: 15.000, mean reward:  2.500 [-5.000, 15.000], mean action: 3149.500 [1596.000, 5072.000],  loss: 59.564747, mae: 14.292801, mean_q: 38.139782\n",
            " 20606/30000: episode: 3436, duration: 0.260s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3508.667 [1610.000, 5348.000],  loss: 36.214844, mae: 13.826121, mean_q: 36.063461\n",
            " 20612/30000: episode: 3437, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3289.167 [1787.000, 5072.000],  loss: 68.709419, mae: 14.800000, mean_q: 37.633934\n",
            " 20618/30000: episode: 3438, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 5072.000 [5072.000, 5072.000],  loss: 78.560600, mae: 17.003168, mean_q: 42.809753\n",
            " 20624/30000: episode: 3439, duration: 0.245s, episode steps:   6, steps per second:  25, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3862.167 [1783.000, 5072.000],  loss: 39.922112, mae: 13.756152, mean_q: 35.843197\n",
            " 20630/30000: episode: 3440, duration: 0.270s, episode steps:   6, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3046.500 [1707.000, 5072.000],  loss: 65.330872, mae: 14.058095, mean_q: 36.810486\n",
            " 20636/30000: episode: 3441, duration: 0.238s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [-5.000, 15.000], mean action: 3683.333 [1344.000, 5072.000],  loss: 60.779827, mae: 14.045341, mean_q: 36.341415\n",
            " 20642/30000: episode: 3442, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3296.500 [590.000, 5072.000],  loss: 49.691757, mae: 14.668221, mean_q: 38.333088\n",
            " 20648/30000: episode: 3443, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3529.833 [783.000, 5072.000],  loss: 54.408100, mae: 14.368894, mean_q: 36.668118\n",
            " 20654/30000: episode: 3444, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3462.000 [1365.000, 5072.000],  loss: 37.691677, mae: 13.906685, mean_q: 36.553921\n",
            " 20660/30000: episode: 3445, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4399.833 [4053.000, 5072.000],  loss: 40.403370, mae: 14.297493, mean_q: 37.297848\n",
            " 20666/30000: episode: 3446, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3141.667 [733.000, 5005.000],  loss: 78.023949, mae: 15.287670, mean_q: 39.057453\n",
            " 20672/30000: episode: 3447, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3698.833 [1509.000, 5072.000],  loss: 57.369663, mae: 14.489738, mean_q: 37.127506\n",
            " 20678/30000: episode: 3448, duration: 0.146s, episode steps:   6, steps per second:  41, episode reward: 65.000, mean reward: 10.833 [ 0.000, 45.000], mean action: 2248.333 [425.000, 5072.000],  loss: 58.690456, mae: 15.320927, mean_q: 38.394032\n",
            " 20684/30000: episode: 3449, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3803.667 [3550.000, 5072.000],  loss: 44.098648, mae: 14.620715, mean_q: 37.349064\n",
            " 20690/30000: episode: 3450, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2915.833 [590.000, 4402.000],  loss: 48.618946, mae: 15.635800, mean_q: 39.732090\n",
            " 20696/30000: episode: 3451, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2367.833 [411.000, 3367.000],  loss: 73.917976, mae: 14.035934, mean_q: 36.254784\n",
            " 20702/30000: episode: 3452, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2205.333 [908.000, 3393.000],  loss: 46.943634, mae: 14.035172, mean_q: 35.988842\n",
            " 20708/30000: episode: 3453, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3129.500 [1344.000, 4404.000],  loss: 40.741974, mae: 13.591751, mean_q: 35.016403\n",
            " 20714/30000: episode: 3454, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2591.833 [213.000, 5624.000],  loss: 36.556892, mae: 13.264308, mean_q: 34.616333\n",
            " 20720/30000: episode: 3455, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2421.000 [353.000, 3615.000],  loss: 53.921825, mae: 14.103818, mean_q: 36.588955\n",
            " 20726/30000: episode: 3456, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4137.667 [3393.000, 5630.000],  loss: 59.051632, mae: 14.725434, mean_q: 37.724026\n",
            " 20732/30000: episode: 3457, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2526.667 [791.000, 4025.000],  loss: 64.411720, mae: 14.669391, mean_q: 37.684124\n",
            " 20738/30000: episode: 3458, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2421.500 [502.000, 3550.000],  loss: 28.723215, mae: 15.516926, mean_q: 37.932388\n",
            " 20744/30000: episode: 3459, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2223.667 [669.000, 5703.000],  loss: 50.590637, mae: 14.438267, mean_q: 36.954418\n",
            " 20750/30000: episode: 3460, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2503.833 [245.000, 4496.000],  loss: 47.539444, mae: 14.032552, mean_q: 35.789547\n",
            " 20756/30000: episode: 3461, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2954.667 [721.000, 5163.000],  loss: 50.014637, mae: 15.390227, mean_q: 38.428829\n",
            " 20762/30000: episode: 3462, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3522.167 [126.000, 5608.000],  loss: 41.094418, mae: 14.115067, mean_q: 36.158607\n",
            " 20768/30000: episode: 3463, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2081.667 [590.000, 3804.000],  loss: 60.681671, mae: 14.358703, mean_q: 36.329792\n",
            " 20774/30000: episode: 3464, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2153.500 [353.000, 4360.000],  loss: 70.580727, mae: 15.063229, mean_q: 37.475445\n",
            " 20780/30000: episode: 3465, duration: 0.183s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2384.833 [353.000, 4824.000],  loss: 36.350014, mae: 13.148786, mean_q: 34.901867\n",
            " 20786/30000: episode: 3466, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2473.167 [238.000, 3550.000],  loss: 41.499454, mae: 15.679634, mean_q: 39.079609\n",
            " 20792/30000: episode: 3467, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 1725.667 [946.000, 5624.000],  loss: 68.491783, mae: 14.765805, mean_q: 37.510754\n",
            " 20798/30000: episode: 3468, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3402.833 [2111.000, 5395.000],  loss: 88.116753, mae: 14.057926, mean_q: 36.352261\n",
            " 20804/30000: episode: 3469, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3323.667 [590.000, 4954.000],  loss: 46.389938, mae: 14.132108, mean_q: 36.469326\n",
            " 20810/30000: episode: 3470, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2481.167 [1199.000, 3834.000],  loss: 53.457912, mae: 14.751956, mean_q: 36.579441\n",
            " 20816/30000: episode: 3471, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2968.833 [945.000, 5706.000],  loss: 54.204281, mae: 14.881581, mean_q: 38.076172\n",
            " 20822/30000: episode: 3472, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3676.833 [1868.000, 5624.000],  loss: 43.525345, mae: 13.877795, mean_q: 36.210468\n",
            " 20828/30000: episode: 3473, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2663.333 [1296.000, 3892.000],  loss: 54.324993, mae: 13.304672, mean_q: 35.280567\n",
            " 20834/30000: episode: 3474, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3028.833 [353.000, 5433.000],  loss: 49.326096, mae: 15.314185, mean_q: 38.086163\n",
            " 20840/30000: episode: 3475, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 35.000, mean reward:  5.833 [ 0.000, 35.000], mean action: 2137.667 [733.000, 3921.000],  loss: 70.157227, mae: 14.598537, mean_q: 37.102642\n",
            " 20846/30000: episode: 3476, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3307.833 [663.000, 5719.000],  loss: 61.201458, mae: 13.375286, mean_q: 35.188736\n",
            " 20852/30000: episode: 3477, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3834.500 [1199.000, 4982.000],  loss: 57.745819, mae: 16.037035, mean_q: 39.326557\n",
            " 20858/30000: episode: 3478, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1502.833 [238.000, 2721.000],  loss: 68.947273, mae: 14.513562, mean_q: 36.493935\n",
            " 20864/30000: episode: 3479, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3503.833 [353.000, 5743.000],  loss: 57.619122, mae: 13.860059, mean_q: 35.495514\n",
            " 20870/30000: episode: 3480, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2928.833 [411.000, 5624.000],  loss: 37.988716, mae: 15.346870, mean_q: 37.769260\n",
            " 20876/30000: episode: 3481, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2314.667 [267.000, 4281.000],  loss: 47.302094, mae: 14.685265, mean_q: 36.552013\n",
            " 20882/30000: episode: 3482, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 60.000, mean reward: 10.000 [ 0.000, 30.000], mean action: 2606.833 [1530.000, 4496.000],  loss: 50.090988, mae: 13.618047, mean_q: 34.804745\n",
            " 20888/30000: episode: 3483, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3585.167 [1082.000, 5373.000],  loss: 59.255829, mae: 14.246577, mean_q: 36.225857\n",
            " 20894/30000: episode: 3484, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3192.000 [429.000, 5324.000],  loss: 29.067474, mae: 14.954505, mean_q: 37.164524\n",
            " 20900/30000: episode: 3485, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3086.000 [1109.000, 5624.000],  loss: 44.365391, mae: 15.349116, mean_q: 37.878452\n",
            " 20906/30000: episode: 3486, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2814.667 [1197.000, 4674.000],  loss: 67.662735, mae: 14.770054, mean_q: 37.660763\n",
            " 20912/30000: episode: 3487, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [-5.000, 10.000], mean action: 3375.667 [1530.000, 4362.000],  loss: 41.810032, mae: 14.774863, mean_q: 36.566223\n",
            " 20918/30000: episode: 3488, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3111.833 [636.000, 5466.000],  loss: 46.354664, mae: 14.644146, mean_q: 36.610104\n",
            " 20924/30000: episode: 3489, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3113.333 [353.000, 4797.000],  loss: 45.933273, mae: 14.325025, mean_q: 37.025467\n",
            " 20930/30000: episode: 3490, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 2342.667 [158.000, 4391.000],  loss: 59.247890, mae: 14.346107, mean_q: 36.578754\n",
            " 20936/30000: episode: 3491, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 75.000, mean reward: 12.500 [ 0.000, 30.000], mean action: 2946.000 [1530.000, 5703.000],  loss: 49.146622, mae: 12.895123, mean_q: 34.163826\n",
            " 20942/30000: episode: 3492, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3510.667 [282.000, 5651.000],  loss: 64.726105, mae: 13.194932, mean_q: 33.788677\n",
            " 20948/30000: episode: 3493, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2540.500 [1869.000, 4027.000],  loss: 58.879059, mae: 14.348889, mean_q: 36.365326\n",
            " 20954/30000: episode: 3494, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2070.000 [285.000, 3367.000],  loss: 67.523468, mae: 14.436111, mean_q: 37.149811\n",
            " 20960/30000: episode: 3495, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2531.500 [14.000, 5742.000],  loss: 51.845154, mae: 15.782593, mean_q: 38.471851\n",
            " 20966/30000: episode: 3496, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2888.833 [1504.000, 4035.000],  loss: 41.451504, mae: 13.392754, mean_q: 34.908619\n",
            " 20972/30000: episode: 3497, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 2119.500 [733.000, 4010.000],  loss: 68.734077, mae: 14.516773, mean_q: 36.903797\n",
            " 20978/30000: episode: 3498, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3692.833 [961.000, 4765.000],  loss: 76.130829, mae: 14.575742, mean_q: 37.287418\n",
            " 20984/30000: episode: 3499, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 40.000, mean reward:  6.667 [-5.000, 25.000], mean action: 2469.167 [238.000, 3987.000],  loss: 29.464655, mae: 13.305961, mean_q: 34.305878\n",
            " 20990/30000: episode: 3500, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 95.000, mean reward: 15.833 [ 0.000, 35.000], mean action: 1972.500 [238.000, 4185.000],  loss: 72.804787, mae: 14.132671, mean_q: 36.304539\n",
            " 20996/30000: episode: 3501, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3404.833 [994.000, 5750.000],  loss: 83.018402, mae: 14.294849, mean_q: 35.605968\n",
            " 21002/30000: episode: 3502, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2749.333 [961.000, 5156.000],  loss: 58.296021, mae: 15.510906, mean_q: 38.649281\n",
            " 21008/30000: episode: 3503, duration: 0.262s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 810.333 [43.000, 1685.000],  loss: 38.324505, mae: 14.491123, mean_q: 36.345627\n",
            " 21014/30000: episode: 3504, duration: 0.263s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2384.000 [733.000, 5475.000],  loss: 57.470066, mae: 15.233186, mean_q: 37.700554\n",
            " 21020/30000: episode: 3505, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4153.667 [2818.000, 4979.000],  loss: 43.099316, mae: 14.327388, mean_q: 36.862545\n",
            " 21026/30000: episode: 3506, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3982.167 [2435.000, 5394.000],  loss: 54.259613, mae: 16.016550, mean_q: 39.936153\n",
            " 21032/30000: episode: 3507, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward: 15.000, mean reward:  2.500 [-5.000, 25.000], mean action: 2285.833 [769.000, 3803.000],  loss: 42.212677, mae: 13.568421, mean_q: 35.524624\n",
            " 21038/30000: episode: 3508, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward: 110.000, mean reward: 18.333 [-5.000, 35.000], mean action: 3458.500 [1596.000, 4185.000],  loss: 78.121681, mae: 13.766726, mean_q: 35.979134\n",
            " 21044/30000: episode: 3509, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 1667.000 [592.000, 2681.000],  loss: 54.006882, mae: 15.091637, mean_q: 37.286835\n",
            " 21050/30000: episode: 3510, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2666.667 [946.000, 5144.000],  loss: 62.043926, mae: 14.788689, mean_q: 37.965771\n",
            " 21056/30000: episode: 3511, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4210.500 [238.000, 5005.000],  loss: 40.713848, mae: 14.827137, mean_q: 38.081871\n",
            " 21062/30000: episode: 3512, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2866.500 [961.000, 5283.000],  loss: 48.385437, mae: 15.062203, mean_q: 37.413307\n",
            " 21068/30000: episode: 3513, duration: 0.264s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3228.167 [238.000, 5249.000],  loss: 41.485962, mae: 14.211669, mean_q: 36.213886\n",
            " 21074/30000: episode: 3514, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2543.167 [1426.000, 5067.000],  loss: 67.040161, mae: 15.251576, mean_q: 38.532833\n",
            " 21080/30000: episode: 3515, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3299.000 [2727.000, 5750.000],  loss: 39.472576, mae: 15.450011, mean_q: 38.261791\n",
            " 21086/30000: episode: 3516, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3332.667 [2277.000, 5523.000],  loss: 32.898251, mae: 15.243119, mean_q: 37.511127\n",
            " 21092/30000: episode: 3517, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 662.500 [11.000, 2277.000],  loss: 62.060352, mae: 13.006396, mean_q: 33.651272\n",
            " 21098/30000: episode: 3518, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2943.167 [607.000, 4227.000],  loss: 49.417496, mae: 14.124641, mean_q: 36.027744\n",
            " 21104/30000: episode: 3519, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2944.333 [1576.000, 5405.000],  loss: 30.216722, mae: 14.413933, mean_q: 36.870892\n",
            " 21110/30000: episode: 3520, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3848.000 [2308.000, 5700.000],  loss: 61.642456, mae: 14.228569, mean_q: 36.376633\n",
            " 21116/30000: episode: 3521, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2244.167 [138.000, 4941.000],  loss: 46.589264, mae: 13.655589, mean_q: 35.395855\n",
            " 21122/30000: episode: 3522, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3254.167 [2008.000, 5140.000],  loss: 45.921787, mae: 13.524480, mean_q: 34.587215\n",
            " 21128/30000: episode: 3523, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 2884.333 [238.000, 5282.000],  loss: 42.290573, mae: 13.854813, mean_q: 35.288822\n",
            " 21134/30000: episode: 3524, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3207.333 [1576.000, 5005.000],  loss: 39.434254, mae: 14.556496, mean_q: 37.437000\n",
            " 21140/30000: episode: 3525, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3868.667 [2558.000, 4138.000],  loss: 56.605228, mae: 15.062706, mean_q: 38.291615\n",
            " 21146/30000: episode: 3526, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3300.000 [2277.000, 5339.000],  loss: 61.373264, mae: 15.915558, mean_q: 39.557373\n",
            " 21152/30000: episode: 3527, duration: 0.146s, episode steps:   6, steps per second:  41, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 1504.000 [648.000, 2277.000],  loss: 36.909565, mae: 14.245419, mean_q: 36.675060\n",
            " 21158/30000: episode: 3528, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2628.167 [1202.000, 5656.000],  loss: 43.138226, mae: 14.142734, mean_q: 35.374680\n",
            " 21164/30000: episode: 3529, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2974.833 [1108.000, 4188.000],  loss: 31.781303, mae: 14.862948, mean_q: 37.463425\n",
            " 21170/30000: episode: 3530, duration: 0.164s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1772.500 [656.000, 2277.000],  loss: 50.996746, mae: 13.244746, mean_q: 35.279484\n",
            " 21176/30000: episode: 3531, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward: -15.000, mean reward: -2.500 [-5.000,  5.000], mean action: 5211.333 [2558.000, 5742.000],  loss: 49.126587, mae: 13.667130, mean_q: 35.548222\n",
            " 21182/30000: episode: 3532, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3868.667 [2558.000, 4138.000],  loss: 54.547092, mae: 15.351387, mean_q: 38.653381\n",
            " 21188/30000: episode: 3533, duration: 0.146s, episode steps:   6, steps per second:  41, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 2387.667 [642.000, 5373.000],  loss: 61.968399, mae: 15.233463, mean_q: 38.123966\n",
            " 21194/30000: episode: 3534, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2123.000 [818.000, 3068.000],  loss: 35.942295, mae: 14.319419, mean_q: 37.039761\n",
            " 21200/30000: episode: 3535, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -15.000, mean reward: -2.500 [-5.000,  5.000], mean action: 5211.333 [2558.000, 5742.000],  loss: 58.163891, mae: 13.552135, mean_q: 34.779301\n",
            " 21206/30000: episode: 3536, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3183.333 [870.000, 5373.000],  loss: 54.682369, mae: 13.459714, mean_q: 35.397762\n",
            " 21212/30000: episode: 3537, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 35.000, mean reward:  5.833 [ 0.000, 25.000], mean action: 3340.500 [1783.000, 4901.000],  loss: 47.702282, mae: 12.987765, mean_q: 34.375568\n",
            " 21218/30000: episode: 3538, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 2274.667 [437.000, 4493.000],  loss: 36.971588, mae: 14.053059, mean_q: 36.505726\n",
            " 21224/30000: episode: 3539, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 1353.500 [339.000, 2558.000],  loss: 44.525467, mae: 13.676600, mean_q: 35.497665\n",
            " 21230/30000: episode: 3540, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2316.167 [1753.000, 2558.000],  loss: 92.289711, mae: 15.031754, mean_q: 38.478374\n",
            " 21236/30000: episode: 3541, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 145.000, mean reward: 24.167 [ 0.000, 35.000], mean action: 3554.000 [1596.000, 4185.000],  loss: 55.876461, mae: 13.861832, mean_q: 35.403160\n",
            " 21242/30000: episode: 3542, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1864.333 [512.000, 2705.000],  loss: 44.811829, mae: 14.408349, mean_q: 37.431412\n",
            " 21248/30000: episode: 3543, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2958.667 [196.000, 5471.000],  loss: 58.476620, mae: 15.617589, mean_q: 38.824089\n",
            " 21254/30000: episode: 3544, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3991.167 [2558.000, 5585.000],  loss: 69.449577, mae: 14.370473, mean_q: 37.135433\n",
            " 21260/30000: episode: 3545, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2970.000 [1606.000, 5564.000],  loss: 44.116440, mae: 14.546102, mean_q: 36.388149\n",
            " 21266/30000: episode: 3546, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2870.833 [2518.000, 3784.000],  loss: 46.298504, mae: 14.599027, mean_q: 36.827454\n",
            " 21272/30000: episode: 3547, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1571.500 [375.000, 3101.000],  loss: 46.152729, mae: 14.121997, mean_q: 36.046032\n",
            " 21278/30000: episode: 3548, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2660.000 [218.000, 4333.000],  loss: 56.011349, mae: 15.568063, mean_q: 38.872196\n",
            " 21284/30000: episode: 3549, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1606.667 [393.000, 3667.000],  loss: 60.227909, mae: 14.135764, mean_q: 36.569469\n",
            " 21290/30000: episode: 3550, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 45.000, mean reward:  7.500 [ 0.000, 45.000], mean action: 2177.667 [363.000, 4742.000],  loss: 38.702953, mae: 15.675128, mean_q: 38.970650\n",
            " 21296/30000: episode: 3551, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: 30.000, mean reward:  5.000 [ 0.000, 25.000], mean action: 3707.000 [818.000, 5656.000],  loss: 64.959389, mae: 13.347984, mean_q: 34.812565\n",
            " 21302/30000: episode: 3552, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3614.000 [733.000, 4742.000],  loss: 52.509583, mae: 15.611737, mean_q: 38.947559\n",
            " 21308/30000: episode: 3553, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1900.000 [733.000, 3249.000],  loss: 46.110264, mae: 13.966165, mean_q: 35.676525\n",
            " 21314/30000: episode: 3554, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [-5.000, 20.000], mean action: 2631.167 [545.000, 4450.000],  loss: 66.943230, mae: 13.982758, mean_q: 35.184849\n",
            " 21320/30000: episode: 3555, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2877.667 [769.000, 5348.000],  loss: 77.638237, mae: 14.623229, mean_q: 36.244843\n",
            " 21326/30000: episode: 3556, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 3082.667 [363.000, 5482.000],  loss: 69.292496, mae: 15.286595, mean_q: 38.180004\n",
            " 21332/30000: episode: 3557, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1355.833 [407.000, 3841.000],  loss: 30.728128, mae: 13.057499, mean_q: 34.018063\n",
            " 21338/30000: episode: 3558, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3389.000 [602.000, 5629.000],  loss: 37.082825, mae: 14.200025, mean_q: 35.991940\n",
            " 21344/30000: episode: 3559, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3218.333 [957.000, 5129.000],  loss: 53.941929, mae: 13.865949, mean_q: 35.654362\n",
            " 21350/30000: episode: 3560, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3431.500 [86.000, 5364.000],  loss: 39.605793, mae: 13.520766, mean_q: 35.076496\n",
            " 21356/30000: episode: 3561, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2836.833 [1127.000, 3684.000],  loss: 44.769917, mae: 14.740669, mean_q: 36.580086\n",
            " 21362/30000: episode: 3562, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2820.667 [1554.000, 5542.000],  loss: 80.600197, mae: 14.532592, mean_q: 37.732052\n",
            " 21368/30000: episode: 3563, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2336.833 [504.000, 3625.000],  loss: 56.110119, mae: 15.565911, mean_q: 38.346405\n",
            " 21374/30000: episode: 3564, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3046.167 [658.000, 5140.000],  loss: 43.780788, mae: 14.798576, mean_q: 37.410645\n",
            " 21380/30000: episode: 3565, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2808.167 [86.000, 4195.000],  loss: 48.661510, mae: 14.301174, mean_q: 35.914753\n",
            " 21386/30000: episode: 3566, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3433.667 [1426.000, 5404.000],  loss: 49.681858, mae: 12.668670, mean_q: 32.739777\n",
            " 21392/30000: episode: 3567, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2185.333 [491.000, 5424.000],  loss: 41.693005, mae: 13.841237, mean_q: 35.392735\n",
            " 21398/30000: episode: 3568, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2709.667 [945.000, 4894.000],  loss: 42.978210, mae: 13.715123, mean_q: 34.969204\n",
            " 21404/30000: episode: 3569, duration: 0.146s, episode steps:   6, steps per second:  41, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 2791.500 [1202.000, 3770.000],  loss: 69.979866, mae: 14.761533, mean_q: 36.753490\n",
            " 21410/30000: episode: 3570, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: 35.000, mean reward:  5.833 [ 0.000, 30.000], mean action: 2989.833 [146.000, 5348.000],  loss: 53.016727, mae: 14.766407, mean_q: 37.633614\n",
            " 21416/30000: episode: 3571, duration: 0.146s, episode steps:   6, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2916.500 [761.000, 5488.000],  loss: 77.794090, mae: 14.558479, mean_q: 36.360531\n",
            " 21422/30000: episode: 3572, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3400.167 [1630.000, 4681.000],  loss: 54.866364, mae: 13.039093, mean_q: 34.189434\n",
            " 21428/30000: episode: 3573, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2810.167 [1212.000, 5730.000],  loss: 31.494125, mae: 13.250241, mean_q: 34.506977\n",
            " 21434/30000: episode: 3574, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2165.167 [575.000, 3797.000],  loss: 72.857368, mae: 14.423581, mean_q: 36.686237\n",
            " 21440/30000: episode: 3575, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2964.000 [1596.000, 4250.000],  loss: 38.933712, mae: 13.882201, mean_q: 35.279163\n",
            " 21446/30000: episode: 3576, duration: 0.193s, episode steps:   6, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2851.667 [700.000, 4978.000],  loss: 34.216908, mae: 14.728775, mean_q: 37.056007\n",
            " 21452/30000: episode: 3577, duration: 0.239s, episode steps:   6, steps per second:  25, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1844.000 [26.000, 3117.000],  loss: 34.831184, mae: 14.309934, mean_q: 36.923931\n",
            " 21458/30000: episode: 3578, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2963.667 [1118.000, 5066.000],  loss: 23.532873, mae: 15.613785, mean_q: 39.095383\n",
            " 21464/30000: episode: 3579, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward: 40.000, mean reward:  6.667 [ 0.000, 20.000], mean action: 2133.167 [1202.000, 4667.000],  loss: 55.640213, mae: 14.852868, mean_q: 38.345432\n",
            " 21470/30000: episode: 3580, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3290.667 [451.000, 5316.000],  loss: 45.193848, mae: 14.244966, mean_q: 36.673363\n",
            " 21476/30000: episode: 3581, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2304.333 [1523.000, 5730.000],  loss: 43.598816, mae: 14.911224, mean_q: 37.675892\n",
            " 21482/30000: episode: 3582, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2053.833 [649.000, 3524.000],  loss: 64.013069, mae: 14.708095, mean_q: 36.894703\n",
            " 21488/30000: episode: 3583, duration: 0.263s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2989.167 [383.000, 5730.000],  loss: 75.414787, mae: 14.665446, mean_q: 37.183941\n",
            " 21494/30000: episode: 3584, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3167.167 [1426.000, 5730.000],  loss: 48.127224, mae: 16.703920, mean_q: 41.299450\n",
            " 21500/30000: episode: 3585, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3432.333 [138.000, 5730.000],  loss: 44.722408, mae: 15.723619, mean_q: 40.006550\n",
            " 21506/30000: episode: 3586, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3725.500 [1530.000, 5199.000],  loss: 49.492596, mae: 14.181552, mean_q: 36.982555\n",
            " 21512/30000: episode: 3587, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3795.000 [945.000, 5730.000],  loss: 31.717148, mae: 14.595328, mean_q: 37.500019\n",
            " 21518/30000: episode: 3588, duration: 0.218s, episode steps:   6, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3968.833 [1274.000, 5730.000],  loss: 80.762550, mae: 14.926056, mean_q: 37.844639\n",
            " 21524/30000: episode: 3589, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 3491.500 [769.000, 5730.000],  loss: 57.315914, mae: 13.794196, mean_q: 36.183662\n",
            " 21530/30000: episode: 3590, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3808.000 [26.000, 5730.000],  loss: 50.538910, mae: 13.781377, mean_q: 36.068668\n",
            " 21536/30000: episode: 3591, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2789.167 [2201.000, 5730.000],  loss: 47.721912, mae: 13.004186, mean_q: 34.075069\n",
            " 21542/30000: episode: 3592, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 2722.333 [1857.000, 4185.000],  loss: 31.054251, mae: 13.666229, mean_q: 35.909851\n",
            " 21548/30000: episode: 3593, duration: 0.147s, episode steps:   6, steps per second:  41, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4184.167 [2277.000, 5486.000],  loss: 44.904327, mae: 15.086026, mean_q: 37.595669\n",
            " 21554/30000: episode: 3594, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000, 10.000], mean action: 4187.167 [2741.000, 5730.000],  loss: 64.391655, mae: 14.607192, mean_q: 37.048435\n",
            " 21560/30000: episode: 3595, duration: 0.148s, episode steps:   6, steps per second:  41, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3149.667 [1202.000, 5316.000],  loss: 44.602325, mae: 13.045081, mean_q: 34.372017\n",
            " 21566/30000: episode: 3596, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3275.500 [568.000, 5730.000],  loss: 69.726364, mae: 14.698631, mean_q: 38.411594\n",
            " 21572/30000: episode: 3597, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 2637.667 [1141.000, 4472.000],  loss: 71.199196, mae: 12.773768, mean_q: 33.927551\n",
            " 21578/30000: episode: 3598, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2710.500 [989.000, 5730.000],  loss: 43.779675, mae: 13.942409, mean_q: 35.779716\n",
            " 21584/30000: episode: 3599, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1856.000 [939.000, 4159.000],  loss: 79.516418, mae: 14.152202, mean_q: 36.809547\n",
            " 21590/30000: episode: 3600, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 55.000, mean reward:  9.167 [ 0.000, 25.000], mean action: 2949.000 [238.000, 5463.000],  loss: 69.644066, mae: 13.928196, mean_q: 36.390121\n",
            " 21596/30000: episode: 3601, duration: 0.148s, episode steps:   6, steps per second:  40, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3077.667 [769.000, 4438.000],  loss: 48.041523, mae: 13.858871, mean_q: 36.662498\n",
            " 21602/30000: episode: 3602, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3791.667 [1633.000, 5670.000],  loss: 45.737045, mae: 14.956474, mean_q: 38.972523\n",
            " 21608/30000: episode: 3603, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 3610.167 [1523.000, 5735.000],  loss: 55.150951, mae: 15.243674, mean_q: 38.639954\n",
            " 21614/30000: episode: 3604, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2853.333 [814.000, 5140.000],  loss: 67.981232, mae: 13.959504, mean_q: 36.952053\n",
            " 21620/30000: episode: 3605, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3740.667 [504.000, 5140.000],  loss: 49.696476, mae: 13.035957, mean_q: 34.873573\n",
            " 21626/30000: episode: 3606, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3744.833 [3199.000, 5140.000],  loss: 77.733376, mae: 14.568089, mean_q: 37.274067\n",
            " 21632/30000: episode: 3607, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward: 45.000, mean reward:  7.500 [ 0.000, 25.000], mean action: 3380.000 [1202.000, 5523.000],  loss: 52.309650, mae: 14.592555, mean_q: 37.265594\n",
            " 21638/30000: episode: 3608, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3171.167 [1530.000, 4742.000],  loss: 39.404404, mae: 14.022003, mean_q: 36.650036\n",
            " 21644/30000: episode: 3609, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward: 10.000, mean reward:  1.667 [-5.000, 20.000], mean action: 3012.833 [184.000, 4185.000],  loss: 50.622837, mae: 14.604958, mean_q: 37.472065\n",
            " 21650/30000: episode: 3610, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [-5.000, 30.000], mean action: 2252.667 [309.000, 4185.000],  loss: 84.761101, mae: 14.408951, mean_q: 36.964451\n",
            " 21656/30000: episode: 3611, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3186.500 [1596.000, 5656.000],  loss: 57.247791, mae: 14.498494, mean_q: 36.906887\n",
            " 21662/30000: episode: 3612, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3059.167 [1074.000, 4900.000],  loss: 42.973099, mae: 12.948865, mean_q: 34.463062\n",
            " 21668/30000: episode: 3613, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 2978.000 [1202.000, 4840.000],  loss: 51.533936, mae: 13.375111, mean_q: 35.922466\n",
            " 21674/30000: episode: 3614, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2950.500 [1046.000, 5656.000],  loss: 65.491341, mae: 14.452113, mean_q: 36.244110\n",
            " 21680/30000: episode: 3615, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3429.833 [356.000, 5253.000],  loss: 72.064384, mae: 14.205604, mean_q: 37.299229\n",
            " 21686/30000: episode: 3616, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3616.500 [2277.000, 4455.000],  loss: 40.713413, mae: 13.172984, mean_q: 35.421978\n",
            " 21692/30000: episode: 3617, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2559.000 [1215.000, 4159.000],  loss: 42.947376, mae: 13.475011, mean_q: 35.345459\n",
            " 21698/30000: episode: 3618, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2367.500 [309.000, 4806.000],  loss: 50.243984, mae: 14.928487, mean_q: 38.788105\n",
            " 21704/30000: episode: 3619, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4644.833 [4159.000, 4742.000],  loss: 48.976917, mae: 14.998512, mean_q: 38.252743\n",
            " 21710/30000: episode: 3620, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [-5.000, 20.000], mean action: 3216.000 [1137.000, 4244.000],  loss: 53.741772, mae: 14.134145, mean_q: 36.924362\n",
            " 21716/30000: episode: 3621, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3369.333 [1523.000, 5523.000],  loss: 35.156681, mae: 14.627664, mean_q: 37.322655\n",
            " 21722/30000: episode: 3622, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2366.167 [282.000, 5576.000],  loss: 53.396683, mae: 14.767013, mean_q: 38.226391\n",
            " 21728/30000: episode: 3623, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3562.500 [2741.000, 4620.000],  loss: 33.008671, mae: 14.108964, mean_q: 36.923779\n",
            " 21734/30000: episode: 3624, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 387.833 [238.000, 1137.000],  loss: 31.684454, mae: 14.480710, mean_q: 36.973675\n",
            " 21740/30000: episode: 3625, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 50.000, mean reward:  8.333 [ 0.000, 30.000], mean action: 3495.500 [1596.000, 4683.000],  loss: 43.279461, mae: 14.325923, mean_q: 37.026608\n",
            " 21746/30000: episode: 3626, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3276.333 [2192.000, 4972.000],  loss: 62.968018, mae: 15.616881, mean_q: 39.334919\n",
            " 21752/30000: episode: 3627, duration: 0.148s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2618.000 [527.000, 4832.000],  loss: 61.590698, mae: 15.324897, mean_q: 39.196526\n",
            " 21758/30000: episode: 3628, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2933.167 [946.000, 4745.000],  loss: 62.762421, mae: 14.100829, mean_q: 36.055264\n",
            " 21764/30000: episode: 3629, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3708.333 [945.000, 5256.000],  loss: 54.503529, mae: 14.253060, mean_q: 36.355331\n",
            " 21770/30000: episode: 3630, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2093.500 [648.000, 4159.000],  loss: 61.712482, mae: 14.149418, mean_q: 36.584438\n",
            " 21776/30000: episode: 3631, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2658.333 [534.000, 4651.000],  loss: 30.449823, mae: 14.740644, mean_q: 37.493176\n",
            " 21782/30000: episode: 3632, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2664.833 [590.000, 4749.000],  loss: 67.627655, mae: 14.686887, mean_q: 37.448509\n",
            " 21788/30000: episode: 3633, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2993.333 [1202.000, 5187.000],  loss: 50.511433, mae: 14.333274, mean_q: 36.519527\n",
            " 21794/30000: episode: 3634, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2246.000 [309.000, 5296.000],  loss: 43.874893, mae: 14.678707, mean_q: 37.746475\n",
            " 21800/30000: episode: 3635, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2284.500 [1530.000, 3447.000],  loss: 65.145088, mae: 14.463155, mean_q: 36.460903\n",
            " 21806/30000: episode: 3636, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3423.000 [238.000, 5742.000],  loss: 38.151760, mae: 13.963478, mean_q: 35.634190\n",
            " 21812/30000: episode: 3637, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2759.833 [590.000, 4530.000],  loss: 60.245789, mae: 15.258822, mean_q: 38.242325\n",
            " 21818/30000: episode: 3638, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3764.000 [257.000, 5511.000],  loss: 52.843563, mae: 14.486862, mean_q: 36.915386\n",
            " 21824/30000: episode: 3639, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2459.833 [590.000, 5719.000],  loss: 80.459312, mae: 15.568333, mean_q: 38.457455\n",
            " 21830/30000: episode: 3640, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 2570.667 [590.000, 4464.000],  loss: 44.402210, mae: 14.030581, mean_q: 36.636204\n",
            " 21836/30000: episode: 3641, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1516.833 [590.000, 4464.000],  loss: 32.288261, mae: 14.104886, mean_q: 36.578342\n",
            " 21842/30000: episode: 3642, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2665.000 [590.000, 4379.000],  loss: 68.369370, mae: 14.172559, mean_q: 35.688648\n",
            " 21848/30000: episode: 3643, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2998.000 [451.000, 5651.000],  loss: 65.275742, mae: 13.945062, mean_q: 36.122326\n",
            " 21854/30000: episode: 3644, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2598.833 [590.000, 5065.000],  loss: 49.333607, mae: 14.427277, mean_q: 37.078594\n",
            " 21860/30000: episode: 3645, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3856.667 [2197.000, 4573.000],  loss: 57.681751, mae: 15.019848, mean_q: 37.734531\n",
            " 21866/30000: episode: 3646, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3579.000 [2558.000, 5652.000],  loss: 41.746067, mae: 13.786435, mean_q: 36.394718\n",
            " 21872/30000: episode: 3647, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3497.333 [567.000, 5348.000],  loss: 41.431812, mae: 13.103195, mean_q: 33.932373\n",
            " 21878/30000: episode: 3648, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2740.000 [1032.000, 5011.000],  loss: 34.264973, mae: 13.430346, mean_q: 34.987400\n",
            " 21884/30000: episode: 3649, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3003.000 [1381.000, 4683.000],  loss: 54.689365, mae: 14.565704, mean_q: 36.591953\n",
            " 21890/30000: episode: 3650, duration: 0.223s, episode steps:   6, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3313.500 [1059.000, 4871.000],  loss: 60.440151, mae: 13.829547, mean_q: 35.497356\n",
            " 21896/30000: episode: 3651, duration: 0.262s, episode steps:   6, steps per second:  23, episode reward: 160.000, mean reward: 26.667 [ 0.000, 35.000], mean action: 3597.500 [1596.000, 4185.000],  loss: 73.650787, mae: 15.638184, mean_q: 39.167492\n",
            " 21902/30000: episode: 3652, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3052.500 [126.000, 5538.000],  loss: 72.720535, mae: 13.464302, mean_q: 35.250881\n",
            " 21908/30000: episode: 3653, duration: 0.269s, episode steps:   6, steps per second:  22, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3510.167 [1710.000, 5296.000],  loss: 34.876637, mae: 14.475349, mean_q: 37.168812\n",
            " 21914/30000: episode: 3654, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2087.500 [545.000, 3221.000],  loss: 43.596554, mae: 14.560791, mean_q: 37.504883\n",
            " 21920/30000: episode: 3655, duration: 0.237s, episode steps:   6, steps per second:  25, episode reward: 30.000, mean reward:  5.000 [ 0.000, 25.000], mean action: 3966.667 [2698.000, 5331.000],  loss: 37.755596, mae: 13.602632, mean_q: 36.097992\n",
            " 21926/30000: episode: 3656, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3107.333 [1202.000, 4693.000],  loss: 33.680725, mae: 14.019282, mean_q: 36.143749\n",
            " 21932/30000: episode: 3657, duration: 0.240s, episode steps:   6, steps per second:  25, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3274.833 [512.000, 5283.000],  loss: 43.925964, mae: 14.163490, mean_q: 37.386524\n",
            " 21938/30000: episode: 3658, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3457.333 [1059.000, 5538.000],  loss: 48.416687, mae: 14.443234, mean_q: 36.879929\n",
            " 21944/30000: episode: 3659, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3975.833 [2180.000, 5258.000],  loss: 40.647343, mae: 14.668873, mean_q: 37.520966\n",
            " 21950/30000: episode: 3660, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3958.500 [2741.000, 5426.000],  loss: 39.708939, mae: 14.106214, mean_q: 36.613617\n",
            " 21956/30000: episode: 3661, duration: 0.265s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2483.667 [280.000, 4455.000],  loss: 65.294250, mae: 14.594466, mean_q: 37.882671\n",
            " 21962/30000: episode: 3662, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4475.167 [2263.000, 5538.000],  loss: 54.130985, mae: 13.884662, mean_q: 36.175152\n",
            " 21968/30000: episode: 3663, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3168.667 [1254.000, 5511.000],  loss: 75.449310, mae: 14.299011, mean_q: 36.954441\n",
            " 21974/30000: episode: 3664, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 50.000, mean reward:  8.333 [ 0.000, 40.000], mean action: 4192.167 [2448.000, 5395.000],  loss: 68.894470, mae: 14.809132, mean_q: 37.676998\n",
            " 21980/30000: episode: 3665, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 3321.000 [1496.000, 4771.000],  loss: 50.135662, mae: 14.517937, mean_q: 37.491634\n",
            " 21986/30000: episode: 3666, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2985.667 [1484.000, 4941.000],  loss: 60.610046, mae: 14.160195, mean_q: 36.116131\n",
            " 21992/30000: episode: 3667, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2924.833 [329.000, 5149.000],  loss: 39.352570, mae: 14.305176, mean_q: 36.381546\n",
            " 21998/30000: episode: 3668, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2152.500 [515.000, 3447.000],  loss: 55.032074, mae: 13.590682, mean_q: 36.072025\n",
            " 22004/30000: episode: 3669, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2844.000 [178.000, 5523.000],  loss: 62.380768, mae: 13.920421, mean_q: 35.258026\n",
            " 22010/30000: episode: 3670, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1955.833 [71.000, 5503.000],  loss: 35.296459, mae: 13.715913, mean_q: 35.199341\n",
            " 22016/30000: episode: 3671, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3232.000 [329.000, 4157.000],  loss: 47.732315, mae: 13.533229, mean_q: 35.327572\n",
            " 22022/30000: episode: 3672, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3042.833 [329.000, 5656.000],  loss: 45.958767, mae: 15.325849, mean_q: 38.501068\n",
            " 22028/30000: episode: 3673, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2122.500 [1710.000, 2205.000],  loss: 36.247589, mae: 13.164231, mean_q: 34.714130\n",
            " 22034/30000: episode: 3674, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 1896.167 [238.000, 4420.000],  loss: 28.288818, mae: 14.072868, mean_q: 36.610836\n",
            " 22040/30000: episode: 3675, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3140.000 [1344.000, 4185.000],  loss: 42.814045, mae: 14.271210, mean_q: 37.397015\n",
            " 22046/30000: episode: 3676, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2920.167 [329.000, 5427.000],  loss: 48.339100, mae: 13.903447, mean_q: 36.038322\n",
            " 22052/30000: episode: 3677, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3584.667 [329.000, 5260.000],  loss: 43.619625, mae: 13.758739, mean_q: 36.334896\n",
            " 22058/30000: episode: 3678, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 2664.500 [1409.000, 4163.000],  loss: 56.133045, mae: 14.042157, mean_q: 36.480061\n",
            " 22064/30000: episode: 3679, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 1647.000 [238.000, 4163.000],  loss: 63.632061, mae: 14.277966, mean_q: 37.161076\n",
            " 22070/30000: episode: 3680, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4716.667 [2278.000, 5597.000],  loss: 46.736004, mae: 13.902423, mean_q: 36.226154\n",
            " 22076/30000: episode: 3681, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3156.667 [329.000, 5433.000],  loss: 45.747360, mae: 13.373975, mean_q: 35.049904\n",
            " 22082/30000: episode: 3682, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3369.500 [329.000, 5734.000],  loss: 45.473911, mae: 14.173968, mean_q: 36.622753\n",
            " 22088/30000: episode: 3683, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 30.000, mean reward:  5.000 [-5.000, 20.000], mean action: 3097.167 [329.000, 4742.000],  loss: 47.752110, mae: 13.996417, mean_q: 36.154442\n",
            " 22094/30000: episode: 3684, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 55.000, mean reward:  9.167 [ 0.000, 25.000], mean action: 2343.667 [329.000, 4464.000],  loss: 55.615711, mae: 14.360879, mean_q: 36.990612\n",
            " 22100/30000: episode: 3685, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2739.500 [1710.000, 3447.000],  loss: 57.380627, mae: 14.799149, mean_q: 37.800388\n",
            " 22106/30000: episode: 3686, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  5.000], mean action: 3369.500 [2448.000, 4185.000],  loss: 36.344028, mae: 15.011811, mean_q: 38.718510\n",
            " 22112/30000: episode: 3687, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3975.667 [329.000, 5734.000],  loss: 69.209320, mae: 14.877374, mean_q: 38.082874\n",
            " 22118/30000: episode: 3688, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2933.333 [614.000, 3951.000],  loss: 32.133026, mae: 14.445340, mean_q: 37.411865\n",
            " 22124/30000: episode: 3689, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2445.000 [329.000, 5597.000],  loss: 38.749470, mae: 14.662400, mean_q: 37.887283\n",
            " 22130/30000: episode: 3690, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3091.333 [329.000, 4214.000],  loss: 54.590405, mae: 15.005722, mean_q: 39.506359\n",
            " 22136/30000: episode: 3691, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3239.500 [2202.000, 4185.000],  loss: 34.316463, mae: 14.225589, mean_q: 36.835785\n",
            " 22142/30000: episode: 3692, duration: 0.146s, episode steps:   6, steps per second:  41, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2583.167 [329.000, 5593.000],  loss: 33.657879, mae: 13.804324, mean_q: 36.709305\n",
            " 22148/30000: episode: 3693, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: 50.000, mean reward:  8.333 [ 0.000, 30.000], mean action: 3329.667 [866.000, 5597.000],  loss: 74.580551, mae: 16.272806, mean_q: 41.364624\n",
            " 22154/30000: episode: 3694, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3651.500 [503.000, 4804.000],  loss: 52.797012, mae: 13.737853, mean_q: 35.712616\n",
            " 22160/30000: episode: 3695, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2732.167 [1344.000, 3921.000],  loss: 46.617065, mae: 14.445773, mean_q: 37.595097\n",
            " 22166/30000: episode: 3696, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2385.500 [451.000, 5564.000],  loss: 54.168842, mae: 14.552072, mean_q: 36.887623\n",
            " 22172/30000: episode: 3697, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3702.333 [1710.000, 5538.000],  loss: 45.846554, mae: 13.703776, mean_q: 35.917549\n",
            " 22178/30000: episode: 3698, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3542.833 [329.000, 5394.000],  loss: 40.192120, mae: 14.483113, mean_q: 36.603092\n",
            " 22184/30000: episode: 3699, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2948.500 [1710.000, 5161.000],  loss: 70.737442, mae: 14.200713, mean_q: 37.089428\n",
            " 22190/30000: episode: 3700, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2886.167 [946.000, 5449.000],  loss: 56.599747, mae: 14.176628, mean_q: 36.643421\n",
            " 22196/30000: episode: 3701, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2953.833 [329.000, 3921.000],  loss: 46.442902, mae: 14.479885, mean_q: 37.180237\n",
            " 22202/30000: episode: 3702, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2431.833 [375.000, 3921.000],  loss: 47.806641, mae: 14.935065, mean_q: 38.852806\n",
            " 22208/30000: episode: 3703, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3810.500 [339.000, 4945.000],  loss: 59.344227, mae: 14.508651, mean_q: 37.726326\n",
            " 22214/30000: episode: 3704, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4746.500 [1344.000, 5427.000],  loss: 34.870083, mae: 14.083758, mean_q: 37.276653\n",
            " 22220/30000: episode: 3705, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1989.333 [145.000, 3473.000],  loss: 48.093822, mae: 14.590905, mean_q: 37.482967\n",
            " 22226/30000: episode: 3706, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 25.000, mean reward:  4.167 [-5.000, 15.000], mean action: 2595.833 [329.000, 4742.000],  loss: 47.050976, mae: 14.377456, mean_q: 37.005245\n",
            " 22232/30000: episode: 3707, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2744.167 [238.000, 5241.000],  loss: 40.737011, mae: 14.402867, mean_q: 36.895969\n",
            " 22238/30000: episode: 3708, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2525.000 [322.000, 3921.000],  loss: 42.179043, mae: 13.607764, mean_q: 36.376968\n",
            " 22244/30000: episode: 3709, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3496.500 [1097.000, 5597.000],  loss: 42.094788, mae: 13.576238, mean_q: 35.189899\n",
            " 22250/30000: episode: 3710, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 1689.333 [230.000, 2745.000],  loss: 60.365265, mae: 13.259974, mean_q: 35.604465\n",
            " 22256/30000: episode: 3711, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3151.667 [329.000, 4339.000],  loss: 52.808613, mae: 14.631794, mean_q: 37.903042\n",
            " 22262/30000: episode: 3712, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 65.000, mean reward: 10.833 [ 0.000, 30.000], mean action: 3579.833 [2652.000, 5369.000],  loss: 46.887074, mae: 14.094649, mean_q: 36.976135\n",
            " 22268/30000: episode: 3713, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3217.333 [163.000, 4909.000],  loss: 67.363419, mae: 13.876564, mean_q: 36.726570\n",
            " 22274/30000: episode: 3714, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3667.500 [329.000, 5742.000],  loss: 55.295731, mae: 13.883050, mean_q: 36.154934\n",
            " 22280/30000: episode: 3715, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3571.833 [497.000, 5624.000],  loss: 84.570389, mae: 14.966827, mean_q: 38.249561\n",
            " 22286/30000: episode: 3716, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2937.500 [1041.000, 4913.000],  loss: 39.377659, mae: 13.471885, mean_q: 35.340908\n",
            " 22292/30000: episode: 3717, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3408.667 [329.000, 5702.000],  loss: 60.542080, mae: 14.158919, mean_q: 36.554218\n",
            " 22298/30000: episode: 3718, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3135.167 [329.000, 5656.000],  loss: 58.122364, mae: 13.960319, mean_q: 36.667004\n",
            " 22304/30000: episode: 3719, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1903.167 [329.000, 4250.000],  loss: 56.094784, mae: 13.766408, mean_q: 36.337059\n",
            " 22310/30000: episode: 3720, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2789.500 [1124.000, 5734.000],  loss: 33.660007, mae: 13.696502, mean_q: 35.372360\n",
            " 22316/30000: episode: 3721, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3075.667 [329.000, 5620.000],  loss: 54.495380, mae: 13.919263, mean_q: 35.922142\n",
            " 22322/30000: episode: 3722, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1319.167 [329.000, 4742.000],  loss: 60.311691, mae: 15.383313, mean_q: 39.383839\n",
            " 22328/30000: episode: 3723, duration: 0.240s, episode steps:   6, steps per second:  25, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1378.167 [329.000, 2680.000],  loss: 49.685543, mae: 14.581181, mean_q: 38.385525\n",
            " 22334/30000: episode: 3724, duration: 0.266s, episode steps:   6, steps per second:  23, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3650.500 [2291.000, 4310.000],  loss: 64.656868, mae: 15.279027, mean_q: 39.207108\n",
            " 22340/30000: episode: 3725, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2590.500 [667.000, 5597.000],  loss: 25.461634, mae: 14.038638, mean_q: 36.805260\n",
            " 22346/30000: episode: 3726, duration: 0.270s, episode steps:   6, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2800.167 [329.000, 5597.000],  loss: 36.494621, mae: 15.158078, mean_q: 38.547028\n",
            " 22352/30000: episode: 3727, duration: 0.241s, episode steps:   6, steps per second:  25, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3314.333 [945.000, 5597.000],  loss: 49.092464, mae: 13.857048, mean_q: 35.236355\n",
            " 22358/30000: episode: 3728, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2961.333 [1438.000, 5326.000],  loss: 53.030270, mae: 13.125188, mean_q: 35.253025\n",
            " 22364/30000: episode: 3729, duration: 0.263s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2872.667 [1847.000, 4316.000],  loss: 38.743252, mae: 14.469048, mean_q: 36.846729\n",
            " 22370/30000: episode: 3730, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3063.167 [329.000, 4820.000],  loss: 43.052021, mae: 13.761247, mean_q: 35.723942\n",
            " 22376/30000: episode: 3731, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 2820.333 [537.000, 5239.000],  loss: 70.944771, mae: 14.679616, mean_q: 37.929272\n",
            " 22382/30000: episode: 3732, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3980.667 [2444.000, 4288.000],  loss: 55.262939, mae: 15.583870, mean_q: 39.227470\n",
            " 22388/30000: episode: 3733, duration: 0.272s, episode steps:   6, steps per second:  22, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 3103.833 [1202.000, 4742.000],  loss: 32.688808, mae: 12.417172, mean_q: 34.104187\n",
            " 22394/30000: episode: 3734, duration: 0.228s, episode steps:   6, steps per second:  26, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2458.667 [161.000, 4616.000],  loss: 29.125483, mae: 13.867589, mean_q: 36.177883\n",
            " 22400/30000: episode: 3735, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: 20.000, mean reward:  3.333 [-5.000, 15.000], mean action: 1820.500 [1137.000, 2721.000],  loss: 68.287651, mae: 14.754753, mean_q: 38.258514\n",
            " 22406/30000: episode: 3736, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 40.000, mean reward:  6.667 [ 0.000, 25.000], mean action: 3495.333 [1560.000, 5597.000],  loss: 74.228767, mae: 13.657500, mean_q: 35.717670\n",
            " 22412/30000: episode: 3737, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2850.833 [522.000, 5597.000],  loss: 71.336884, mae: 14.477294, mean_q: 37.220333\n",
            " 22418/30000: episode: 3738, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2460.500 [450.000, 5269.000],  loss: 51.326508, mae: 12.288484, mean_q: 33.412140\n",
            " 22424/30000: episode: 3739, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2365.167 [1137.000, 4847.000],  loss: 40.712387, mae: 13.638640, mean_q: 36.259495\n",
            " 22430/30000: episode: 3740, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2304.333 [183.000, 5473.000],  loss: 83.035362, mae: 12.957559, mean_q: 34.855270\n",
            " 22436/30000: episode: 3741, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2286.333 [1109.000, 5157.000],  loss: 75.842567, mae: 14.866653, mean_q: 38.950172\n",
            " 22442/30000: episode: 3742, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2585.000 [497.000, 5145.000],  loss: 38.502865, mae: 13.990917, mean_q: 36.748089\n",
            " 22448/30000: episode: 3743, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3776.833 [590.000, 4742.000],  loss: 58.671295, mae: 13.913177, mean_q: 37.339127\n",
            " 22454/30000: episode: 3744, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2032.667 [555.000, 5597.000],  loss: 50.632915, mae: 14.594952, mean_q: 38.111900\n",
            " 22460/30000: episode: 3745, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2326.167 [150.000, 4941.000],  loss: 56.715534, mae: 14.975030, mean_q: 39.010254\n",
            " 22466/30000: episode: 3746, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3393.167 [1358.000, 5597.000],  loss: 54.803295, mae: 15.291115, mean_q: 39.821957\n",
            " 22472/30000: episode: 3747, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 40.000, mean reward:  6.667 [ 0.000, 20.000], mean action: 3216.167 [1202.000, 5197.000],  loss: 45.575115, mae: 13.873654, mean_q: 36.633793\n",
            " 22478/30000: episode: 3748, duration: 0.183s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2439.833 [1530.000, 3473.000],  loss: 41.941452, mae: 13.280185, mean_q: 35.852528\n",
            " 22484/30000: episode: 3749, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3129.500 [1344.000, 4783.000],  loss: 44.600201, mae: 13.519948, mean_q: 35.713814\n",
            " 22490/30000: episode: 3750, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2099.000 [461.000, 5597.000],  loss: 44.768066, mae: 15.522950, mean_q: 39.652664\n",
            " 22496/30000: episode: 3751, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2826.500 [517.000, 5161.000],  loss: 59.406445, mae: 14.371120, mean_q: 37.678181\n",
            " 22502/30000: episode: 3752, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3969.000 [1094.000, 5597.000],  loss: 31.465754, mae: 13.553191, mean_q: 36.175014\n",
            " 22508/30000: episode: 3753, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 2829.333 [1109.000, 4616.000],  loss: 52.646374, mae: 13.029027, mean_q: 34.851704\n",
            " 22514/30000: episode: 3754, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2425.167 [1137.000, 4360.000],  loss: 37.708683, mae: 13.739758, mean_q: 36.432163\n",
            " 22520/30000: episode: 3755, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 35.000, mean reward:  5.833 [ 0.000, 35.000], mean action: 3146.667 [269.000, 5683.000],  loss: 41.571106, mae: 13.980166, mean_q: 36.514347\n",
            " 22526/30000: episode: 3756, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 45.000, mean reward:  7.500 [ 0.000, 20.000], mean action: 1735.333 [442.000, 3103.000],  loss: 33.118412, mae: 14.334702, mean_q: 37.813507\n",
            " 22532/30000: episode: 3757, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2421.667 [253.000, 4829.000],  loss: 61.895512, mae: 13.963708, mean_q: 37.228130\n",
            " 22538/30000: episode: 3758, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2205.000 [442.000, 3265.000],  loss: 67.711189, mae: 14.956061, mean_q: 38.519913\n",
            " 22544/30000: episode: 3759, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 2119.500 [158.000, 4185.000],  loss: 57.416428, mae: 14.555264, mean_q: 37.690899\n",
            " 22550/30000: episode: 3760, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [-5.000, 10.000], mean action: 3887.500 [555.000, 5608.000],  loss: 55.039028, mae: 13.631810, mean_q: 36.081413\n",
            " 22556/30000: episode: 3761, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2261.667 [442.000, 4820.000],  loss: 31.918139, mae: 14.435829, mean_q: 38.152157\n",
            " 22562/30000: episode: 3762, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 2270.667 [1344.000, 3473.000],  loss: 51.997879, mae: 14.076171, mean_q: 37.010509\n",
            " 22568/30000: episode: 3763, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3698.500 [1419.000, 5269.000],  loss: 39.055244, mae: 13.345727, mean_q: 35.418087\n",
            " 22574/30000: episode: 3764, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 160.000, mean reward: 26.667 [ 0.000, 35.000], mean action: 3597.500 [1596.000, 4185.000],  loss: 51.975525, mae: 15.313802, mean_q: 39.462315\n",
            " 22580/30000: episode: 3765, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3716.167 [225.000, 5750.000],  loss: 79.502373, mae: 14.228709, mean_q: 36.756130\n",
            " 22586/30000: episode: 3766, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3030.333 [1344.000, 5252.000],  loss: 49.887009, mae: 15.604403, mean_q: 39.335392\n",
            " 22592/30000: episode: 3767, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3832.333 [2062.000, 5473.000],  loss: 54.468143, mae: 13.918575, mean_q: 36.710178\n",
            " 22598/30000: episode: 3768, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4598.333 [2444.000, 5597.000],  loss: 41.666931, mae: 14.903817, mean_q: 38.337688\n",
            " 22604/30000: episode: 3769, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 3036.000 [1596.000, 4725.000],  loss: 63.130146, mae: 14.638591, mean_q: 38.676224\n",
            " 22610/30000: episode: 3770, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2114.000 [1530.000, 2752.000],  loss: 41.229885, mae: 14.124802, mean_q: 35.929611\n",
            " 22616/30000: episode: 3771, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3346.000 [1634.000, 5675.000],  loss: 40.777451, mae: 13.947098, mean_q: 36.516117\n",
            " 22622/30000: episode: 3772, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2813.667 [614.000, 3283.000],  loss: 66.040764, mae: 13.966015, mean_q: 36.305096\n",
            " 22628/30000: episode: 3773, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000, 15.000], mean action: 3404.500 [3343.000, 3712.000],  loss: 46.190536, mae: 13.307922, mean_q: 35.091377\n",
            " 22634/30000: episode: 3774, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3985.333 [1344.000, 5129.000],  loss: 61.690510, mae: 15.318475, mean_q: 38.582088\n",
            " 22640/30000: episode: 3775, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2929.000 [275.000, 4683.000],  loss: 58.499542, mae: 14.361697, mean_q: 37.620743\n",
            " 22646/30000: episode: 3776, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4385.500 [3103.000, 5367.000],  loss: 59.797634, mae: 14.323270, mean_q: 36.282608\n",
            " 22652/30000: episode: 3777, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1937.667 [218.000, 4130.000],  loss: 58.391460, mae: 13.727551, mean_q: 35.508297\n",
            " 22658/30000: episode: 3778, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3558.500 [2312.000, 4360.000],  loss: 42.892323, mae: 14.554119, mean_q: 37.219402\n",
            " 22664/30000: episode: 3779, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2825.667 [428.000, 4820.000],  loss: 41.499203, mae: 15.100682, mean_q: 38.555264\n",
            " 22670/30000: episode: 3780, duration: 0.183s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3690.167 [1344.000, 5494.000],  loss: 37.423359, mae: 15.280251, mean_q: 38.686329\n",
            " 22676/30000: episode: 3781, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 2779.500 [542.000, 5608.000],  loss: 50.260651, mae: 14.432513, mean_q: 38.106415\n",
            " 22682/30000: episode: 3782, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [-5.000, 20.000], mean action: 2672.500 [607.000, 3892.000],  loss: 57.356945, mae: 14.221457, mean_q: 36.141373\n",
            " 22688/30000: episode: 3783, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3885.667 [1560.000, 5597.000],  loss: 38.862175, mae: 13.490502, mean_q: 35.403263\n",
            " 22694/30000: episode: 3784, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 160.000, mean reward: 26.667 [ 0.000, 35.000], mean action: 3152.333 [578.000, 4185.000],  loss: 48.249790, mae: 14.422551, mean_q: 37.292171\n",
            " 22700/30000: episode: 3785, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3049.667 [910.000, 3769.000],  loss: 43.298004, mae: 13.195008, mean_q: 35.049702\n",
            " 22706/30000: episode: 3786, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2661.833 [221.000, 4220.000],  loss: 65.220650, mae: 14.303246, mean_q: 36.766911\n",
            " 22712/30000: episode: 3787, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2264.333 [1254.000, 5016.000],  loss: 59.539749, mae: 14.307992, mean_q: 36.844032\n",
            " 22718/30000: episode: 3788, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3554.833 [547.000, 5152.000],  loss: 38.519791, mae: 14.285453, mean_q: 37.104572\n",
            " 22724/30000: episode: 3789, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3458.167 [238.000, 5747.000],  loss: 58.831722, mae: 14.000731, mean_q: 36.097347\n",
            " 22730/30000: episode: 3790, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  5.000], mean action: 1226.833 [590.000, 3103.000],  loss: 39.376896, mae: 13.686329, mean_q: 35.862442\n",
            " 22736/30000: episode: 3791, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3659.500 [1904.000, 5213.000],  loss: 28.853716, mae: 14.119460, mean_q: 35.689510\n",
            " 22742/30000: episode: 3792, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1120.833 [238.000, 2312.000],  loss: 43.956470, mae: 13.162888, mean_q: 34.744057\n",
            " 22748/30000: episode: 3793, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3152.833 [425.000, 5400.000],  loss: 66.988571, mae: 14.487376, mean_q: 37.079334\n",
            " 22754/30000: episode: 3794, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward: 160.000, mean reward: 26.667 [ 0.000, 35.000], mean action: 3597.500 [1596.000, 4185.000],  loss: 62.311054, mae: 14.309405, mean_q: 36.126995\n",
            " 22760/30000: episode: 3795, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward: 65.000, mean reward: 10.833 [ 0.000, 30.000], mean action: 2097.167 [946.000, 3855.000],  loss: 37.956791, mae: 13.918492, mean_q: 35.983395\n",
            " 22766/30000: episode: 3796, duration: 0.245s, episode steps:   6, steps per second:  25, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3652.833 [1032.000, 5386.000],  loss: 66.797867, mae: 13.270253, mean_q: 34.672436\n",
            " 22772/30000: episode: 3797, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4801.500 [3249.000, 5597.000],  loss: 68.390259, mae: 15.064857, mean_q: 38.448498\n",
            " 22778/30000: episode: 3798, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2402.000 [555.000, 4185.000],  loss: 61.052811, mae: 13.440213, mean_q: 35.077305\n",
            " 22784/30000: episode: 3799, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3605.000 [1530.000, 5000.000],  loss: 50.004192, mae: 13.907126, mean_q: 36.069416\n",
            " 22790/30000: episode: 3800, duration: 0.236s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2635.333 [28.000, 4804.000],  loss: 28.273275, mae: 14.506541, mean_q: 37.738720\n",
            " 22796/30000: episode: 3801, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3061.833 [356.000, 5362.000],  loss: 45.665615, mae: 12.918794, mean_q: 33.848885\n",
            " 22802/30000: episode: 3802, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2644.167 [461.000, 4319.000],  loss: 55.489941, mae: 14.218025, mean_q: 37.494694\n",
            " 22808/30000: episode: 3803, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2757.167 [1530.000, 3921.000],  loss: 64.270134, mae: 12.401913, mean_q: 33.093075\n",
            " 22814/30000: episode: 3804, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2477.000 [1124.000, 3921.000],  loss: 68.840492, mae: 14.821681, mean_q: 38.363018\n",
            " 22820/30000: episode: 3805, duration: 0.269s, episode steps:   6, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3994.167 [491.000, 5747.000],  loss: 55.989258, mae: 14.406944, mean_q: 37.091820\n",
            " 22826/30000: episode: 3806, duration: 0.229s, episode steps:   6, steps per second:  26, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 1962.333 [555.000, 3921.000],  loss: 41.213291, mae: 12.915807, mean_q: 34.047985\n",
            " 22832/30000: episode: 3807, duration: 0.148s, episode steps:   6, steps per second:  40, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 3917.000 [2192.000, 5367.000],  loss: 63.084461, mae: 14.552659, mean_q: 37.710880\n",
            " 22838/30000: episode: 3808, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 2369.333 [1002.000, 3921.000],  loss: 40.769726, mae: 13.532788, mean_q: 34.775841\n",
            " 22844/30000: episode: 3809, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2325.667 [946.000, 4306.000],  loss: 33.088779, mae: 13.194622, mean_q: 34.332378\n",
            " 22850/30000: episode: 3810, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2853.333 [1419.000, 3921.000],  loss: 62.777119, mae: 13.491261, mean_q: 35.646290\n",
            " 22856/30000: episode: 3811, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2087.500 [962.000, 4176.000],  loss: 40.782032, mae: 13.636821, mean_q: 36.238125\n",
            " 22862/30000: episode: 3812, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2608.333 [846.000, 4438.000],  loss: 28.165674, mae: 14.029447, mean_q: 36.798828\n",
            " 22868/30000: episode: 3813, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 3995.000 [2008.000, 5129.000],  loss: 46.416748, mae: 14.424016, mean_q: 38.188316\n",
            " 22874/30000: episode: 3814, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2499.500 [145.000, 5554.000],  loss: 37.383736, mae: 13.228699, mean_q: 35.502819\n",
            " 22880/30000: episode: 3815, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2779.167 [1094.000, 3921.000],  loss: 53.636539, mae: 14.108594, mean_q: 37.390202\n",
            " 22886/30000: episode: 3816, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 3921.000 [3921.000, 3921.000],  loss: 49.972599, mae: 14.564465, mean_q: 38.660351\n",
            " 22892/30000: episode: 3817, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4347.000 [3056.000, 5747.000],  loss: 59.039108, mae: 13.991516, mean_q: 37.803703\n",
            " 22898/30000: episode: 3818, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1968.500 [145.000, 3921.000],  loss: 66.625595, mae: 14.946869, mean_q: 39.213734\n",
            " 22904/30000: episode: 3819, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3201.667 [1345.000, 4893.000],  loss: 41.192562, mae: 13.318385, mean_q: 36.593929\n",
            " 22910/30000: episode: 3820, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3522.500 [1530.000, 3921.000],  loss: 35.627323, mae: 13.095149, mean_q: 35.152287\n",
            " 22916/30000: episode: 3821, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 3921.000 [3921.000, 3921.000],  loss: 54.555187, mae: 12.802945, mean_q: 35.003231\n",
            " 22922/30000: episode: 3822, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: 35.000, mean reward:  5.833 [ 0.000, 35.000], mean action: 3999.167 [1670.000, 5129.000],  loss: 52.159107, mae: 13.027768, mean_q: 35.567665\n",
            " 22928/30000: episode: 3823, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3152.333 [1243.000, 4552.000],  loss: 73.689217, mae: 14.236398, mean_q: 37.790493\n",
            " 22934/30000: episode: 3824, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3533.167 [1202.000, 5426.000],  loss: 56.985210, mae: 15.598226, mean_q: 40.372299\n",
            " 22940/30000: episode: 3825, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3493.333 [1344.000, 5603.000],  loss: 34.985271, mae: 14.164883, mean_q: 37.198990\n",
            " 22946/30000: episode: 3826, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3517.000 [1344.000, 5675.000],  loss: 36.884579, mae: 14.480469, mean_q: 37.832901\n",
            " 22952/30000: episode: 3827, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3289.167 [339.000, 5283.000],  loss: 50.927948, mae: 14.382625, mean_q: 37.518764\n",
            " 22958/30000: episode: 3828, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3781.333 [3083.000, 3921.000],  loss: 46.582901, mae: 13.823666, mean_q: 37.006767\n",
            " 22964/30000: episode: 3829, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1887.500 [245.000, 3921.000],  loss: 47.426708, mae: 15.191487, mean_q: 38.988873\n",
            " 22970/30000: episode: 3830, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3128.500 [186.000, 4134.000],  loss: 45.705708, mae: 13.862713, mean_q: 35.807087\n",
            " 22976/30000: episode: 3831, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3187.333 [1628.000, 4410.000],  loss: 59.308826, mae: 13.577438, mean_q: 36.334503\n",
            " 22982/30000: episode: 3832, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3452.000 [1710.000, 5597.000],  loss: 32.606148, mae: 14.304343, mean_q: 37.288162\n",
            " 22988/30000: episode: 3833, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2463.000 [238.000, 3878.000],  loss: 34.143799, mae: 13.036021, mean_q: 34.578480\n",
            " 22994/30000: episode: 3834, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3476.833 [1988.000, 3921.000],  loss: 37.892979, mae: 14.815262, mean_q: 38.471020\n",
            " 23000/30000: episode: 3835, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3794.167 [2277.000, 4939.000],  loss: 48.768341, mae: 14.411775, mean_q: 38.110798\n",
            " 23006/30000: episode: 3836, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3491.167 [1710.000, 5426.000],  loss: 39.736935, mae: 12.671623, mean_q: 34.210835\n",
            " 23012/30000: episode: 3837, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3368.000 [145.000, 5634.000],  loss: 44.726639, mae: 14.878124, mean_q: 38.347439\n",
            " 23018/30000: episode: 3838, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3130.667 [1338.000, 5400.000],  loss: 61.543865, mae: 13.469602, mean_q: 35.343609\n",
            " 23024/30000: episode: 3839, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2444.667 [555.000, 4303.000],  loss: 49.733402, mae: 13.480720, mean_q: 35.054016\n",
            " 23030/30000: episode: 3840, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 35.000, mean reward:  5.833 [ 0.000, 15.000], mean action: 3631.500 [2448.000, 5651.000],  loss: 87.434181, mae: 13.360614, mean_q: 35.428608\n",
            " 23036/30000: episode: 3841, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 20.000, mean reward:  3.333 [ 0.000, 10.000], mean action: 2280.167 [555.000, 4403.000],  loss: 46.900654, mae: 14.161442, mean_q: 37.186405\n",
            " 23042/30000: episode: 3842, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [-5.000, 10.000], mean action: 2263.667 [145.000, 3951.000],  loss: 59.730145, mae: 13.842636, mean_q: 36.865620\n",
            " 23048/30000: episode: 3843, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 3497.000 [1736.000, 5656.000],  loss: 38.875999, mae: 13.996742, mean_q: 36.882198\n",
            " 23054/30000: episode: 3844, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3603.833 [1628.000, 5129.000],  loss: 70.655174, mae: 14.302170, mean_q: 37.283752\n",
            " 23060/30000: episode: 3845, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3971.333 [2107.000, 5550.000],  loss: 77.145638, mae: 14.474208, mean_q: 37.450558\n",
            " 23066/30000: episode: 3846, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2773.333 [145.000, 5089.000],  loss: 66.957794, mae: 15.026787, mean_q: 37.971439\n",
            " 23072/30000: episode: 3847, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1928.833 [547.000, 3083.000],  loss: 56.328098, mae: 13.961544, mean_q: 36.680134\n",
            " 23078/30000: episode: 3848, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1027.000 [245.000, 3220.000],  loss: 49.215698, mae: 14.475784, mean_q: 37.483139\n",
            " 23084/30000: episode: 3849, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3272.667 [974.000, 5737.000],  loss: 40.299374, mae: 14.398583, mean_q: 37.220184\n",
            " 23090/30000: episode: 3850, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 20.000, mean reward:  3.333 [-5.000, 20.000], mean action: 2301.833 [1667.000, 3921.000],  loss: 55.066456, mae: 14.538678, mean_q: 37.877277\n",
            " 23096/30000: episode: 3851, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: 30.000, mean reward:  5.000 [ 0.000, 25.000], mean action: 3306.833 [1512.000, 5608.000],  loss: 40.787395, mae: 14.001446, mean_q: 37.097321\n",
            " 23102/30000: episode: 3852, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3299.000 [2205.000, 3921.000],  loss: 46.034454, mae: 12.856126, mean_q: 34.400333\n",
            " 23108/30000: episode: 3853, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2665.500 [225.000, 4397.000],  loss: 46.891552, mae: 14.373788, mean_q: 37.212337\n",
            " 23114/30000: episode: 3854, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 140.000, mean reward: 23.333 [ 0.000, 35.000], mean action: 3162.167 [1573.000, 4185.000],  loss: 35.735340, mae: 12.867283, mean_q: 33.912388\n",
            " 23120/30000: episode: 3855, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2747.000 [1958.000, 3921.000],  loss: 66.511009, mae: 15.601329, mean_q: 39.841770\n",
            " 23126/30000: episode: 3856, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3536.167 [2727.000, 4163.000],  loss: 63.262939, mae: 13.642924, mean_q: 35.584164\n",
            " 23132/30000: episode: 3857, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2382.000 [145.000, 4905.000],  loss: 51.998768, mae: 13.721981, mean_q: 35.621784\n",
            " 23138/30000: episode: 3858, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 95.000, mean reward: 15.833 [ 0.000, 45.000], mean action: 2239.500 [852.000, 3103.000],  loss: 47.763107, mae: 14.272720, mean_q: 37.159969\n",
            " 23144/30000: episode: 3859, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2107.333 [245.000, 4576.000],  loss: 53.001129, mae: 14.857719, mean_q: 37.840816\n",
            " 23150/30000: episode: 3860, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 35.000, mean reward:  5.833 [ 0.000, 25.000], mean action: 3816.833 [1118.000, 5494.000],  loss: 47.011005, mae: 13.679623, mean_q: 35.618465\n",
            " 23156/30000: episode: 3861, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3540.833 [1776.000, 5400.000],  loss: 30.383570, mae: 12.948962, mean_q: 33.944126\n",
            " 23162/30000: episode: 3862, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2901.000 [700.000, 5086.000],  loss: 45.995159, mae: 15.113822, mean_q: 37.809559\n",
            " 23168/30000: episode: 3863, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3059.667 [578.000, 4516.000],  loss: 53.140888, mae: 14.130803, mean_q: 38.436481\n",
            " 23174/30000: episode: 3864, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3163.500 [578.000, 5395.000],  loss: 61.153637, mae: 14.579782, mean_q: 37.794910\n",
            " 23180/30000: episode: 3865, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1899.000 [578.000, 3249.000],  loss: 72.727623, mae: 15.218305, mean_q: 38.912537\n",
            " 23186/30000: episode: 3866, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2311.833 [648.000, 5656.000],  loss: 62.433384, mae: 13.063775, mean_q: 34.082737\n",
            " 23192/30000: episode: 3867, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4205.667 [145.000, 5675.000],  loss: 45.394150, mae: 13.702250, mean_q: 35.944942\n",
            " 23198/30000: episode: 3868, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2834.000 [356.000, 4332.000],  loss: 57.159328, mae: 14.007980, mean_q: 36.463505\n",
            " 23204/30000: episode: 3869, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3268.000 [578.000, 5400.000],  loss: 66.804558, mae: 13.094010, mean_q: 35.517120\n",
            " 23210/30000: episode: 3870, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3939.667 [2277.000, 4982.000],  loss: 41.747814, mae: 14.263569, mean_q: 36.309383\n",
            " 23216/30000: episode: 3871, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3777.500 [2277.000, 4775.000],  loss: 64.703583, mae: 14.565418, mean_q: 37.351765\n",
            " 23222/30000: episode: 3872, duration: 0.274s, episode steps:   6, steps per second:  22, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3751.833 [2277.000, 5506.000],  loss: 42.474621, mae: 13.763074, mean_q: 35.899883\n",
            " 23228/30000: episode: 3873, duration: 0.265s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2605.167 [958.000, 3921.000],  loss: 55.693981, mae: 13.564629, mean_q: 35.153667\n",
            " 23234/30000: episode: 3874, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2546.500 [62.000, 5486.000],  loss: 99.627892, mae: 16.083731, mean_q: 41.738674\n",
            " 23240/30000: episode: 3875, duration: 0.245s, episode steps:   6, steps per second:  24, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 3169.333 [982.000, 5256.000],  loss: 21.241385, mae: 13.649582, mean_q: 35.952675\n",
            " 23246/30000: episode: 3876, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3258.333 [580.000, 5598.000],  loss: 48.547207, mae: 14.144633, mean_q: 36.842533\n",
            " 23252/30000: episode: 3877, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3759.167 [1540.000, 4939.000],  loss: 49.710724, mae: 14.371413, mean_q: 37.375011\n",
            " 23258/30000: episode: 3878, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3922.000 [2277.000, 4251.000],  loss: 59.259876, mae: 13.709462, mean_q: 35.885086\n",
            " 23264/30000: episode: 3879, duration: 0.215s, episode steps:   6, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3087.167 [2000.000, 4742.000],  loss: 58.699116, mae: 15.787709, mean_q: 40.287731\n",
            " 23270/30000: episode: 3880, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2793.167 [1202.000, 3921.000],  loss: 36.404705, mae: 13.787313, mean_q: 36.773586\n",
            " 23276/30000: episode: 3881, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3403.000 [1540.000, 5400.000],  loss: 59.622768, mae: 14.666481, mean_q: 37.318447\n",
            " 23282/30000: episode: 3882, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2018.167 [238.000, 3083.000],  loss: 44.595688, mae: 15.177812, mean_q: 38.499592\n",
            " 23288/30000: episode: 3883, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 50.000, mean reward:  8.333 [ 0.000, 30.000], mean action: 2343.833 [551.000, 5385.000],  loss: 57.151993, mae: 14.122067, mean_q: 36.459019\n",
            " 23294/30000: episode: 3884, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3541.500 [1540.000, 5598.000],  loss: 41.611088, mae: 14.159599, mean_q: 37.572109\n",
            " 23300/30000: episode: 3885, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 20.000, mean reward:  3.333 [ 0.000, 10.000], mean action: 2001.167 [709.000, 3265.000],  loss: 52.580303, mae: 14.595825, mean_q: 37.689651\n",
            " 23306/30000: episode: 3886, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3104.833 [1334.000, 4909.000],  loss: 41.798370, mae: 14.031906, mean_q: 36.856350\n",
            " 23312/30000: episode: 3887, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3319.000 [1109.000, 5456.000],  loss: 55.776489, mae: 13.835263, mean_q: 36.277416\n",
            " 23318/30000: episode: 3888, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3953.333 [1381.000, 5598.000],  loss: 66.358498, mae: 14.035470, mean_q: 36.757545\n",
            " 23324/30000: episode: 3889, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 125.000, mean reward: 20.833 [ 0.000, 35.000], mean action: 3076.000 [162.000, 4185.000],  loss: 36.568363, mae: 13.775590, mean_q: 36.462524\n",
            " 23330/30000: episode: 3890, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2804.833 [1112.000, 4289.000],  loss: 67.829445, mae: 14.858017, mean_q: 38.070988\n",
            " 23336/30000: episode: 3891, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 2365.167 [1540.000, 3231.000],  loss: 101.791359, mae: 15.097674, mean_q: 38.520107\n",
            " 23342/30000: episode: 3892, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2236.333 [534.000, 4742.000],  loss: 42.473682, mae: 14.974704, mean_q: 38.460560\n",
            " 23348/30000: episode: 3893, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2117.167 [555.000, 4185.000],  loss: 78.618279, mae: 14.209508, mean_q: 37.630421\n",
            " 23354/30000: episode: 3894, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3048.500 [870.000, 4284.000],  loss: 70.685356, mae: 15.269775, mean_q: 39.946117\n",
            " 23360/30000: episode: 3895, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2276.833 [339.000, 4616.000],  loss: 48.500656, mae: 13.915250, mean_q: 36.645985\n",
            " 23366/30000: episode: 3896, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 1540.000 [1540.000, 1540.000],  loss: 75.418655, mae: 14.634295, mean_q: 38.166599\n",
            " 23372/30000: episode: 3897, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 1540.000 [1540.000, 1540.000],  loss: 42.971775, mae: 13.438672, mean_q: 35.782192\n",
            " 23378/30000: episode: 3898, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3223.333 [1540.000, 5140.000],  loss: 51.716572, mae: 13.384505, mean_q: 35.952267\n",
            " 23384/30000: episode: 3899, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4760.833 [1540.000, 5486.000],  loss: 48.645275, mae: 13.574103, mean_q: 36.619244\n",
            " 23390/30000: episode: 3900, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2510.167 [588.000, 5011.000],  loss: 53.144573, mae: 14.225904, mean_q: 37.786442\n",
            " 23396/30000: episode: 3901, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 1540.000 [1540.000, 1540.000],  loss: 62.359375, mae: 14.154007, mean_q: 37.660160\n",
            " 23402/30000: episode: 3902, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2241.667 [1540.000, 5750.000],  loss: 58.082394, mae: 14.657608, mean_q: 37.937077\n",
            " 23408/30000: episode: 3903, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3546.500 [1540.000, 5598.000],  loss: 31.746515, mae: 14.159053, mean_q: 37.652325\n",
            " 23414/30000: episode: 3904, duration: 0.148s, episode steps:   6, steps per second:  41, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2555.333 [551.000, 4518.000],  loss: 79.173660, mae: 14.519206, mean_q: 37.975998\n",
            " 23420/30000: episode: 3905, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2795.333 [700.000, 5568.000],  loss: 49.771923, mae: 14.149812, mean_q: 37.518963\n",
            " 23426/30000: episode: 3906, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1785.667 [1540.000, 2277.000],  loss: 75.198784, mae: 14.873082, mean_q: 39.637768\n",
            " 23432/30000: episode: 3907, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2785.000 [1539.000, 5078.000],  loss: 59.851612, mae: 14.868984, mean_q: 39.073421\n",
            " 23438/30000: episode: 3908, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3082.500 [1344.000, 4176.000],  loss: 38.540638, mae: 13.464519, mean_q: 35.239605\n",
            " 23444/30000: episode: 3909, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2966.000 [1190.000, 5161.000],  loss: 37.572483, mae: 14.977418, mean_q: 39.495392\n",
            " 23450/30000: episode: 3910, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: 30.000, mean reward:  5.000 [ 0.000, 20.000], mean action: 3928.500 [1790.000, 5463.000],  loss: 46.179722, mae: 12.994491, mean_q: 34.974285\n",
            " 23456/30000: episode: 3911, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1495.833 [393.000, 4251.000],  loss: 63.880024, mae: 13.609577, mean_q: 36.038036\n",
            " 23462/30000: episode: 3912, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3576.500 [2008.000, 5593.000],  loss: 34.059582, mae: 13.898114, mean_q: 36.482853\n",
            " 23468/30000: episode: 3913, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2899.667 [2277.000, 4459.000],  loss: 42.686710, mae: 14.055076, mean_q: 36.787521\n",
            " 23474/30000: episode: 3914, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1365.500 [238.000, 4082.000],  loss: 56.337589, mae: 13.751728, mean_q: 36.591274\n",
            " 23480/30000: episode: 3915, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3670.167 [1963.000, 4847.000],  loss: 51.327393, mae: 13.378472, mean_q: 34.974110\n",
            " 23486/30000: episode: 3916, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 2228.333 [158.000, 4185.000],  loss: 59.046490, mae: 14.133232, mean_q: 37.115067\n",
            " 23492/30000: episode: 3917, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2635.500 [350.000, 4460.000],  loss: 54.034687, mae: 15.352898, mean_q: 39.397938\n",
            " 23498/30000: episode: 3918, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1929.000 [245.000, 3658.000],  loss: 45.099091, mae: 14.688377, mean_q: 37.908905\n",
            " 23504/30000: episode: 3919, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 2308.833 [238.000, 4360.000],  loss: 39.277779, mae: 14.037674, mean_q: 37.480511\n",
            " 23510/30000: episode: 3920, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3802.500 [1419.000, 5282.000],  loss: 60.468136, mae: 14.546502, mean_q: 37.921055\n",
            " 23516/30000: episode: 3921, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2900.167 [163.000, 5597.000],  loss: 48.748753, mae: 12.802040, mean_q: 34.251816\n",
            " 23522/30000: episode: 3922, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3065.500 [238.000, 5629.000],  loss: 37.457012, mae: 14.007581, mean_q: 36.812160\n",
            " 23528/30000: episode: 3923, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3222.167 [1344.000, 4742.000],  loss: 41.159771, mae: 13.792241, mean_q: 36.296413\n",
            " 23534/30000: episode: 3924, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1354.833 [451.000, 2544.000],  loss: 61.387218, mae: 13.457489, mean_q: 36.116383\n",
            " 23540/30000: episode: 3925, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3343.333 [1596.000, 5597.000],  loss: 51.713314, mae: 13.925675, mean_q: 36.619671\n",
            " 23546/30000: episode: 3926, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1922.167 [238.000, 4144.000],  loss: 53.081142, mae: 13.064384, mean_q: 34.988903\n",
            " 23552/30000: episode: 3927, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 115.000, mean reward: 19.167 [10.000, 25.000], mean action: 2091.667 [1338.000, 4185.000],  loss: 47.317062, mae: 14.626796, mean_q: 36.933186\n",
            " 23558/30000: episode: 3928, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [-5.000, 10.000], mean action: 1752.667 [1496.000, 2277.000],  loss: 34.784267, mae: 14.035824, mean_q: 36.504604\n",
            " 23564/30000: episode: 3929, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3021.833 [117.000, 5597.000],  loss: 75.231163, mae: 14.807202, mean_q: 38.202320\n",
            " 23570/30000: episode: 3930, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2286.333 [238.000, 3712.000],  loss: 52.989689, mae: 14.991233, mean_q: 37.706478\n",
            " 23576/30000: episode: 3931, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2210.500 [238.000, 5597.000],  loss: 63.979908, mae: 13.549702, mean_q: 35.771778\n",
            " 23582/30000: episode: 3932, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3503.833 [1258.000, 5597.000],  loss: 51.401749, mae: 13.945122, mean_q: 36.780575\n",
            " 23588/30000: episode: 3933, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2560.333 [1032.000, 4573.000],  loss: 34.833118, mae: 13.436376, mean_q: 35.606815\n",
            " 23594/30000: episode: 3934, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3527.167 [238.000, 5390.000],  loss: 45.069534, mae: 13.770322, mean_q: 35.473660\n",
            " 23600/30000: episode: 3935, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3031.500 [534.000, 5579.000],  loss: 51.889378, mae: 14.179404, mean_q: 37.015347\n",
            " 23606/30000: episode: 3936, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4036.833 [712.000, 5597.000],  loss: 58.404037, mae: 15.013637, mean_q: 38.496529\n",
            " 23612/30000: episode: 3937, duration: 0.187s, episode steps:   6, steps per second:  32, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2466.500 [238.000, 3554.000],  loss: 52.023701, mae: 14.616226, mean_q: 37.681808\n",
            " 23618/30000: episode: 3938, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1834.000 [572.000, 5597.000],  loss: 68.166931, mae: 14.191413, mean_q: 37.076935\n",
            " 23624/30000: episode: 3939, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1419.833 [238.000, 3678.000],  loss: 64.132286, mae: 14.167266, mean_q: 36.682232\n",
            " 23630/30000: episode: 3940, duration: 0.210s, episode steps:   6, steps per second:  29, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2604.667 [1901.000, 4921.000],  loss: 53.529232, mae: 13.177674, mean_q: 34.175171\n",
            " 23636/30000: episode: 3941, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward: 70.000, mean reward: 11.667 [ 0.000, 30.000], mean action: 2243.500 [238.000, 4978.000],  loss: 50.421387, mae: 12.645405, mean_q: 33.853901\n",
            " 23642/30000: episode: 3942, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2307.500 [1224.000, 4185.000],  loss: 98.121559, mae: 13.853944, mean_q: 36.380039\n",
            " 23648/30000: episode: 3943, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2848.833 [650.000, 4974.000],  loss: 45.830677, mae: 14.274587, mean_q: 36.898102\n",
            " 23654/30000: episode: 3944, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4682.167 [2994.000, 5719.000],  loss: 56.709564, mae: 13.921809, mean_q: 36.655334\n",
            " 23660/30000: episode: 3945, duration: 0.240s, episode steps:   6, steps per second:  25, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2804.500 [578.000, 4185.000],  loss: 50.928192, mae: 14.294995, mean_q: 36.702862\n",
            " 23666/30000: episode: 3946, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward: 30.000, mean reward:  5.000 [-5.000, 25.000], mean action: 2200.667 [124.000, 4250.000],  loss: 24.143415, mae: 13.341267, mean_q: 35.036999\n",
            " 23672/30000: episode: 3947, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3655.333 [648.000, 4847.000],  loss: 48.612930, mae: 13.246533, mean_q: 34.931412\n",
            " 23678/30000: episode: 3948, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2807.667 [712.000, 4454.000],  loss: 64.115929, mae: 13.105157, mean_q: 34.656216\n",
            " 23684/30000: episode: 3949, duration: 0.265s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2240.500 [578.000, 3249.000],  loss: 34.911182, mae: 13.681155, mean_q: 35.969959\n",
            " 23690/30000: episode: 3950, duration: 0.265s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4404.667 [3249.000, 5597.000],  loss: 59.472610, mae: 13.727715, mean_q: 35.890274\n",
            " 23696/30000: episode: 3951, duration: 0.269s, episode steps:   6, steps per second:  22, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2145.000 [356.000, 3118.000],  loss: 35.874577, mae: 13.887268, mean_q: 35.488937\n",
            " 23702/30000: episode: 3952, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 40.000, mean reward:  6.667 [ 0.000, 15.000], mean action: 2610.667 [1596.000, 4185.000],  loss: 42.183704, mae: 14.042747, mean_q: 36.990429\n",
            " 23708/30000: episode: 3953, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4295.500 [2455.000, 5554.000],  loss: 36.567631, mae: 13.708093, mean_q: 35.905819\n",
            " 23714/30000: episode: 3954, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2767.667 [1202.000, 3475.000],  loss: 31.368757, mae: 13.862002, mean_q: 36.055035\n",
            " 23720/30000: episode: 3955, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 45.000, mean reward:  7.500 [-5.000, 25.000], mean action: 1986.000 [700.000, 2818.000],  loss: 44.255314, mae: 13.414234, mean_q: 35.420979\n",
            " 23726/30000: episode: 3956, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3547.667 [2335.000, 4450.000],  loss: 77.447304, mae: 14.543330, mean_q: 36.954372\n",
            " 23732/30000: episode: 3957, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3585.167 [1202.000, 4979.000],  loss: 53.636303, mae: 13.959643, mean_q: 36.202305\n",
            " 23738/30000: episode: 3958, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3556.000 [1554.000, 4522.000],  loss: 39.820801, mae: 13.421410, mean_q: 35.511372\n",
            " 23744/30000: episode: 3959, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2909.167 [547.000, 4742.000],  loss: 49.776783, mae: 13.669578, mean_q: 36.411045\n",
            " 23750/30000: episode: 3960, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3037.667 [493.000, 5675.000],  loss: 36.823666, mae: 13.792249, mean_q: 36.730118\n",
            " 23756/30000: episode: 3961, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3426.833 [277.000, 4450.000],  loss: 66.755821, mae: 13.172504, mean_q: 34.688591\n",
            " 23762/30000: episode: 3962, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2689.000 [280.000, 4678.000],  loss: 42.941181, mae: 13.092700, mean_q: 34.597469\n",
            " 23768/30000: episode: 3963, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2528.667 [1015.000, 5538.000],  loss: 46.549603, mae: 13.862834, mean_q: 36.501011\n",
            " 23774/30000: episode: 3964, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3535.500 [245.000, 5517.000],  loss: 33.556767, mae: 12.925847, mean_q: 35.661102\n",
            " 23780/30000: episode: 3965, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2975.000 [1381.000, 4939.000],  loss: 38.277660, mae: 13.427658, mean_q: 35.318195\n",
            " 23786/30000: episode: 3966, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 3569.000 [1596.000, 4925.000],  loss: 42.950581, mae: 13.739136, mean_q: 35.954037\n",
            " 23792/30000: episode: 3967, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3761.500 [1613.000, 4450.000],  loss: 32.129086, mae: 14.283340, mean_q: 37.278576\n",
            " 23798/30000: episode: 3968, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2442.500 [238.000, 5737.000],  loss: 55.147766, mae: 14.031788, mean_q: 37.240177\n",
            " 23804/30000: episode: 3969, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2879.000 [578.000, 5619.000],  loss: 44.917816, mae: 13.387611, mean_q: 35.243011\n",
            " 23810/30000: episode: 3970, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3447.667 [1901.000, 4450.000],  loss: 44.888233, mae: 12.404256, mean_q: 32.495708\n",
            " 23816/30000: episode: 3971, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3567.000 [1554.000, 4450.000],  loss: 47.559780, mae: 13.583095, mean_q: 36.638233\n",
            " 23822/30000: episode: 3972, duration: 0.148s, episode steps:   6, steps per second:  40, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3374.500 [1041.000, 5074.000],  loss: 42.540768, mae: 14.266247, mean_q: 37.103703\n",
            " 23828/30000: episode: 3973, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 30.000, mean reward:  5.000 [ 0.000, 20.000], mean action: 1439.500 [578.000, 2074.000],  loss: 54.111156, mae: 12.936915, mean_q: 34.764637\n",
            " 23834/30000: episode: 3974, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 40.000, mean reward:  6.667 [ 0.000, 20.000], mean action: 2990.000 [642.000, 5538.000],  loss: 48.386730, mae: 13.057948, mean_q: 35.074345\n",
            " 23840/30000: episode: 3975, duration: 0.296s, episode steps:   6, steps per second:  20, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3275.500 [238.000, 5706.000],  loss: 77.903297, mae: 13.660115, mean_q: 36.355503\n",
            " 23846/30000: episode: 3976, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3516.500 [578.000, 5538.000],  loss: 45.888386, mae: 13.419639, mean_q: 35.487400\n",
            " 23852/30000: episode: 3977, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2272.500 [140.000, 3951.000],  loss: 57.564655, mae: 14.151993, mean_q: 35.921932\n",
            " 23858/30000: episode: 3978, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 85.000, mean reward: 14.167 [-5.000, 35.000], mean action: 2605.167 [238.000, 4185.000],  loss: 54.455189, mae: 14.218651, mean_q: 37.789776\n",
            " 23864/30000: episode: 3979, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4004.167 [1834.000, 5675.000],  loss: 31.817116, mae: 13.650475, mean_q: 36.311115\n",
            " 23870/30000: episode: 3980, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [-5.000, 20.000], mean action: 3814.833 [578.000, 4742.000],  loss: 46.248783, mae: 14.910332, mean_q: 38.013371\n",
            " 23876/30000: episode: 3981, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3259.000 [578.000, 5161.000],  loss: 44.013844, mae: 14.491446, mean_q: 37.314774\n",
            " 23882/30000: episode: 3982, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3095.500 [852.000, 4921.000],  loss: 100.687958, mae: 14.139140, mean_q: 37.330372\n",
            " 23888/30000: episode: 3983, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 150.000, mean reward: 25.000 [ 0.000, 35.000], mean action: 2374.333 [238.000, 4185.000],  loss: 41.817280, mae: 14.130656, mean_q: 36.058178\n",
            " 23894/30000: episode: 3984, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4136.833 [2868.000, 5241.000],  loss: 46.602383, mae: 13.030131, mean_q: 34.496143\n",
            " 23900/30000: episode: 3985, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [ 0.000, 10.000], mean action: 3904.500 [578.000, 5656.000],  loss: 57.101536, mae: 15.426345, mean_q: 40.029022\n",
            " 23906/30000: episode: 3986, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 2579.500 [238.000, 5620.000],  loss: 55.568741, mae: 13.686624, mean_q: 36.339603\n",
            " 23912/30000: episode: 3987, duration: 0.185s, episode steps:   6, steps per second:  32, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2336.167 [68.000, 3498.000],  loss: 61.669079, mae: 14.076233, mean_q: 37.736557\n",
            " 23918/30000: episode: 3988, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2731.000 [578.000, 5212.000],  loss: 35.642830, mae: 15.020286, mean_q: 39.398643\n",
            " 23924/30000: episode: 3989, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2843.833 [273.000, 5538.000],  loss: 70.080208, mae: 15.794682, mean_q: 40.929596\n",
            " 23930/30000: episode: 3990, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1802.000 [238.000, 3475.000],  loss: 38.877983, mae: 13.670672, mean_q: 36.867695\n",
            " 23936/30000: episode: 3991, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3163.833 [578.000, 5348.000],  loss: 39.057186, mae: 12.975544, mean_q: 35.444191\n",
            " 23942/30000: episode: 3992, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 2120.000 [269.000, 5538.000],  loss: 68.764366, mae: 14.038506, mean_q: 38.222698\n",
            " 23948/30000: episode: 3993, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3642.000 [578.000, 5538.000],  loss: 65.933617, mae: 13.558533, mean_q: 36.608662\n",
            " 23954/30000: episode: 3994, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 115.000, mean reward: 19.167 [ 0.000, 35.000], mean action: 3615.667 [238.000, 4819.000],  loss: 59.482269, mae: 13.774067, mean_q: 37.022930\n",
            " 23960/30000: episode: 3995, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3716.833 [1937.000, 5675.000],  loss: 43.254669, mae: 14.118491, mean_q: 37.119518\n",
            " 23966/30000: episode: 3996, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3564.833 [1942.000, 4742.000],  loss: 55.452621, mae: 13.905574, mean_q: 37.469315\n",
            " 23972/30000: episode: 3997, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3912.167 [907.000, 5218.000],  loss: 39.817978, mae: 13.884341, mean_q: 37.708050\n",
            " 23978/30000: episode: 3998, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4852.333 [3658.000, 5675.000],  loss: 39.055374, mae: 13.659333, mean_q: 38.003475\n",
            " 23984/30000: episode: 3999, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4377.667 [2297.000, 5661.000],  loss: 49.146160, mae: 15.343738, mean_q: 40.157635\n",
            " 23990/30000: episode: 4000, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 35.000, mean reward:  5.833 [-5.000, 40.000], mean action: 1922.667 [57.000, 3284.000],  loss: 51.468830, mae: 14.464751, mean_q: 38.720776\n",
            " 23996/30000: episode: 4001, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3448.333 [1793.000, 5449.000],  loss: 37.987476, mae: 13.892102, mean_q: 37.565399\n",
            " 24002/30000: episode: 4002, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2017.167 [249.000, 5367.000],  loss: 54.909595, mae: 13.100972, mean_q: 35.576698\n",
            " 24008/30000: episode: 4003, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2502.500 [12.000, 5656.000],  loss: 58.514282, mae: 14.008004, mean_q: 38.362675\n",
            " 24014/30000: episode: 4004, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3728.667 [3658.000, 4082.000],  loss: 37.991550, mae: 12.579810, mean_q: 34.063496\n",
            " 24020/30000: episode: 4005, duration: 0.185s, episode steps:   6, steps per second:  32, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3191.667 [555.000, 4444.000],  loss: 53.712635, mae: 14.264732, mean_q: 37.263165\n",
            " 24026/30000: episode: 4006, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2624.500 [2444.000, 3083.000],  loss: 38.332409, mae: 14.411387, mean_q: 38.077854\n",
            " 24032/30000: episode: 4007, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3009.167 [550.000, 5620.000],  loss: 28.567987, mae: 13.930333, mean_q: 37.569828\n",
            " 24038/30000: episode: 4008, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2932.167 [578.000, 5383.000],  loss: 43.200638, mae: 14.022130, mean_q: 36.907963\n",
            " 24044/30000: episode: 4009, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 4543.833 [2745.000, 5620.000],  loss: 49.295063, mae: 12.478728, mean_q: 34.849995\n",
            " 24050/30000: episode: 4010, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3296.333 [238.000, 5159.000],  loss: 50.211121, mae: 13.263077, mean_q: 34.589859\n",
            " 24056/30000: episode: 4011, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2457.833 [578.000, 5674.000],  loss: 49.912861, mae: 14.197062, mean_q: 37.294880\n",
            " 24062/30000: episode: 4012, duration: 0.232s, episode steps:   6, steps per second:  26, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3655.000 [712.000, 4444.000],  loss: 60.256901, mae: 14.290192, mean_q: 37.270138\n",
            " 24068/30000: episode: 4013, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2968.667 [1387.000, 4957.000],  loss: 67.838165, mae: 13.636211, mean_q: 36.363293\n",
            " 24074/30000: episode: 4014, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2945.667 [578.000, 4616.000],  loss: 49.241566, mae: 12.887900, mean_q: 34.426910\n",
            " 24080/30000: episode: 4015, duration: 0.241s, episode steps:   6, steps per second:  25, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4405.667 [1021.000, 5538.000],  loss: 41.694962, mae: 13.678041, mean_q: 36.436832\n",
            " 24086/30000: episode: 4016, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 4041.667 [608.000, 5523.000],  loss: 48.993732, mae: 13.497954, mean_q: 36.595604\n",
            " 24092/30000: episode: 4017, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3348.000 [578.000, 4616.000],  loss: 65.280243, mae: 13.439767, mean_q: 36.325943\n",
            " 24098/30000: episode: 4018, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2156.167 [578.000, 5424.000],  loss: 48.496571, mae: 13.133906, mean_q: 35.818134\n",
            " 24104/30000: episode: 4019, duration: 0.237s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2488.500 [712.000, 4450.000],  loss: 46.261295, mae: 14.095250, mean_q: 37.201641\n",
            " 24110/30000: episode: 4020, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1806.833 [238.000, 5382.000],  loss: 56.929920, mae: 13.546871, mean_q: 37.130711\n",
            " 24116/30000: episode: 4021, duration: 0.238s, episode steps:   6, steps per second:  25, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 667.333 [578.000, 712.000],  loss: 52.636585, mae: 14.000988, mean_q: 36.784451\n",
            " 24122/30000: episode: 4022, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward: 25.000, mean reward:  4.167 [-5.000, 15.000], mean action: 2545.667 [578.000, 4250.000],  loss: 56.547501, mae: 13.719158, mean_q: 36.814018\n",
            " 24128/30000: episode: 4023, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3745.000 [238.000, 5620.000],  loss: 58.768215, mae: 14.262786, mean_q: 38.021313\n",
            " 24134/30000: episode: 4024, duration: 0.240s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2644.167 [1454.000, 4464.000],  loss: 69.873680, mae: 14.943728, mean_q: 38.950859\n",
            " 24140/30000: episode: 4025, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 3072.833 [1495.000, 4973.000],  loss: 61.772064, mae: 13.201344, mean_q: 35.950542\n",
            " 24146/30000: episode: 4026, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 70.000, mean reward: 11.667 [ 0.000, 40.000], mean action: 1522.833 [183.000, 2462.000],  loss: 63.144119, mae: 14.151864, mean_q: 37.567020\n",
            " 24152/30000: episode: 4027, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 130.000, mean reward: 21.667 [ 0.000, 35.000], mean action: 2453.333 [1530.000, 4185.000],  loss: 31.392954, mae: 12.090510, mean_q: 33.127872\n",
            " 24158/30000: episode: 4028, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 1965.000 [578.000, 4185.000],  loss: 39.807373, mae: 12.937939, mean_q: 35.293407\n",
            " 24164/30000: episode: 4029, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2655.833 [1057.000, 5656.000],  loss: 51.685345, mae: 15.834427, mean_q: 41.527973\n",
            " 24170/30000: episode: 4030, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2530.167 [578.000, 4360.000],  loss: 36.601608, mae: 13.565361, mean_q: 36.271473\n",
            " 24176/30000: episode: 4031, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2731.667 [578.000, 4725.000],  loss: 40.609322, mae: 14.832795, mean_q: 38.968456\n",
            " 24182/30000: episode: 4032, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2764.667 [462.000, 5620.000],  loss: 37.506550, mae: 14.024349, mean_q: 37.651814\n",
            " 24188/30000: episode: 4033, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3486.167 [527.000, 5433.000],  loss: 57.371464, mae: 14.209270, mean_q: 37.725567\n",
            " 24194/30000: episode: 4034, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 145.000, mean reward: 24.167 [ 0.000, 35.000], mean action: 3152.333 [578.000, 4185.000],  loss: 53.810902, mae: 13.369275, mean_q: 36.861073\n",
            " 24200/30000: episode: 4035, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2175.833 [684.000, 4493.000],  loss: 54.458538, mae: 14.601348, mean_q: 38.408512\n",
            " 24206/30000: episode: 4036, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3015.833 [162.000, 5241.000],  loss: 51.894543, mae: 13.872849, mean_q: 37.125645\n",
            " 24212/30000: episode: 4037, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 889.000 [578.000, 2444.000],  loss: 57.573170, mae: 14.151324, mean_q: 37.759037\n",
            " 24218/30000: episode: 4038, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2383.167 [1247.000, 4444.000],  loss: 46.506504, mae: 13.643617, mean_q: 36.531620\n",
            " 24224/30000: episode: 4039, duration: 0.148s, episode steps:   6, steps per second:  41, episode reward: 160.000, mean reward: 26.667 [ 0.000, 35.000], mean action: 2717.000 [578.000, 4185.000],  loss: 48.192036, mae: 14.137914, mean_q: 38.018993\n",
            " 24230/30000: episode: 4040, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3249.333 [578.000, 5656.000],  loss: 49.044498, mae: 14.711240, mean_q: 39.725246\n",
            " 24236/30000: episode: 4041, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward: 45.000, mean reward:  7.500 [ 0.000, 30.000], mean action: 2553.000 [578.000, 4573.000],  loss: 64.185265, mae: 13.337775, mean_q: 36.244125\n",
            " 24242/30000: episode: 4042, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 55.000, mean reward:  9.167 [ 0.000, 35.000], mean action: 1478.833 [273.000, 2357.000],  loss: 59.562267, mae: 14.100010, mean_q: 38.167812\n",
            " 24248/30000: episode: 4043, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 578.000 [578.000, 578.000],  loss: 37.951782, mae: 14.334599, mean_q: 39.331989\n",
            " 24254/30000: episode: 4044, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1501.667 [27.000, 3164.000],  loss: 37.118210, mae: 13.698558, mean_q: 37.471416\n",
            " 24260/30000: episode: 4045, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3310.500 [578.000, 5532.000],  loss: 40.808697, mae: 14.251678, mean_q: 38.890137\n",
            " 24266/30000: episode: 4046, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 578.000 [578.000, 578.000],  loss: 45.681522, mae: 14.271527, mean_q: 38.735748\n",
            " 24272/30000: episode: 4047, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 578.000 [578.000, 578.000],  loss: 54.275986, mae: 15.698829, mean_q: 42.037457\n",
            " 24278/30000: episode: 4048, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3943.000 [578.000, 4616.000],  loss: 29.604475, mae: 12.195563, mean_q: 34.423595\n",
            " 24284/30000: episode: 4049, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 35.000, mean reward:  5.833 [ 0.000, 35.000], mean action: 2181.667 [578.000, 5347.000],  loss: 86.964851, mae: 14.936091, mean_q: 40.051422\n",
            " 24290/30000: episode: 4050, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2698.333 [578.000, 5576.000],  loss: 66.156776, mae: 13.766881, mean_q: 37.532436\n",
            " 24296/30000: episode: 4051, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2416.167 [578.000, 5598.000],  loss: 46.606655, mae: 13.372712, mean_q: 37.354031\n",
            " 24302/30000: episode: 4052, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2950.167 [578.000, 4616.000],  loss: 57.240032, mae: 14.495410, mean_q: 39.252232\n",
            " 24308/30000: episode: 4053, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: 40.000, mean reward:  6.667 [ 0.000, 30.000], mean action: 1953.500 [560.000, 4742.000],  loss: 42.389568, mae: 13.429654, mean_q: 36.639004\n",
            " 24314/30000: episode: 4054, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3235.000 [53.000, 4721.000],  loss: 35.327469, mae: 13.956822, mean_q: 37.012516\n",
            " 24320/30000: episode: 4055, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3632.000 [578.000, 5656.000],  loss: 43.755688, mae: 13.823453, mean_q: 36.650600\n",
            " 24326/30000: episode: 4056, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2777.833 [578.000, 5598.000],  loss: 54.503063, mae: 11.966361, mean_q: 33.783588\n",
            " 24332/30000: episode: 4057, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 578.000 [578.000, 578.000],  loss: 62.713459, mae: 13.966053, mean_q: 38.110813\n",
            " 24338/30000: episode: 4058, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1935.333 [578.000, 4185.000],  loss: 49.723637, mae: 13.461232, mean_q: 36.757137\n",
            " 24344/30000: episode: 4059, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1843.000 [187.000, 5598.000],  loss: 40.699459, mae: 12.549098, mean_q: 35.022869\n",
            " 24350/30000: episode: 4060, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1725.500 [163.000, 5282.000],  loss: 71.077019, mae: 13.580193, mean_q: 37.011600\n",
            " 24356/30000: episode: 4061, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 1916.500 [578.000, 3231.000],  loss: 56.127384, mae: 14.120894, mean_q: 38.054356\n",
            " 24362/30000: episode: 4062, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3433.500 [132.000, 5675.000],  loss: 53.185268, mae: 14.459149, mean_q: 38.281582\n",
            " 24368/30000: episode: 4063, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3630.667 [106.000, 5747.000],  loss: 60.203472, mae: 14.479796, mean_q: 38.737389\n",
            " 24374/30000: episode: 4064, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 2883.500 [578.000, 5608.000],  loss: 42.537418, mae: 12.423934, mean_q: 33.556675\n",
            " 24380/30000: episode: 4065, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2354.833 [898.000, 4185.000],  loss: 73.426949, mae: 14.676861, mean_q: 37.775032\n",
            " 24386/30000: episode: 4066, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 3028.667 [26.000, 5282.000],  loss: 40.376278, mae: 12.878262, mean_q: 34.163651\n",
            " 24392/30000: episode: 4067, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2267.833 [225.000, 5424.000],  loss: 53.904202, mae: 13.871754, mean_q: 36.326733\n",
            " 24398/30000: episode: 4068, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2664.167 [1573.000, 4185.000],  loss: 54.897869, mae: 14.593334, mean_q: 38.133755\n",
            " 24404/30000: episode: 4069, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3160.167 [578.000, 4616.000],  loss: 48.122219, mae: 13.949997, mean_q: 36.308533\n",
            " 24410/30000: episode: 4070, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3861.833 [2128.000, 5723.000],  loss: 40.879108, mae: 14.001925, mean_q: 37.041653\n",
            " 24416/30000: episode: 4071, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4531.667 [2128.000, 5282.000],  loss: 51.185364, mae: 13.343033, mean_q: 35.196323\n",
            " 24422/30000: episode: 4072, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2427.167 [161.000, 5367.000],  loss: 46.058308, mae: 13.061601, mean_q: 35.072567\n",
            " 24428/30000: episode: 4073, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2854.833 [138.000, 5620.000],  loss: 56.311939, mae: 14.126815, mean_q: 37.848499\n",
            " 24434/30000: episode: 4074, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2270.667 [808.000, 4346.000],  loss: 56.704884, mae: 14.239087, mean_q: 37.511765\n",
            " 24440/30000: episode: 4075, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward: 130.000, mean reward: 21.667 [ 0.000, 35.000], mean action: 2624.333 [578.000, 4185.000],  loss: 60.639389, mae: 13.121316, mean_q: 34.525249\n",
            " 24446/30000: episode: 4076, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3090.000 [2507.000, 4444.000],  loss: 37.878529, mae: 13.010929, mean_q: 34.664814\n",
            " 24452/30000: episode: 4077, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3308.167 [1197.000, 5503.000],  loss: 80.306702, mae: 13.834732, mean_q: 36.518047\n",
            " 24458/30000: episode: 4078, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3523.500 [808.000, 5424.000],  loss: 47.360744, mae: 13.392624, mean_q: 34.918777\n",
            " 24464/30000: episode: 4079, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3456.333 [1491.000, 5433.000],  loss: 47.377869, mae: 13.922109, mean_q: 37.850124\n",
            " 24470/30000: episode: 4080, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3273.000 [1491.000, 4981.000],  loss: 61.234116, mae: 14.021243, mean_q: 36.916534\n",
            " 24476/30000: episode: 4081, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: -15.000, mean reward: -2.500 [-5.000,  5.000], mean action: 3149.667 [2128.000, 3354.000],  loss: 55.885288, mae: 13.413894, mean_q: 36.239971\n",
            " 24482/30000: episode: 4082, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2656.333 [1439.000, 4250.000],  loss: 44.866405, mae: 13.275043, mean_q: 35.671772\n",
            " 24488/30000: episode: 4083, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2657.667 [1560.000, 4444.000],  loss: 54.719173, mae: 13.457669, mean_q: 36.249847\n",
            " 24494/30000: episode: 4084, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 3261.667 [1515.000, 5474.000],  loss: 22.880495, mae: 12.060206, mean_q: 32.695484\n",
            " 24500/30000: episode: 4085, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2593.667 [238.000, 5598.000],  loss: 61.414597, mae: 14.915647, mean_q: 39.378929\n",
            " 24506/30000: episode: 4086, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3079.000 [1483.000, 5282.000],  loss: 81.617065, mae: 14.432591, mean_q: 37.512142\n",
            " 24512/30000: episode: 4087, duration: 0.236s, episode steps:   6, steps per second:  25, episode reward: 45.000, mean reward:  7.500 [ 0.000, 30.000], mean action: 3672.500 [1573.000, 5259.000],  loss: 56.001251, mae: 13.991348, mean_q: 36.915272\n",
            " 24518/30000: episode: 4088, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 1701.667 [278.000, 3231.000],  loss: 53.082855, mae: 13.895409, mean_q: 37.354450\n",
            " 24524/30000: episode: 4089, duration: 0.233s, episode steps:   6, steps per second:  26, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3030.500 [1931.000, 4683.000],  loss: 71.420776, mae: 14.442594, mean_q: 37.886475\n",
            " 24530/30000: episode: 4090, duration: 0.238s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3007.167 [1260.000, 5188.000],  loss: 51.249065, mae: 13.903869, mean_q: 37.135479\n",
            " 24536/30000: episode: 4091, duration: 0.238s, episode steps:   6, steps per second:  25, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3145.333 [1097.000, 5598.000],  loss: 35.548531, mae: 12.498694, mean_q: 34.029762\n",
            " 24542/30000: episode: 4092, duration: 0.238s, episode steps:   6, steps per second:  25, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3107.667 [1573.000, 4185.000],  loss: 36.488148, mae: 12.638150, mean_q: 34.141422\n",
            " 24548/30000: episode: 4093, duration: 0.238s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3067.833 [873.000, 5447.000],  loss: 47.186741, mae: 14.204473, mean_q: 37.392288\n",
            " 24554/30000: episode: 4094, duration: 0.239s, episode steps:   6, steps per second:  25, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4528.000 [2831.000, 4932.000],  loss: 47.194275, mae: 13.847133, mean_q: 36.925842\n",
            " 24560/30000: episode: 4095, duration: 0.270s, episode steps:   6, steps per second:  22, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2496.000 [1338.000, 3655.000],  loss: 54.343723, mae: 13.569020, mean_q: 36.007339\n",
            " 24566/30000: episode: 4096, duration: 0.262s, episode steps:   6, steps per second:  23, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3159.333 [1573.000, 4444.000],  loss: 52.368603, mae: 15.347281, mean_q: 40.002102\n",
            " 24572/30000: episode: 4097, duration: 0.270s, episode steps:   6, steps per second:  22, episode reward: 50.000, mean reward:  8.333 [ 0.000, 30.000], mean action: 2844.000 [669.000, 4847.000],  loss: 39.365662, mae: 13.398735, mean_q: 35.564884\n",
            " 24578/30000: episode: 4098, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3062.000 [522.000, 5687.000],  loss: 33.266476, mae: 13.279796, mean_q: 35.053394\n",
            " 24584/30000: episode: 4099, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2349.500 [309.000, 4091.000],  loss: 44.993504, mae: 15.017154, mean_q: 39.418900\n",
            " 24590/30000: episode: 4100, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3314.500 [1863.000, 5656.000],  loss: 43.546314, mae: 12.546268, mean_q: 34.395298\n",
            " 24596/30000: episode: 4101, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2308.833 [826.000, 5447.000],  loss: 58.005733, mae: 13.634117, mean_q: 36.265667\n",
            " 24602/30000: episode: 4102, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4345.833 [1072.000, 5679.000],  loss: 54.678391, mae: 13.448093, mean_q: 34.932049\n",
            " 24608/30000: episode: 4103, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: 100.000, mean reward: 16.667 [ 0.000, 35.000], mean action: 2585.333 [1495.000, 4185.000],  loss: 57.606323, mae: 13.689529, mean_q: 36.188633\n",
            " 24614/30000: episode: 4104, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 65.000, mean reward: 10.833 [ 0.000, 35.000], mean action: 4489.833 [2128.000, 5687.000],  loss: 44.246799, mae: 14.397779, mean_q: 37.167355\n",
            " 24620/30000: episode: 4105, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2137.333 [393.000, 5674.000],  loss: 44.617542, mae: 14.377441, mean_q: 37.111298\n",
            " 24626/30000: episode: 4106, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2949.167 [462.000, 5058.000],  loss: 40.130589, mae: 13.944847, mean_q: 36.118324\n",
            " 24632/30000: episode: 4107, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4327.667 [2247.000, 5447.000],  loss: 29.536638, mae: 13.773860, mean_q: 36.022572\n",
            " 24638/30000: episode: 4108, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1698.667 [163.000, 3712.000],  loss: 59.331470, mae: 14.300109, mean_q: 37.525616\n",
            " 24644/30000: episode: 4109, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2632.500 [1573.000, 4185.000],  loss: 47.116482, mae: 14.541160, mean_q: 38.113678\n",
            " 24650/30000: episode: 4110, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3646.667 [1074.000, 5564.000],  loss: 51.927708, mae: 13.035504, mean_q: 34.868572\n",
            " 24656/30000: episode: 4111, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 1961.500 [887.000, 2819.000],  loss: 65.416328, mae: 13.968272, mean_q: 36.165451\n",
            " 24662/30000: episode: 4112, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3481.167 [393.000, 5171.000],  loss: 33.540165, mae: 13.010196, mean_q: 34.554226\n",
            " 24668/30000: episode: 4113, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: 20.000, mean reward:  3.333 [-5.000, 25.000], mean action: 3364.333 [1202.000, 5140.000],  loss: 53.136456, mae: 14.117737, mean_q: 36.411522\n",
            " 24674/30000: episode: 4114, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3538.667 [1202.000, 5598.000],  loss: 42.244488, mae: 14.285320, mean_q: 37.734753\n",
            " 24680/30000: episode: 4115, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3707.833 [245.000, 4681.000],  loss: 98.933899, mae: 15.618682, mean_q: 39.812439\n",
            " 24686/30000: episode: 4116, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3838.000 [2448.000, 5696.000],  loss: 56.459827, mae: 13.696698, mean_q: 35.548100\n",
            " 24692/30000: episode: 4117, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 2490.833 [132.000, 4444.000],  loss: 61.209976, mae: 12.852535, mean_q: 34.175667\n",
            " 24698/30000: episode: 4118, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2754.667 [2455.000, 3354.000],  loss: 35.112598, mae: 12.989324, mean_q: 34.657921\n",
            " 24704/30000: episode: 4119, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2637.167 [1596.000, 3712.000],  loss: 66.685661, mae: 14.168387, mean_q: 37.553188\n",
            " 24710/30000: episode: 4120, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2474.500 [459.000, 5463.000],  loss: 45.578663, mae: 13.982312, mean_q: 36.943390\n",
            " 24716/30000: episode: 4121, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 1859.667 [183.000, 3542.000],  loss: 65.206383, mae: 13.040950, mean_q: 35.653809\n",
            " 24722/30000: episode: 4122, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2642.167 [607.000, 4678.000],  loss: 71.185097, mae: 12.498627, mean_q: 33.501312\n",
            " 24728/30000: episode: 4123, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 4196.500 [607.000, 5723.000],  loss: 42.200390, mae: 13.658162, mean_q: 36.898048\n",
            " 24734/30000: episode: 4124, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2053.667 [1412.000, 4185.000],  loss: 48.598240, mae: 12.953766, mean_q: 34.501537\n",
            " 24740/30000: episode: 4125, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3280.667 [1846.000, 4632.000],  loss: 51.935513, mae: 13.699180, mean_q: 36.012474\n",
            " 24746/30000: episode: 4126, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3893.667 [1596.000, 5743.000],  loss: 56.726532, mae: 14.616478, mean_q: 37.772388\n",
            " 24752/30000: episode: 4127, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2400.667 [1358.000, 2831.000],  loss: 39.215446, mae: 13.890643, mean_q: 36.673679\n",
            " 24758/30000: episode: 4128, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2653.000 [791.000, 3667.000],  loss: 34.365540, mae: 12.896992, mean_q: 34.315273\n",
            " 24764/30000: episode: 4129, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2118.833 [1438.000, 3667.000],  loss: 32.027176, mae: 13.159015, mean_q: 34.925186\n",
            " 24770/30000: episode: 4130, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3247.500 [238.000, 5619.000],  loss: 53.002338, mae: 14.140132, mean_q: 37.169598\n",
            " 24776/30000: episode: 4131, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 2125.000 [471.000, 3199.000],  loss: 69.468025, mae: 13.330832, mean_q: 35.944958\n",
            " 24782/30000: episode: 4132, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3595.500 [2831.000, 4767.000],  loss: 46.486515, mae: 13.838061, mean_q: 36.502388\n",
            " 24788/30000: episode: 4133, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2513.667 [581.000, 4954.000],  loss: 51.205791, mae: 14.389985, mean_q: 38.128880\n",
            " 24794/30000: episode: 4134, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2967.000 [946.000, 5282.000],  loss: 58.459305, mae: 13.452907, mean_q: 36.337353\n",
            " 24800/30000: episode: 4135, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2440.333 [10.000, 4530.000],  loss: 57.365219, mae: 13.911841, mean_q: 36.921185\n",
            " 24806/30000: episode: 4136, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4903.167 [2831.000, 5629.000],  loss: 31.278969, mae: 13.558928, mean_q: 36.156952\n",
            " 24812/30000: episode: 4137, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2131.500 [581.000, 5471.000],  loss: 63.568005, mae: 14.455279, mean_q: 37.136890\n",
            " 24818/30000: episode: 4138, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2358.333 [581.000, 3763.000],  loss: 67.178177, mae: 14.402009, mean_q: 37.837097\n",
            " 24824/30000: episode: 4139, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2712.667 [1942.000, 4185.000],  loss: 57.998322, mae: 13.383575, mean_q: 35.096329\n",
            " 24830/30000: episode: 4140, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3536.667 [1227.000, 5736.000],  loss: 68.041107, mae: 14.498497, mean_q: 37.862576\n",
            " 24836/30000: episode: 4141, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4988.667 [3249.000, 5687.000],  loss: 65.193756, mae: 13.879853, mean_q: 36.352802\n",
            " 24842/30000: episode: 4142, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2995.167 [1573.000, 4185.000],  loss: 57.876629, mae: 14.071269, mean_q: 37.070946\n",
            " 24848/30000: episode: 4143, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2658.333 [1109.000, 4176.000],  loss: 57.196148, mae: 13.604548, mean_q: 36.632278\n",
            " 24854/30000: episode: 4144, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2468.833 [393.000, 5197.000],  loss: 61.413589, mae: 14.210205, mean_q: 37.435406\n",
            " 24860/30000: episode: 4145, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 4796.667 [1888.000, 5719.000],  loss: 43.147457, mae: 12.992592, mean_q: 36.110226\n",
            " 24866/30000: episode: 4146, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward: 140.000, mean reward: 23.333 [ 0.000, 35.000], mean action: 3162.167 [1573.000, 4185.000],  loss: 71.971443, mae: 13.703286, mean_q: 36.652176\n",
            " 24872/30000: episode: 4147, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1782.333 [578.000, 3249.000],  loss: 39.210743, mae: 14.132825, mean_q: 36.696335\n",
            " 24878/30000: episode: 4148, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2015.167 [238.000, 3933.000],  loss: 28.769377, mae: 13.686645, mean_q: 36.837536\n",
            " 24884/30000: episode: 4149, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3542.500 [3204.000, 4683.000],  loss: 48.184078, mae: 13.460536, mean_q: 35.218384\n",
            " 24890/30000: episode: 4150, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2074.000 [607.000, 3550.000],  loss: 46.517811, mae: 13.804837, mean_q: 36.410007\n",
            " 24896/30000: episode: 4151, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3197.167 [94.000, 5532.000],  loss: 31.884058, mae: 13.959660, mean_q: 36.620728\n",
            " 24902/30000: episode: 4152, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3570.667 [1262.000, 5523.000],  loss: 57.883762, mae: 12.602981, mean_q: 34.675522\n",
            " 24908/30000: episode: 4153, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3453.333 [2442.000, 4196.000],  loss: 48.846516, mae: 13.826749, mean_q: 36.400188\n",
            " 24914/30000: episode: 4154, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 2523.833 [502.000, 5267.000],  loss: 49.158741, mae: 13.667169, mean_q: 36.927650\n",
            " 24920/30000: episode: 4155, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2927.500 [404.000, 5447.000],  loss: 65.674889, mae: 14.351276, mean_q: 37.803455\n",
            " 24926/30000: episode: 4156, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3823.833 [1212.000, 5671.000],  loss: 60.422485, mae: 14.452991, mean_q: 38.609760\n",
            " 24932/30000: episode: 4157, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2245.667 [1253.000, 5274.000],  loss: 61.711933, mae: 13.736870, mean_q: 36.810944\n",
            " 24938/30000: episode: 4158, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2675.667 [1137.000, 5670.000],  loss: 54.919956, mae: 14.125062, mean_q: 38.823284\n",
            " 24944/30000: episode: 4159, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3464.000 [2027.000, 4982.000],  loss: 44.251675, mae: 14.639375, mean_q: 39.113804\n",
            " 24950/30000: episode: 4160, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2510.167 [229.000, 4360.000],  loss: 46.792526, mae: 14.953316, mean_q: 40.655884\n",
            " 24956/30000: episode: 4161, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward: 30.000, mean reward:  5.000 [ 0.000, 20.000], mean action: 2403.667 [394.000, 5687.000],  loss: 55.804367, mae: 13.769864, mean_q: 37.371853\n",
            " 24962/30000: episode: 4162, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3559.833 [1426.000, 4982.000],  loss: 42.079296, mae: 13.458584, mean_q: 36.714081\n",
            " 24968/30000: episode: 4163, duration: 0.235s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3579.667 [1558.000, 4926.000],  loss: 43.853413, mae: 13.398448, mean_q: 36.142406\n",
            " 24974/30000: episode: 4164, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2727.167 [224.000, 5147.000],  loss: 45.815094, mae: 13.603265, mean_q: 37.074280\n",
            " 24980/30000: episode: 4165, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2600.500 [394.000, 4840.000],  loss: 43.572628, mae: 14.679050, mean_q: 39.466511\n",
            " 24986/30000: episode: 4166, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward: 20.000, mean reward:  3.333 [-5.000, 30.000], mean action: 3446.167 [1530.000, 4767.000],  loss: 50.462955, mae: 13.186036, mean_q: 36.541607\n",
            " 24992/30000: episode: 4167, duration: 0.237s, episode steps:   6, steps per second:  25, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3638.667 [1846.000, 5329.000],  loss: 48.550831, mae: 13.969871, mean_q: 38.030254\n",
            " 24998/30000: episode: 4168, duration: 0.265s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2302.167 [717.000, 4464.000],  loss: 47.881489, mae: 14.042877, mean_q: 37.525471\n",
            " 25004/30000: episode: 4169, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward: 100.000, mean reward: 16.667 [ 0.000, 100.000], mean action: 2477.500 [238.000, 4211.000],  loss: 89.613708, mae: 15.271160, mean_q: 40.917294\n",
            " 25010/30000: episode: 4170, duration: 0.271s, episode steps:   6, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2605.333 [784.000, 3667.000],  loss: 51.999218, mae: 14.430972, mean_q: 39.144337\n",
            " 25016/30000: episode: 4171, duration: 0.239s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 4072.000 [2454.000, 5538.000],  loss: 32.301445, mae: 13.727310, mean_q: 36.742702\n",
            " 25022/30000: episode: 4172, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3809.333 [2831.000, 5127.000],  loss: 48.772236, mae: 13.013444, mean_q: 34.967560\n",
            " 25028/30000: episode: 4173, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4191.833 [2831.000, 4464.000],  loss: 46.519531, mae: 14.783112, mean_q: 38.966873\n",
            " 25034/30000: episode: 4174, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3389.333 [2149.000, 4464.000],  loss: 53.687088, mae: 14.590641, mean_q: 38.637211\n",
            " 25040/30000: episode: 4175, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward: 15.000, mean reward:  2.500 [-5.000, 10.000], mean action: 1987.000 [1530.000, 3960.000],  loss: 50.648411, mae: 14.477674, mean_q: 38.445572\n",
            " 25046/30000: episode: 4176, duration: 0.147s, episode steps:   6, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3179.667 [2245.000, 5089.000],  loss: 68.895630, mae: 15.015616, mean_q: 39.739349\n",
            " 25052/30000: episode: 4177, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward: 20.000, mean reward:  3.333 [ 0.000, 10.000], mean action: 2419.667 [138.000, 4550.000],  loss: 35.665787, mae: 13.714005, mean_q: 36.809444\n",
            " 25058/30000: episode: 4178, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3018.167 [717.000, 5146.000],  loss: 54.202499, mae: 13.704785, mean_q: 36.562122\n",
            " 25064/30000: episode: 4179, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 165.000, mean reward: 27.500 [10.000, 35.000], mean action: 3301.500 [238.000, 4185.000],  loss: 68.401878, mae: 13.675838, mean_q: 36.831318\n",
            " 25070/30000: episode: 4180, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3207.333 [2205.000, 3951.000],  loss: 49.878414, mae: 13.637576, mean_q: 36.229385\n",
            " 25076/30000: episode: 4181, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [-5.000, 15.000], mean action: 2152.833 [1530.000, 2555.000],  loss: 45.148975, mae: 14.081882, mean_q: 37.957668\n",
            " 25082/30000: episode: 4182, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3242.667 [356.000, 4913.000],  loss: 32.067657, mae: 13.578916, mean_q: 36.791111\n",
            " 25088/30000: episode: 4183, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2210.833 [149.000, 4893.000],  loss: 82.602966, mae: 14.427928, mean_q: 37.942394\n",
            " 25094/30000: episode: 4184, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 3954.833 [1530.000, 5747.000],  loss: 48.747467, mae: 14.684048, mean_q: 38.143040\n",
            " 25100/30000: episode: 4185, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2393.333 [1530.000, 4203.000],  loss: 57.337475, mae: 14.839756, mean_q: 39.234325\n",
            " 25106/30000: episode: 4186, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 35.000, mean reward:  5.833 [ 0.000, 35.000], mean action: 3485.333 [1530.000, 5219.000],  loss: 33.866116, mae: 13.838348, mean_q: 37.435520\n",
            " 25112/30000: episode: 4187, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 1948.167 [229.000, 3550.000],  loss: 55.306988, mae: 13.363289, mean_q: 36.390060\n",
            " 25118/30000: episode: 4188, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 1530.000 [1530.000, 1530.000],  loss: 57.157543, mae: 14.191554, mean_q: 37.874454\n",
            " 25124/30000: episode: 4189, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1712.333 [138.000, 4762.000],  loss: 47.637234, mae: 14.560922, mean_q: 39.008785\n",
            " 25130/30000: episode: 4190, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 1530.000 [1530.000, 1530.000],  loss: 43.536255, mae: 12.934682, mean_q: 35.367031\n",
            " 25136/30000: episode: 4191, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1889.667 [138.000, 5743.000],  loss: 43.755344, mae: 13.892306, mean_q: 37.074520\n",
            " 25142/30000: episode: 4192, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3102.833 [1530.000, 5014.000],  loss: 65.777199, mae: 12.462607, mean_q: 34.904396\n",
            " 25148/30000: episode: 4193, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 110.000, mean reward: 18.333 [ 0.000, 35.000], mean action: 3040.000 [1530.000, 5148.000],  loss: 38.293503, mae: 12.764258, mean_q: 34.764141\n",
            " 25154/30000: episode: 4194, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2257.000 [138.000, 4954.000],  loss: 54.021290, mae: 13.544051, mean_q: 36.324894\n",
            " 25160/30000: episode: 4195, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3387.667 [2132.000, 4176.000],  loss: 55.873241, mae: 13.743817, mean_q: 37.166889\n",
            " 25166/30000: episode: 4196, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2292.333 [138.000, 4860.000],  loss: 47.322857, mae: 13.744639, mean_q: 36.378391\n",
            " 25172/30000: episode: 4197, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3574.500 [2448.000, 4742.000],  loss: 56.601955, mae: 13.409369, mean_q: 36.255882\n",
            " 25178/30000: episode: 4198, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3023.167 [485.000, 5307.000],  loss: 51.606632, mae: 13.530427, mean_q: 36.334740\n",
            " 25184/30000: episode: 4199, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3321.833 [72.000, 5449.000],  loss: 61.771015, mae: 13.154480, mean_q: 35.243519\n",
            " 25190/30000: episode: 4200, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2735.833 [1274.000, 4642.000],  loss: 67.260445, mae: 13.872555, mean_q: 36.363396\n",
            " 25196/30000: episode: 4201, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 30.000, mean reward:  5.000 [ 0.000, 20.000], mean action: 3042.667 [1596.000, 4185.000],  loss: 50.595398, mae: 13.720969, mean_q: 36.576641\n",
            " 25202/30000: episode: 4202, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3722.333 [1556.000, 5700.000],  loss: 39.549034, mae: 13.185405, mean_q: 34.998466\n",
            " 25208/30000: episode: 4203, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3123.000 [1290.000, 5743.000],  loss: 61.246826, mae: 14.376204, mean_q: 37.311844\n",
            " 25214/30000: episode: 4204, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3296.167 [1256.000, 5424.000],  loss: 54.128620, mae: 13.625568, mean_q: 36.124496\n",
            " 25220/30000: episode: 4205, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4062.333 [3108.000, 5689.000],  loss: 77.506111, mae: 13.766828, mean_q: 35.672451\n",
            " 25226/30000: episode: 4206, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3091.000 [669.000, 5179.000],  loss: 53.844860, mae: 14.557937, mean_q: 38.418980\n",
            " 25232/30000: episode: 4207, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [-5.000, 10.000], mean action: 3595.667 [2147.000, 5253.000],  loss: 69.753151, mae: 13.986515, mean_q: 37.074123\n",
            " 25238/30000: episode: 4208, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 45.000, mean reward:  7.500 [ 0.000, 25.000], mean action: 2787.833 [1206.000, 5474.000],  loss: 69.736420, mae: 15.194400, mean_q: 38.478313\n",
            " 25244/30000: episode: 4209, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 130.000, mean reward: 21.667 [ 0.000, 35.000], mean action: 2778.333 [578.000, 4530.000],  loss: 51.664658, mae: 14.181119, mean_q: 37.702293\n",
            " 25250/30000: episode: 4210, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2159.833 [1573.000, 3249.000],  loss: 46.351788, mae: 13.339770, mean_q: 36.356190\n",
            " 25256/30000: episode: 4211, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2757.000 [1530.000, 3712.000],  loss: 48.082302, mae: 13.919518, mean_q: 36.220512\n",
            " 25262/30000: episode: 4212, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4135.833 [2444.000, 5361.000],  loss: 39.983925, mae: 13.523271, mean_q: 35.944832\n",
            " 25268/30000: episode: 4213, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1883.667 [608.000, 2800.000],  loss: 39.168819, mae: 12.425919, mean_q: 33.801979\n",
            " 25274/30000: episode: 4214, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3068.000 [1344.000, 4819.000],  loss: 58.420135, mae: 13.599735, mean_q: 36.538055\n",
            " 25280/30000: episode: 4215, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2745.833 [1082.000, 4530.000],  loss: 60.650867, mae: 13.154926, mean_q: 35.730255\n",
            " 25286/30000: episode: 4216, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2912.167 [245.000, 5047.000],  loss: 64.554115, mae: 13.679507, mean_q: 36.742580\n",
            " 25292/30000: episode: 4217, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2875.500 [1596.000, 4683.000],  loss: 77.478600, mae: 12.478625, mean_q: 35.240952\n",
            " 25298/30000: episode: 4218, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2985.833 [1988.000, 4337.000],  loss: 40.737698, mae: 13.584763, mean_q: 36.312901\n",
            " 25304/30000: episode: 4219, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2944.833 [1573.000, 4405.000],  loss: 36.303017, mae: 13.312489, mean_q: 35.955578\n",
            " 25310/30000: episode: 4220, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3871.000 [1988.000, 4806.000],  loss: 49.103947, mae: 14.162830, mean_q: 37.019474\n",
            " 25316/30000: episode: 4221, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3004.000 [1988.000, 5390.000],  loss: 48.805553, mae: 14.015121, mean_q: 36.434586\n",
            " 25322/30000: episode: 4222, duration: 0.148s, episode steps:   6, steps per second:  41, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 1673.167 [471.000, 3083.000],  loss: 54.413517, mae: 13.508610, mean_q: 35.483444\n",
            " 25328/30000: episode: 4223, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2275.333 [907.000, 3307.000],  loss: 37.876270, mae: 14.217973, mean_q: 37.385357\n",
            " 25334/30000: episode: 4224, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2753.167 [229.000, 5424.000],  loss: 45.056091, mae: 13.954013, mean_q: 36.746559\n",
            " 25340/30000: episode: 4225, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1496.500 [580.000, 3337.000],  loss: 62.532227, mae: 14.740372, mean_q: 38.988426\n",
            " 25346/30000: episode: 4226, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2633.333 [1042.000, 4298.000],  loss: 27.224627, mae: 12.964498, mean_q: 34.583012\n",
            " 25352/30000: episode: 4227, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2280.833 [158.000, 4185.000],  loss: 37.768887, mae: 14.217720, mean_q: 37.871002\n",
            " 25358/30000: episode: 4228, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3000.833 [907.000, 5466.000],  loss: 40.246086, mae: 12.530032, mean_q: 34.237194\n",
            " 25364/30000: episode: 4229, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2308.333 [542.000, 3273.000],  loss: 51.997570, mae: 13.091908, mean_q: 34.995918\n",
            " 25370/30000: episode: 4230, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1896.000 [992.000, 3925.000],  loss: 49.641109, mae: 13.673387, mean_q: 36.073345\n",
            " 25376/30000: episode: 4231, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3144.667 [1296.000, 5713.000],  loss: 55.912708, mae: 14.247321, mean_q: 36.578335\n",
            " 25382/30000: episode: 4232, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2956.167 [852.000, 4731.000],  loss: 51.508755, mae: 13.920672, mean_q: 37.619247\n",
            " 25388/30000: episode: 4233, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2973.000 [1042.000, 4530.000],  loss: 29.856499, mae: 13.648019, mean_q: 36.232471\n",
            " 25394/30000: episode: 4234, duration: 0.233s, episode steps:   6, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3289.667 [1180.000, 5213.000],  loss: 61.507797, mae: 14.509395, mean_q: 38.373100\n",
            " 25400/30000: episode: 4235, duration: 0.237s, episode steps:   6, steps per second:  25, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2563.167 [1573.000, 4464.000],  loss: 62.233585, mae: 14.304292, mean_q: 37.161694\n",
            " 25406/30000: episode: 4236, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 1428.833 [1094.000, 3103.000],  loss: 54.474365, mae: 13.637631, mean_q: 36.477524\n",
            " 25412/30000: episode: 4237, duration: 0.245s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3495.500 [805.000, 5538.000],  loss: 62.392345, mae: 13.962747, mean_q: 36.588718\n",
            " 25418/30000: episode: 4238, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward: -15.000, mean reward: -2.500 [-5.000,  5.000], mean action: 2949.333 [2818.000, 3606.000],  loss: 73.943169, mae: 13.331389, mean_q: 34.292561\n",
            " 25424/30000: episode: 4239, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2471.667 [1573.000, 4182.000],  loss: 63.623734, mae: 14.085197, mean_q: 36.742447\n",
            " 25430/30000: episode: 4240, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 4381.333 [992.000, 5736.000],  loss: 40.971752, mae: 13.582385, mean_q: 35.905472\n",
            " 25436/30000: episode: 4241, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward: 40.000, mean reward:  6.667 [ 0.000, 25.000], mean action: 1838.667 [425.000, 5369.000],  loss: 40.319996, mae: 12.665436, mean_q: 35.097977\n",
            " 25442/30000: episode: 4242, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4033.667 [2811.000, 5424.000],  loss: 43.997768, mae: 14.281657, mean_q: 37.849922\n",
            " 25448/30000: episode: 4243, duration: 0.264s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2749.500 [590.000, 4870.000],  loss: 50.458317, mae: 13.377912, mean_q: 35.489033\n",
            " 25454/30000: episode: 4244, duration: 0.261s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2286.667 [161.000, 4220.000],  loss: 42.784252, mae: 12.883720, mean_q: 34.987965\n",
            " 25460/30000: episode: 4245, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3040.667 [1094.000, 5424.000],  loss: 56.888844, mae: 15.229545, mean_q: 39.066196\n",
            " 25466/30000: episode: 4246, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3190.000 [375.000, 5703.000],  loss: 38.882557, mae: 13.118843, mean_q: 35.531895\n",
            " 25472/30000: episode: 4247, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2263.167 [547.000, 5424.000],  loss: 39.545460, mae: 13.436460, mean_q: 36.378834\n",
            " 25478/30000: episode: 4248, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000, 10.000], mean action: 3262.167 [1530.000, 3684.000],  loss: 68.280449, mae: 13.830254, mean_q: 36.408928\n",
            " 25484/30000: episode: 4249, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2540.333 [1268.000, 3826.000],  loss: 28.487360, mae: 13.395452, mean_q: 36.060398\n",
            " 25490/30000: episode: 4250, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2852.167 [951.000, 3606.000],  loss: 45.367840, mae: 13.383327, mean_q: 35.773289\n",
            " 25496/30000: episode: 4251, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3735.667 [2205.000, 5703.000],  loss: 31.997429, mae: 12.836907, mean_q: 34.166039\n",
            " 25502/30000: episode: 4252, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2647.500 [1003.000, 5011.000],  loss: 35.408634, mae: 13.076358, mean_q: 35.071774\n",
            " 25508/30000: episode: 4253, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2328.500 [375.000, 5093.000],  loss: 79.557587, mae: 13.552017, mean_q: 36.071419\n",
            " 25514/30000: episode: 4254, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3891.167 [561.000, 5623.000],  loss: 62.972431, mae: 13.875848, mean_q: 37.066151\n",
            " 25520/30000: episode: 4255, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2348.333 [163.000, 4808.000],  loss: 51.841583, mae: 14.534607, mean_q: 37.443310\n",
            " 25526/30000: episode: 4256, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3060.000 [587.000, 5018.000],  loss: 58.822010, mae: 14.118095, mean_q: 37.374538\n",
            " 25532/30000: episode: 4257, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3220.667 [1773.000, 4806.000],  loss: 42.610119, mae: 14.457359, mean_q: 37.946095\n",
            " 25538/30000: episode: 4258, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3384.500 [1246.000, 5482.000],  loss: 62.642139, mae: 13.114299, mean_q: 35.892963\n",
            " 25544/30000: episode: 4259, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2959.500 [309.000, 4842.000],  loss: 50.196133, mae: 13.746586, mean_q: 36.565956\n",
            " 25550/30000: episode: 4260, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3366.000 [1147.000, 5395.000],  loss: 58.358196, mae: 13.525462, mean_q: 35.375240\n",
            " 25556/30000: episode: 4261, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4743.000 [2448.000, 5466.000],  loss: 43.996418, mae: 13.394298, mean_q: 35.645966\n",
            " 25562/30000: episode: 4262, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2145.500 [79.000, 4616.000],  loss: 51.170002, mae: 14.313095, mean_q: 37.884598\n",
            " 25568/30000: episode: 4263, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3232.833 [1094.000, 5736.000],  loss: 87.921684, mae: 14.246101, mean_q: 37.654232\n",
            " 25574/30000: episode: 4264, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2504.500 [472.000, 3753.000],  loss: 76.277634, mae: 13.586051, mean_q: 35.884182\n",
            " 25580/30000: episode: 4265, duration: 0.148s, episode steps:   6, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2785.167 [462.000, 5561.000],  loss: 37.852097, mae: 14.380531, mean_q: 37.935253\n",
            " 25586/30000: episode: 4266, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3425.667 [267.000, 5362.000],  loss: 78.319008, mae: 13.687869, mean_q: 36.507130\n",
            " 25592/30000: episode: 4267, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2256.500 [68.000, 4552.000],  loss: 88.158577, mae: 13.374417, mean_q: 36.464844\n",
            " 25598/30000: episode: 4268, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [-5.000, 30.000], mean action: 2609.000 [375.000, 4573.000],  loss: 48.880859, mae: 13.401111, mean_q: 35.752689\n",
            " 25604/30000: episode: 4269, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 40.000, mean reward:  6.667 [ 0.000, 20.000], mean action: 2810.833 [1094.000, 4530.000],  loss: 58.135483, mae: 14.211583, mean_q: 37.811390\n",
            " 25610/30000: episode: 4270, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3967.333 [1910.000, 5456.000],  loss: 63.373409, mae: 13.944733, mean_q: 37.878834\n",
            " 25616/30000: episode: 4271, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3383.667 [717.000, 5723.000],  loss: 51.110912, mae: 14.865481, mean_q: 38.896683\n",
            " 25622/30000: episode: 4272, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [-5.000, 30.000], mean action: 2264.333 [238.000, 3236.000],  loss: 84.632637, mae: 15.538807, mean_q: 40.194336\n",
            " 25628/30000: episode: 4273, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3467.167 [282.000, 5538.000],  loss: 43.648853, mae: 12.603413, mean_q: 34.373394\n",
            " 25634/30000: episode: 4274, duration: 0.164s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3617.833 [2003.000, 5051.000],  loss: 52.547119, mae: 14.383621, mean_q: 38.427219\n",
            " 25640/30000: episode: 4275, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2453.667 [590.000, 5059.000],  loss: 41.266552, mae: 13.616832, mean_q: 36.676777\n",
            " 25646/30000: episode: 4276, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3187.500 [238.000, 4530.000],  loss: 39.839115, mae: 15.071475, mean_q: 39.937588\n",
            " 25652/30000: episode: 4277, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: 40.000, mean reward:  6.667 [ 0.000, 25.000], mean action: 2529.000 [213.000, 4185.000],  loss: 53.786926, mae: 14.294774, mean_q: 38.320187\n",
            " 25658/30000: episode: 4278, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2716.000 [1094.000, 4918.000],  loss: 55.998783, mae: 13.054977, mean_q: 36.408222\n",
            " 25664/30000: episode: 4279, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2328.500 [339.000, 4955.000],  loss: 39.125435, mae: 13.151141, mean_q: 35.940586\n",
            " 25670/30000: episode: 4280, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3548.000 [428.000, 5538.000],  loss: 54.132084, mae: 14.095326, mean_q: 37.172039\n",
            " 25676/30000: episode: 4281, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3857.167 [1202.000, 5675.000],  loss: 68.332497, mae: 14.235778, mean_q: 38.027161\n",
            " 25682/30000: episode: 4282, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  5.000], mean action: 4107.500 [3569.000, 5494.000],  loss: 66.031273, mae: 13.784617, mean_q: 36.718544\n",
            " 25688/30000: episode: 4283, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1864.667 [242.000, 4978.000],  loss: 53.292591, mae: 14.887581, mean_q: 38.799557\n",
            " 25694/30000: episode: 4284, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2833.500 [1530.000, 3684.000],  loss: 49.805981, mae: 13.893608, mean_q: 37.220608\n",
            " 25700/30000: episode: 4285, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 45.000, mean reward:  7.500 [ 0.000, 20.000], mean action: 3867.500 [2698.000, 5622.000],  loss: 61.466827, mae: 13.544418, mean_q: 36.196625\n",
            " 25706/30000: episode: 4286, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3874.000 [2448.000, 4992.000],  loss: 34.254925, mae: 12.789966, mean_q: 35.456417\n",
            " 25712/30000: episode: 4287, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward: 50.000, mean reward:  8.333 [ 0.000, 35.000], mean action: 3265.167 [2277.000, 4809.000],  loss: 36.490635, mae: 12.313832, mean_q: 33.762287\n",
            " 25718/30000: episode: 4288, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 1531.000 [587.000, 3052.000],  loss: 52.435795, mae: 13.483571, mean_q: 36.667492\n",
            " 25724/30000: episode: 4289, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3153.500 [587.000, 4992.000],  loss: 45.048080, mae: 13.432445, mean_q: 36.387131\n",
            " 25730/30000: episode: 4290, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2774.833 [587.000, 4816.000],  loss: 43.229248, mae: 13.671425, mean_q: 37.317516\n",
            " 25736/30000: episode: 4291, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1355.833 [587.000, 3510.000],  loss: 88.194794, mae: 13.846622, mean_q: 36.614628\n",
            " 25742/30000: episode: 4292, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2682.000 [587.000, 4616.000],  loss: 37.595078, mae: 13.815403, mean_q: 36.961376\n",
            " 25748/30000: episode: 4293, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2644.667 [119.000, 5283.000],  loss: 64.741737, mae: 13.711192, mean_q: 37.628315\n",
            " 25754/30000: episode: 4294, duration: 0.187s, episode steps:   6, steps per second:  32, episode reward: 40.000, mean reward:  6.667 [ 0.000, 20.000], mean action: 2170.333 [1202.000, 3661.000],  loss: 40.512604, mae: 13.034627, mean_q: 36.278698\n",
            " 25760/30000: episode: 4295, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3347.833 [587.000, 5129.000],  loss: 56.016830, mae: 14.332788, mean_q: 38.642300\n",
            " 25766/30000: episode: 4296, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2633.333 [1344.000, 3447.000],  loss: 53.563519, mae: 12.959039, mean_q: 34.864578\n",
            " 25772/30000: episode: 4297, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3149.333 [1094.000, 5334.000],  loss: 60.624390, mae: 13.299697, mean_q: 36.410976\n",
            " 25778/30000: episode: 4298, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1502.000 [238.000, 2390.000],  loss: 56.824932, mae: 13.060498, mean_q: 35.438267\n",
            " 25784/30000: episode: 4299, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4101.833 [1988.000, 5734.000],  loss: 48.438282, mae: 14.872707, mean_q: 38.975521\n",
            " 25790/30000: episode: 4300, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3812.667 [1596.000, 4573.000],  loss: 53.767548, mae: 14.280152, mean_q: 38.884716\n",
            " 25796/30000: episode: 4301, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 2326.333 [471.000, 4530.000],  loss: 56.021866, mae: 14.295320, mean_q: 37.933056\n",
            " 25802/30000: episode: 4302, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2595.667 [1082.000, 4440.000],  loss: 55.534222, mae: 13.401470, mean_q: 36.160862\n",
            " 25808/30000: episode: 4303, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2015.667 [709.000, 2277.000],  loss: 67.863304, mae: 14.471192, mean_q: 38.212097\n",
            " 25814/30000: episode: 4304, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2184.333 [363.000, 4742.000],  loss: 66.607178, mae: 13.652972, mean_q: 35.704739\n",
            " 25820/30000: episode: 4305, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3035.000 [1132.000, 5062.000],  loss: 25.210426, mae: 12.162796, mean_q: 32.869122\n",
            " 25826/30000: episode: 4306, duration: 0.237s, episode steps:   6, steps per second:  25, episode reward: 55.000, mean reward:  9.167 [ 0.000, 35.000], mean action: 2915.667 [1421.000, 4630.000],  loss: 55.040813, mae: 13.206406, mean_q: 35.462940\n",
            " 25832/30000: episode: 4307, duration: 0.263s, episode steps:   6, steps per second:  23, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3773.000 [3083.000, 4713.000],  loss: 41.912212, mae: 13.290436, mean_q: 35.714203\n",
            " 25838/30000: episode: 4308, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3154.167 [366.000, 4727.000],  loss: 82.862862, mae: 14.555271, mean_q: 38.312881\n",
            " 25844/30000: episode: 4309, duration: 0.240s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2777.333 [245.000, 4831.000],  loss: 60.437374, mae: 12.950582, mean_q: 34.983128\n",
            " 25850/30000: episode: 4310, duration: 0.258s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3467.667 [1076.000, 5449.000],  loss: 48.418575, mae: 14.533642, mean_q: 38.598595\n",
            " 25856/30000: episode: 4311, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3885.333 [245.000, 5737.000],  loss: 52.932892, mae: 14.154016, mean_q: 37.774166\n",
            " 25862/30000: episode: 4312, duration: 0.262s, episode steps:   6, steps per second:  23, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 2997.667 [1207.000, 5156.000],  loss: 30.781145, mae: 12.564742, mean_q: 34.800861\n",
            " 25868/30000: episode: 4313, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4298.500 [2988.000, 5734.000],  loss: 43.075001, mae: 13.711159, mean_q: 35.976208\n",
            " 25874/30000: episode: 4314, duration: 0.259s, episode steps:   6, steps per second:  23, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4565.833 [4530.000, 4573.000],  loss: 43.145264, mae: 13.594962, mean_q: 36.726589\n",
            " 25880/30000: episode: 4315, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3090.833 [1958.000, 4530.000],  loss: 39.190708, mae: 12.878572, mean_q: 34.971714\n",
            " 25886/30000: episode: 4316, duration: 0.260s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1399.333 [309.000, 4530.000],  loss: 67.341286, mae: 13.575722, mean_q: 36.482723\n",
            " 25892/30000: episode: 4317, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3881.500 [2003.000, 5736.000],  loss: 42.513805, mae: 13.496673, mean_q: 36.542728\n",
            " 25898/30000: episode: 4318, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 45.000, mean reward:  7.500 [ 0.000, 30.000], mean action: 3094.500 [126.000, 4683.000],  loss: 40.496002, mae: 14.250014, mean_q: 37.777576\n",
            " 25904/30000: episode: 4319, duration: 0.179s, episode steps:   6, steps per second:  33, episode reward: 105.000, mean reward: 17.500 [ 0.000, 35.000], mean action: 3053.000 [1596.000, 4185.000],  loss: 55.200562, mae: 12.918422, mean_q: 35.146343\n",
            " 25910/30000: episode: 4320, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2760.833 [1082.000, 4507.000],  loss: 53.481236, mae: 13.235803, mean_q: 35.777607\n",
            " 25916/30000: episode: 4321, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3027.833 [1132.000, 5178.000],  loss: 73.365974, mae: 13.553658, mean_q: 36.485996\n",
            " 25922/30000: episode: 4322, duration: 0.147s, episode steps:   6, steps per second:  41, episode reward: 105.000, mean reward: 17.500 [ 0.000, 40.000], mean action: 2169.833 [27.000, 4185.000],  loss: 67.881828, mae: 13.133824, mean_q: 35.355507\n",
            " 25928/30000: episode: 4323, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3031.167 [329.000, 5449.000],  loss: 54.937511, mae: 12.938904, mean_q: 34.533749\n",
            " 25934/30000: episode: 4324, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3153.333 [1136.000, 5466.000],  loss: 73.448151, mae: 12.978873, mean_q: 35.130791\n",
            " 25940/30000: episode: 4325, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2905.667 [1253.000, 5283.000],  loss: 50.346592, mae: 13.243916, mean_q: 35.466980\n",
            " 25946/30000: episode: 4326, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4201.667 [2277.000, 5148.000],  loss: 65.032555, mae: 13.683105, mean_q: 36.512589\n",
            " 25952/30000: episode: 4327, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 55.000, mean reward:  9.167 [ 0.000, 20.000], mean action: 2459.167 [1130.000, 5736.000],  loss: 49.900257, mae: 13.479077, mean_q: 35.560162\n",
            " 25958/30000: episode: 4328, duration: 0.147s, episode steps:   6, steps per second:  41, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2498.833 [571.000, 5747.000],  loss: 77.799202, mae: 12.614886, mean_q: 34.148281\n",
            " 25964/30000: episode: 4329, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2424.833 [504.000, 5243.000],  loss: 33.131466, mae: 13.174422, mean_q: 34.775852\n",
            " 25970/30000: episode: 4330, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2936.667 [1090.000, 5118.000],  loss: 69.325287, mae: 14.068669, mean_q: 37.526806\n",
            " 25976/30000: episode: 4331, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2989.333 [992.000, 4420.000],  loss: 45.436096, mae: 13.503507, mean_q: 36.176304\n",
            " 25982/30000: episode: 4332, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2920.000 [1031.000, 5129.000],  loss: 33.782299, mae: 12.522202, mean_q: 34.256134\n",
            " 25988/30000: episode: 4333, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 125.000, mean reward: 20.833 [ 0.000, 40.000], mean action: 2938.833 [238.000, 4185.000],  loss: 72.637161, mae: 14.091636, mean_q: 37.635448\n",
            " 25994/30000: episode: 4334, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3874.500 [2192.000, 5283.000],  loss: 63.317429, mae: 12.734044, mean_q: 34.719193\n",
            " 26000/30000: episode: 4335, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [-5.000, 30.000], mean action: 2110.333 [238.000, 2528.000],  loss: 37.432602, mae: 14.584922, mean_q: 39.131611\n",
            " 26006/30000: episode: 4336, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2836.000 [2107.000, 3992.000],  loss: 35.915546, mae: 13.841443, mean_q: 36.821049\n",
            " 26012/30000: episode: 4337, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2208.000 [375.000, 3712.000],  loss: 32.692917, mae: 13.030834, mean_q: 35.061718\n",
            " 26018/30000: episode: 4338, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3040.833 [2312.000, 4860.000],  loss: 58.310955, mae: 13.939903, mean_q: 36.718609\n",
            " 26024/30000: episode: 4339, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2506.667 [238.000, 4404.000],  loss: 44.965057, mae: 14.439584, mean_q: 37.877369\n",
            " 26030/30000: episode: 4340, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2759.333 [907.000, 3946.000],  loss: 57.250916, mae: 15.282337, mean_q: 39.429958\n",
            " 26036/30000: episode: 4341, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2497.833 [717.000, 5736.000],  loss: 40.480446, mae: 15.094525, mean_q: 39.780579\n",
            " 26042/30000: episode: 4342, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3648.000 [2672.000, 4746.000],  loss: 50.233410, mae: 14.086010, mean_q: 36.860458\n",
            " 26048/30000: episode: 4343, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3584.000 [2277.000, 5140.000],  loss: 39.921452, mae: 14.904494, mean_q: 39.596836\n",
            " 26054/30000: episode: 4344, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 70.000, mean reward: 11.667 [ 0.000, 35.000], mean action: 2260.500 [784.000, 3753.000],  loss: 37.916431, mae: 13.409345, mean_q: 36.010101\n",
            " 26060/30000: episode: 4345, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2649.500 [1202.000, 5736.000],  loss: 31.741125, mae: 13.976312, mean_q: 37.087704\n",
            " 26066/30000: episode: 4346, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3213.833 [159.000, 4412.000],  loss: 61.748737, mae: 14.131230, mean_q: 36.983624\n",
            " 26072/30000: episode: 4347, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2816.500 [990.000, 4826.000],  loss: 66.745361, mae: 13.918929, mean_q: 37.386700\n",
            " 26078/30000: episode: 4348, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2428.667 [1596.000, 3097.000],  loss: 57.965137, mae: 12.525943, mean_q: 34.301609\n",
            " 26084/30000: episode: 4349, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 30.000, mean reward:  5.000 [ 0.000, 20.000], mean action: 2439.333 [858.000, 3712.000],  loss: 39.616261, mae: 13.073714, mean_q: 34.953899\n",
            " 26090/30000: episode: 4350, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3663.667 [1703.000, 4921.000],  loss: 48.294010, mae: 14.044913, mean_q: 36.387154\n",
            " 26096/30000: episode: 4351, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3363.667 [992.000, 5156.000],  loss: 53.942028, mae: 13.984138, mean_q: 37.509174\n",
            " 26102/30000: episode: 4352, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3033.167 [886.000, 4407.000],  loss: 42.102829, mae: 13.087399, mean_q: 34.200172\n",
            " 26108/30000: episode: 4353, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3768.333 [238.000, 5471.000],  loss: 73.170837, mae: 13.994166, mean_q: 36.782532\n",
            " 26114/30000: episode: 4354, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 3031.000 [1344.000, 4616.000],  loss: 42.142769, mae: 13.292442, mean_q: 35.744785\n",
            " 26120/30000: episode: 4355, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward: 40.000, mean reward:  6.667 [ 0.000, 20.000], mean action: 2616.167 [1495.000, 5400.000],  loss: 67.984612, mae: 14.738635, mean_q: 38.099964\n",
            " 26126/30000: episode: 4356, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4189.667 [2277.000, 5430.000],  loss: 39.740295, mae: 12.267247, mean_q: 33.630802\n",
            " 26132/30000: episode: 4357, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2926.000 [1744.000, 5400.000],  loss: 58.886059, mae: 14.332280, mean_q: 37.941906\n",
            " 26138/30000: episode: 4358, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2470.000 [1163.000, 3834.000],  loss: 40.905285, mae: 13.773395, mean_q: 37.190903\n",
            " 26144/30000: episode: 4359, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward: 55.000, mean reward:  9.167 [ 0.000, 20.000], mean action: 2667.167 [1202.000, 4840.000],  loss: 38.977409, mae: 12.744156, mean_q: 34.238228\n",
            " 26150/30000: episode: 4360, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2917.167 [1421.000, 5257.000],  loss: 61.003208, mae: 13.220338, mean_q: 35.831181\n",
            " 26156/30000: episode: 4361, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2081.167 [872.000, 4782.000],  loss: 40.339703, mae: 13.224355, mean_q: 36.358829\n",
            " 26162/30000: episode: 4362, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2893.000 [820.000, 4493.000],  loss: 50.169270, mae: 14.047653, mean_q: 37.421444\n",
            " 26168/30000: episode: 4363, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4111.333 [1344.000, 5283.000],  loss: 66.653389, mae: 14.187977, mean_q: 38.032650\n",
            " 26174/30000: episode: 4364, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3251.167 [2312.000, 5475.000],  loss: 62.441914, mae: 13.354333, mean_q: 36.317257\n",
            " 26180/30000: episode: 4365, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 120.000, mean reward: 20.000 [ 0.000, 35.000], mean action: 3091.167 [1573.000, 4185.000],  loss: 76.047249, mae: 12.952164, mean_q: 35.128521\n",
            " 26186/30000: episode: 4366, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3694.000 [607.000, 5597.000],  loss: 49.718601, mae: 14.451758, mean_q: 37.907215\n",
            " 26192/30000: episode: 4367, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 45.000, mean reward:  7.500 [ 0.000, 20.000], mean action: 2185.000 [1348.000, 3843.000],  loss: 36.318592, mae: 14.175113, mean_q: 38.077930\n",
            " 26198/30000: episode: 4368, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3417.167 [1260.000, 4158.000],  loss: 45.380478, mae: 13.670545, mean_q: 36.769867\n",
            " 26204/30000: episode: 4369, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3312.833 [163.000, 5597.000],  loss: 70.743935, mae: 13.546060, mean_q: 36.736633\n",
            " 26210/30000: episode: 4370, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4553.500 [1421.000, 5180.000],  loss: 102.623352, mae: 13.750339, mean_q: 37.212730\n",
            " 26216/30000: episode: 4371, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2363.000 [1421.000, 3294.000],  loss: 27.947195, mae: 12.854614, mean_q: 35.725231\n",
            " 26222/30000: episode: 4372, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2557.667 [281.000, 4316.000],  loss: 55.432255, mae: 12.170567, mean_q: 32.878185\n",
            " 26228/30000: episode: 4373, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3236.500 [1113.000, 5218.000],  loss: 56.714783, mae: 13.064136, mean_q: 34.765667\n",
            " 26234/30000: episode: 4374, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 2088.500 [158.000, 4185.000],  loss: 86.918159, mae: 14.131729, mean_q: 37.381458\n",
            " 26240/30000: episode: 4375, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3948.500 [852.000, 5413.000],  loss: 37.156246, mae: 13.256858, mean_q: 35.245350\n",
            " 26246/30000: episode: 4376, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3657.333 [1620.000, 4727.000],  loss: 76.190041, mae: 14.443485, mean_q: 38.546391\n",
            " 26252/30000: episode: 4377, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2819.667 [238.000, 4788.000],  loss: 41.242123, mae: 12.958125, mean_q: 36.473766\n",
            " 26258/30000: episode: 4378, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2363.667 [541.000, 4806.000],  loss: 62.028687, mae: 14.711144, mean_q: 38.435055\n",
            " 26264/30000: episode: 4379, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward: 50.000, mean reward:  8.333 [ 0.000, 20.000], mean action: 3123.667 [1202.000, 4727.000],  loss: 40.561840, mae: 15.085544, mean_q: 39.239994\n",
            " 26270/30000: episode: 4380, duration: 0.272s, episode steps:   6, steps per second:  22, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2743.000 [2727.000, 2823.000],  loss: 44.700748, mae: 12.688842, mean_q: 34.329659\n",
            " 26276/30000: episode: 4381, duration: 0.236s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1771.333 [132.000, 4727.000],  loss: 35.625156, mae: 14.127585, mean_q: 36.943653\n",
            " 26282/30000: episode: 4382, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [-5.000, 10.000], mean action: 3622.000 [1247.000, 5708.000],  loss: 61.743038, mae: 14.665909, mean_q: 39.199833\n",
            " 26288/30000: episode: 4383, duration: 0.240s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1485.000 [10.000, 4158.000],  loss: 53.193577, mae: 14.010755, mean_q: 37.749615\n",
            " 26294/30000: episode: 4384, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1871.833 [608.000, 4332.000],  loss: 71.279190, mae: 14.028854, mean_q: 37.459911\n",
            " 26300/30000: episode: 4385, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3259.500 [238.000, 5180.000],  loss: 61.894547, mae: 14.028676, mean_q: 37.228828\n",
            " 26306/30000: episode: 4386, duration: 0.236s, episode steps:   6, steps per second:  25, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 5557.000 [4727.000, 5723.000],  loss: 77.519455, mae: 14.170642, mean_q: 37.854134\n",
            " 26312/30000: episode: 4387, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3212.333 [1132.000, 3951.000],  loss: 59.097088, mae: 14.254562, mean_q: 37.800655\n",
            " 26318/30000: episode: 4388, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1789.667 [10.000, 4407.000],  loss: 70.572609, mae: 14.071011, mean_q: 36.934772\n",
            " 26324/30000: episode: 4389, duration: 0.277s, episode steps:   6, steps per second:  22, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 2231.500 [197.000, 3892.000],  loss: 40.181728, mae: 13.338036, mean_q: 36.013485\n",
            " 26330/30000: episode: 4390, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1489.667 [238.000, 2580.000],  loss: 44.342251, mae: 14.058768, mean_q: 37.031116\n",
            " 26336/30000: episode: 4391, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3195.167 [835.000, 4410.000],  loss: 79.049919, mae: 14.619244, mean_q: 37.833752\n",
            " 26342/30000: episode: 4392, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1980.833 [607.000, 4130.000],  loss: 32.651653, mae: 13.445934, mean_q: 35.822983\n",
            " 26348/30000: episode: 4393, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 2553.500 [1596.000, 3748.000],  loss: 73.027817, mae: 14.279538, mean_q: 37.219234\n",
            " 26354/30000: episode: 4394, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3464.333 [1145.000, 5736.000],  loss: 41.504887, mae: 13.972470, mean_q: 37.436352\n",
            " 26360/30000: episode: 4395, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2699.167 [709.000, 4630.000],  loss: 55.600483, mae: 13.892769, mean_q: 37.616261\n",
            " 26366/30000: episode: 4396, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2521.000 [1160.000, 3748.000],  loss: 75.606812, mae: 14.056493, mean_q: 36.790554\n",
            " 26372/30000: episode: 4397, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3531.333 [1137.000, 5564.000],  loss: 74.450806, mae: 13.714080, mean_q: 36.466816\n",
            " 26378/30000: episode: 4398, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 30.000, mean reward:  5.000 [ 0.000, 20.000], mean action: 3068.833 [238.000, 5640.000],  loss: 37.056374, mae: 13.239347, mean_q: 35.127277\n",
            " 26384/30000: episode: 4399, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1237.500 [238.000, 2577.000],  loss: 33.763554, mae: 13.977055, mean_q: 37.587822\n",
            " 26390/30000: episode: 4400, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3223.667 [527.000, 5433.000],  loss: 64.792442, mae: 16.137720, mean_q: 40.565174\n",
            " 26396/30000: episode: 4401, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2245.333 [238.000, 5450.000],  loss: 39.255028, mae: 13.206661, mean_q: 34.453411\n",
            " 26402/30000: episode: 4402, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2676.833 [677.000, 4122.000],  loss: 38.825687, mae: 13.537530, mean_q: 35.949879\n",
            " 26408/30000: episode: 4403, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3223.500 [180.000, 4799.000],  loss: 46.029064, mae: 12.418286, mean_q: 33.776474\n",
            " 26414/30000: episode: 4404, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3456.667 [1082.000, 5597.000],  loss: 37.625927, mae: 12.450955, mean_q: 33.971817\n",
            " 26420/30000: episode: 4405, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4072.333 [3008.000, 4799.000],  loss: 72.798409, mae: 13.731938, mean_q: 35.986977\n",
            " 26426/30000: episode: 4406, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 95.000, mean reward: 15.833 [ 0.000, 35.000], mean action: 4314.167 [3685.000, 5140.000],  loss: 75.661469, mae: 13.409856, mean_q: 35.452190\n",
            " 26432/30000: episode: 4407, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2804.167 [590.000, 4505.000],  loss: 80.251289, mae: 14.169166, mean_q: 37.784245\n",
            " 26438/30000: episode: 4408, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1381.833 [420.000, 3206.000],  loss: 51.830685, mae: 14.134694, mean_q: 36.928951\n",
            " 26444/30000: episode: 4409, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2680.167 [1714.000, 4079.000],  loss: 70.370789, mae: 15.302361, mean_q: 38.946484\n",
            " 26450/30000: episode: 4410, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2601.833 [816.000, 5396.000],  loss: 23.414177, mae: 13.420148, mean_q: 35.508583\n",
            " 26456/30000: episode: 4411, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3158.167 [1215.000, 5597.000],  loss: 74.488609, mae: 14.675274, mean_q: 38.080338\n",
            " 26462/30000: episode: 4412, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 10.000], mean action: 2099.500 [238.000, 4360.000],  loss: 48.403370, mae: 13.753444, mean_q: 37.059311\n",
            " 26468/30000: episode: 4413, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3691.833 [872.000, 4860.000],  loss: 84.093315, mae: 14.066299, mean_q: 37.309742\n",
            " 26474/30000: episode: 4414, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2489.167 [571.000, 5218.000],  loss: 45.069225, mae: 13.270190, mean_q: 35.587654\n",
            " 26480/30000: episode: 4415, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2793.000 [429.000, 5652.000],  loss: 65.867805, mae: 12.993977, mean_q: 35.067230\n",
            " 26486/30000: episode: 4416, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2910.333 [1358.000, 5326.000],  loss: 58.959229, mae: 13.340198, mean_q: 35.526836\n",
            " 26492/30000: episode: 4417, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3317.167 [26.000, 5564.000],  loss: 58.731220, mae: 13.093292, mean_q: 35.056530\n",
            " 26498/30000: episode: 4418, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 1623.000 [502.000, 3497.000],  loss: 33.056786, mae: 12.946149, mean_q: 35.365387\n",
            " 26504/30000: episode: 4419, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3760.500 [2798.000, 4742.000],  loss: 54.554104, mae: 13.497209, mean_q: 35.320282\n",
            " 26510/30000: episode: 4420, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 2072.667 [138.000, 3854.000],  loss: 43.776119, mae: 13.146495, mean_q: 34.931347\n",
            " 26516/30000: episode: 4421, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward: 100.000, mean reward: 16.667 [ 0.000, 45.000], mean action: 3609.500 [238.000, 5680.000],  loss: 53.880203, mae: 12.854195, mean_q: 35.520679\n",
            " 26522/30000: episode: 4422, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3247.333 [2021.000, 5368.000],  loss: 77.376961, mae: 15.190735, mean_q: 38.988941\n",
            " 26528/30000: episode: 4423, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4144.500 [872.000, 4799.000],  loss: 40.999851, mae: 13.738754, mean_q: 36.468395\n",
            " 26534/30000: episode: 4424, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2739.833 [590.000, 4405.000],  loss: 51.263111, mae: 11.728942, mean_q: 32.156307\n",
            " 26540/30000: episode: 4425, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 1930.833 [158.000, 4185.000],  loss: 41.420227, mae: 12.498108, mean_q: 34.349289\n",
            " 26546/30000: episode: 4426, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2297.500 [1358.000, 5283.000],  loss: 42.102531, mae: 13.007298, mean_q: 34.933308\n",
            " 26552/30000: episode: 4427, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3511.333 [728.000, 5747.000],  loss: 53.638371, mae: 13.291790, mean_q: 36.197018\n",
            " 26558/30000: episode: 4428, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1173.833 [375.000, 2221.000],  loss: 77.626808, mae: 14.557385, mean_q: 38.273777\n",
            " 26564/30000: episode: 4429, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2887.667 [258.000, 5148.000],  loss: 48.060440, mae: 14.041637, mean_q: 37.908756\n",
            " 26570/30000: episode: 4430, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1975.333 [213.000, 5475.000],  loss: 47.015034, mae: 12.916710, mean_q: 35.315403\n",
            " 26576/30000: episode: 4431, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1131.667 [648.000, 1393.000],  loss: 53.359829, mae: 13.012256, mean_q: 34.835052\n",
            " 26582/30000: episode: 4432, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2836.167 [106.000, 4397.000],  loss: 55.017609, mae: 15.115182, mean_q: 39.326641\n",
            " 26588/30000: episode: 4433, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2061.333 [238.000, 4507.000],  loss: 44.587299, mae: 13.398288, mean_q: 35.414795\n",
            " 26594/30000: episode: 4434, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2010.500 [451.000, 4244.000],  loss: 46.679691, mae: 13.421239, mean_q: 35.690067\n",
            " 26600/30000: episode: 4435, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4219.500 [1485.000, 5564.000],  loss: 66.628899, mae: 13.621715, mean_q: 36.059212\n",
            " 26606/30000: episode: 4436, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4486.833 [3716.000, 5608.000],  loss: 44.387650, mae: 13.627416, mean_q: 36.053600\n",
            " 26612/30000: episode: 4437, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: 40.000, mean reward:  6.667 [ 0.000, 25.000], mean action: 1846.167 [138.000, 4397.000],  loss: 71.138344, mae: 13.706538, mean_q: 37.093410\n",
            " 26618/30000: episode: 4438, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3358.167 [1958.000, 5656.000],  loss: 44.233887, mae: 12.974370, mean_q: 34.573956\n",
            " 26624/30000: episode: 4439, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [-5.000, 20.000], mean action: 5091.667 [3705.000, 5369.000],  loss: 52.663395, mae: 13.283250, mean_q: 35.703117\n",
            " 26630/30000: episode: 4440, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4623.000 [1494.000, 5718.000],  loss: 88.156425, mae: 14.461385, mean_q: 38.047680\n",
            " 26636/30000: episode: 4441, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 3325.833 [238.000, 5140.000],  loss: 38.388458, mae: 13.887555, mean_q: 37.003849\n",
            " 26642/30000: episode: 4442, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2347.500 [501.000, 4573.000],  loss: 69.317162, mae: 13.514584, mean_q: 35.554424\n",
            " 26648/30000: episode: 4443, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2519.833 [316.000, 5656.000],  loss: 55.031513, mae: 14.624963, mean_q: 39.305431\n",
            " 26654/30000: episode: 4444, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  5.000], mean action: 2988.833 [2480.000, 4407.000],  loss: 47.725677, mae: 13.399323, mean_q: 35.805550\n",
            " 26660/30000: episode: 4445, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1076.833 [428.000, 4171.000],  loss: 43.576107, mae: 12.584239, mean_q: 34.106846\n",
            " 26666/30000: episode: 4446, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3710.500 [990.000, 5701.000],  loss: 52.859646, mae: 13.283856, mean_q: 35.940086\n",
            " 26672/30000: episode: 4447, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2602.167 [459.000, 3606.000],  loss: 65.762016, mae: 13.586357, mean_q: 36.696346\n",
            " 26678/30000: episode: 4448, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 100.000, mean reward: 16.667 [ 0.000, 35.000], mean action: 3277.833 [158.000, 4185.000],  loss: 45.224289, mae: 13.158641, mean_q: 35.377796\n",
            " 26684/30000: episode: 4449, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward: 30.000, mean reward:  5.000 [ 0.000, 25.000], mean action: 3020.167 [428.000, 5488.000],  loss: 48.095627, mae: 13.363210, mean_q: 36.800869\n",
            " 26690/30000: episode: 4450, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  5.000], mean action: 1970.500 [1613.000, 3705.000],  loss: 55.562454, mae: 13.591300, mean_q: 36.899746\n",
            " 26696/30000: episode: 4451, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4048.500 [2652.000, 5482.000],  loss: 56.463078, mae: 15.359558, mean_q: 40.449062\n",
            " 26702/30000: episode: 4452, duration: 0.213s, episode steps:   6, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2743.167 [238.000, 4185.000],  loss: 54.622772, mae: 13.382305, mean_q: 36.313225\n",
            " 26708/30000: episode: 4453, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2394.667 [238.000, 4391.000],  loss: 52.351254, mae: 14.039365, mean_q: 37.711021\n",
            " 26714/30000: episode: 4454, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3192.667 [1972.000, 5608.000],  loss: 63.765213, mae: 12.711068, mean_q: 35.421337\n",
            " 26720/30000: episode: 4455, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2756.167 [1461.000, 3705.000],  loss: 68.008347, mae: 14.373248, mean_q: 38.879192\n",
            " 26726/30000: episode: 4456, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3808.333 [2752.000, 4731.000],  loss: 35.811314, mae: 13.636712, mean_q: 36.509544\n",
            " 26732/30000: episode: 4457, duration: 0.266s, episode steps:   6, steps per second:  23, episode reward: 45.000, mean reward:  7.500 [ 0.000, 35.000], mean action: 4175.000 [2460.000, 5656.000],  loss: 55.554749, mae: 14.151505, mean_q: 37.954044\n",
            " 26738/30000: episode: 4458, duration: 0.239s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4121.000 [3294.000, 4683.000],  loss: 42.273434, mae: 13.596602, mean_q: 35.885998\n",
            " 26744/30000: episode: 4459, duration: 0.236s, episode steps:   6, steps per second:  25, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 3026.167 [69.000, 5410.000],  loss: 71.753601, mae: 13.351315, mean_q: 36.443447\n",
            " 26750/30000: episode: 4460, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3088.500 [534.000, 5339.000],  loss: 67.263512, mae: 13.875763, mean_q: 38.605793\n",
            " 26756/30000: episode: 4461, duration: 0.245s, episode steps:   6, steps per second:  24, episode reward: -10.000, mean reward: -1.667 [-5.000,  5.000], mean action: 3555.167 [1348.000, 4220.000],  loss: 32.249104, mae: 12.740466, mean_q: 35.252869\n",
            " 26762/30000: episode: 4462, duration: 0.262s, episode steps:   6, steps per second:  23, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2955.000 [1215.000, 5218.000],  loss: 60.668308, mae: 14.506621, mean_q: 39.967007\n",
            " 26768/30000: episode: 4463, duration: 0.238s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1930.333 [138.000, 4806.000],  loss: 52.689133, mae: 13.336514, mean_q: 36.542255\n",
            " 26774/30000: episode: 4464, duration: 0.208s, episode steps:   6, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2792.500 [2111.000, 3705.000],  loss: 44.164814, mae: 13.620613, mean_q: 37.958656\n",
            " 26780/30000: episode: 4465, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 150.000, mean reward: 25.000 [ 0.000, 35.000], mean action: 3919.833 [1596.000, 5656.000],  loss: 61.758255, mae: 13.385358, mean_q: 36.860653\n",
            " 26786/30000: episode: 4466, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 50.000, mean reward:  8.333 [ 0.000, 25.000], mean action: 2642.333 [1596.000, 3705.000],  loss: 57.303448, mae: 13.633517, mean_q: 37.913151\n",
            " 26792/30000: episode: 4467, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3095.333 [852.000, 5708.000],  loss: 76.548332, mae: 14.705951, mean_q: 40.279648\n",
            " 26798/30000: episode: 4468, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 5009.000 [3705.000, 5622.000],  loss: 36.874561, mae: 11.961964, mean_q: 34.780716\n",
            " 26804/30000: episode: 4469, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2771.167 [429.000, 5388.000],  loss: 29.153091, mae: 12.821258, mean_q: 36.784515\n",
            " 26810/30000: episode: 4470, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [-5.000, 10.000], mean action: 1909.333 [27.000, 5667.000],  loss: 75.193611, mae: 13.237915, mean_q: 37.442005\n",
            " 26816/30000: episode: 4471, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 2184.167 [162.000, 4893.000],  loss: 54.593670, mae: 12.775880, mean_q: 36.307690\n",
            " 26822/30000: episode: 4472, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 4982.333 [3705.000, 5597.000],  loss: 91.356117, mae: 14.616043, mean_q: 40.791004\n",
            " 26828/30000: episode: 4473, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3164.500 [1910.000, 3705.000],  loss: 74.581642, mae: 14.292984, mean_q: 40.274979\n",
            " 26834/30000: episode: 4474, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3260.500 [476.000, 5506.000],  loss: 43.555557, mae: 13.975376, mean_q: 38.738018\n",
            " 26840/30000: episode: 4475, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 3705.000 [3705.000, 3705.000],  loss: 76.014702, mae: 13.573410, mean_q: 38.816357\n",
            " 26846/30000: episode: 4476, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 40.000, mean reward:  6.667 [ 0.000, 15.000], mean action: 1888.833 [113.000, 3705.000],  loss: 62.679958, mae: 14.976746, mean_q: 40.822517\n",
            " 26852/30000: episode: 4477, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2882.333 [1596.000, 4941.000],  loss: 62.431805, mae: 14.294300, mean_q: 39.109142\n",
            " 26858/30000: episode: 4478, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3112.833 [30.000, 5585.000],  loss: 54.400253, mae: 13.244701, mean_q: 36.035130\n",
            " 26864/30000: episode: 4479, duration: 0.186s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2345.000 [709.000, 3712.000],  loss: 39.777512, mae: 13.194005, mean_q: 35.440269\n",
            " 26870/30000: episode: 4480, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2048.333 [253.000, 5340.000],  loss: 40.933102, mae: 11.493960, mean_q: 32.077690\n",
            " 26876/30000: episode: 4481, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3665.167 [1961.000, 5656.000],  loss: 32.648693, mae: 13.480714, mean_q: 36.118008\n",
            " 26882/30000: episode: 4482, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1974.500 [568.000, 5734.000],  loss: 52.760517, mae: 13.658588, mean_q: 36.422169\n",
            " 26888/30000: episode: 4483, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3334.667 [1530.000, 5722.000],  loss: 44.307541, mae: 14.182774, mean_q: 37.899315\n",
            " 26894/30000: episode: 4484, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [-5.000, 15.000], mean action: 1950.667 [1358.000, 4742.000],  loss: 35.559063, mae: 13.757510, mean_q: 36.995312\n",
            " 26900/30000: episode: 4485, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3570.833 [292.000, 5658.000],  loss: 59.355408, mae: 13.464221, mean_q: 37.266102\n",
            " 26906/30000: episode: 4486, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: 25.000, mean reward:  4.167 [-5.000, 30.000], mean action: 1944.833 [1141.000, 3079.000],  loss: 36.144241, mae: 11.836707, mean_q: 32.961567\n",
            " 26912/30000: episode: 4487, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2320.500 [990.000, 4158.000],  loss: 70.849388, mae: 13.373788, mean_q: 36.657986\n",
            " 26918/30000: episode: 4488, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 1530.000 [1530.000, 1530.000],  loss: 76.376381, mae: 13.705196, mean_q: 37.408215\n",
            " 26924/30000: episode: 4489, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3337.500 [238.000, 4415.000],  loss: 51.824383, mae: 13.422837, mean_q: 36.629734\n",
            " 26930/30000: episode: 4490, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1765.500 [144.000, 4002.000],  loss: 80.026909, mae: 13.718128, mean_q: 37.135067\n",
            " 26936/30000: episode: 4491, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1310.000 [709.000, 1530.000],  loss: 45.412449, mae: 14.124348, mean_q: 38.398891\n",
            " 26942/30000: episode: 4492, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 709.000 [709.000, 709.000],  loss: 55.471939, mae: 13.248734, mean_q: 36.487484\n",
            " 26948/30000: episode: 4493, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2932.500 [709.000, 4921.000],  loss: 64.308495, mae: 14.529868, mean_q: 39.311787\n",
            " 26954/30000: episode: 4494, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1759.500 [229.000, 4075.000],  loss: 57.794170, mae: 13.847992, mean_q: 37.240269\n",
            " 26960/30000: episode: 4495, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [-5.000, 10.000], mean action: 3298.667 [709.000, 5433.000],  loss: 53.828690, mae: 15.134524, mean_q: 40.336109\n",
            " 26966/30000: episode: 4496, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 35.000, mean reward:  5.833 [ 0.000, 10.000], mean action: 2021.333 [709.000, 5233.000],  loss: 56.771942, mae: 13.413142, mean_q: 37.054790\n",
            " 26972/30000: episode: 4497, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3871.167 [709.000, 5640.000],  loss: 37.524605, mae: 13.938031, mean_q: 37.621319\n",
            " 26978/30000: episode: 4498, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  5.000], mean action: 703.167 [149.000, 2818.000],  loss: 70.669106, mae: 12.982760, mean_q: 35.500069\n",
            " 26984/30000: episode: 4499, duration: 0.179s, episode steps:   6, steps per second:  33, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2259.500 [238.000, 3550.000],  loss: 69.377693, mae: 14.449346, mean_q: 39.016907\n",
            " 26990/30000: episode: 4500, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1546.667 [356.000, 4297.000],  loss: 47.470737, mae: 12.799080, mean_q: 35.422077\n",
            " 26996/30000: episode: 4501, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -15.000, mean reward: -2.500 [-5.000,  5.000], mean action: 2353.167 [709.000, 2682.000],  loss: 61.426777, mae: 14.779965, mean_q: 38.566444\n",
            " 27002/30000: episode: 4502, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 150.000, mean reward: 25.000 [ 0.000, 35.000], mean action: 3124.667 [1530.000, 5656.000],  loss: 45.230679, mae: 12.708099, mean_q: 34.944759\n",
            " 27008/30000: episode: 4503, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3649.333 [2909.000, 5339.000],  loss: 53.619900, mae: 14.056446, mean_q: 38.475109\n",
            " 27014/30000: episode: 4504, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 4104.333 [1530.000, 5656.000],  loss: 72.130241, mae: 12.937491, mean_q: 35.272335\n",
            " 27020/30000: episode: 4505, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 30.000, mean reward:  5.000 [ 0.000, 20.000], mean action: 2140.667 [872.000, 4806.000],  loss: 54.355885, mae: 12.846546, mean_q: 34.830654\n",
            " 27026/30000: episode: 4506, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1145.500 [38.000, 4542.000],  loss: 77.531197, mae: 14.279393, mean_q: 38.588440\n",
            " 27032/30000: episode: 4507, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1658.333 [1358.000, 2988.000],  loss: 34.640411, mae: 14.020131, mean_q: 37.712662\n",
            " 27038/30000: episode: 4508, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1647.333 [27.000, 4860.000],  loss: 58.577374, mae: 14.759692, mean_q: 39.774250\n",
            " 27044/30000: episode: 4509, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3036.500 [946.000, 4840.000],  loss: 52.926708, mae: 12.739429, mean_q: 34.930981\n",
            " 27050/30000: episode: 4510, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 15.000, mean reward:  2.500 [ 0.000, 10.000], mean action: 4147.667 [2985.000, 5656.000],  loss: 29.927933, mae: 14.541038, mean_q: 38.641800\n",
            " 27056/30000: episode: 4511, duration: 0.179s, episode steps:   6, steps per second:  33, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 1530.000 [1530.000, 1530.000],  loss: 54.761669, mae: 14.583524, mean_q: 39.017178\n",
            " 27062/30000: episode: 4512, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 1682.333 [1530.000, 2444.000],  loss: 38.800209, mae: 13.195476, mean_q: 35.421978\n",
            " 27068/30000: episode: 4513, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 1134.833 [149.000, 3221.000],  loss: 70.276062, mae: 14.139049, mean_q: 38.489697\n",
            " 27074/30000: episode: 4514, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2717.833 [309.000, 5233.000],  loss: 50.852570, mae: 13.274035, mean_q: 36.134644\n",
            " 27080/30000: episode: 4515, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3558.167 [1438.000, 5656.000],  loss: 41.170433, mae: 14.081181, mean_q: 36.812988\n",
            " 27086/30000: episode: 4516, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2884.000 [844.000, 3951.000],  loss: 61.933689, mae: 14.311630, mean_q: 37.353161\n",
            " 27092/30000: episode: 4517, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3310.667 [1530.000, 5338.000],  loss: 37.545200, mae: 12.854281, mean_q: 34.772408\n",
            " 27098/30000: episode: 4518, duration: 0.147s, episode steps:   6, steps per second:  41, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2522.833 [238.000, 4693.000],  loss: 37.062946, mae: 12.818756, mean_q: 35.607754\n",
            " 27104/30000: episode: 4519, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3679.167 [1554.000, 5722.000],  loss: 37.265400, mae: 14.314041, mean_q: 37.652378\n",
            " 27110/30000: episode: 4520, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2759.667 [1509.000, 4588.000],  loss: 58.527435, mae: 14.389073, mean_q: 37.836887\n",
            " 27116/30000: episode: 4521, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2813.667 [946.000, 5369.000],  loss: 40.673702, mae: 12.600562, mean_q: 34.249348\n",
            " 27122/30000: episode: 4522, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 30.000, mean reward:  5.000 [ 0.000, 30.000], mean action: 3087.000 [1530.000, 5000.000],  loss: 36.476246, mae: 12.693302, mean_q: 35.196865\n",
            " 27128/30000: episode: 4523, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2815.333 [1554.000, 4809.000],  loss: 66.664772, mae: 13.085959, mean_q: 35.777103\n",
            " 27134/30000: episode: 4524, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1392.167 [709.000, 3498.000],  loss: 58.536316, mae: 14.235390, mean_q: 38.231838\n",
            " 27140/30000: episode: 4525, duration: 0.261s, episode steps:   6, steps per second:  23, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 2189.000 [872.000, 4742.000],  loss: 50.292446, mae: 13.372162, mean_q: 35.852436\n",
            " 27146/30000: episode: 4526, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2622.500 [709.000, 4656.000],  loss: 45.999340, mae: 12.591846, mean_q: 33.847218\n",
            " 27152/30000: episode: 4527, duration: 0.240s, episode steps:   6, steps per second:  25, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2653.000 [872.000, 3922.000],  loss: 47.613194, mae: 14.111164, mean_q: 38.165306\n",
            " 27158/30000: episode: 4528, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2695.833 [150.000, 4687.000],  loss: 38.977840, mae: 12.389625, mean_q: 33.078457\n",
            " 27164/30000: episode: 4529, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3450.167 [2623.000, 5213.000],  loss: 40.824795, mae: 13.926712, mean_q: 37.268845\n",
            " 27170/30000: episode: 4530, duration: 0.259s, episode steps:   6, steps per second:  23, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2192.167 [371.000, 4826.000],  loss: 45.809368, mae: 14.416134, mean_q: 37.837025\n",
            " 27176/30000: episode: 4531, duration: 0.233s, episode steps:   6, steps per second:  26, episode reward: 15.000, mean reward:  2.500 [-5.000, 15.000], mean action: 1469.500 [1059.000, 2985.000],  loss: 65.512848, mae: 12.956161, mean_q: 35.259235\n",
            " 27182/30000: episode: 4532, duration: 0.245s, episode steps:   6, steps per second:  24, episode reward: 40.000, mean reward:  6.667 [ 0.000, 25.000], mean action: 1060.000 [183.000, 1988.000],  loss: 69.478561, mae: 13.971250, mean_q: 37.046169\n",
            " 27188/30000: episode: 4533, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3008.833 [1104.000, 5656.000],  loss: 38.473915, mae: 14.242236, mean_q: 37.954308\n",
            " 27194/30000: episode: 4534, duration: 0.266s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2808.833 [350.000, 5608.000],  loss: 78.448761, mae: 14.861316, mean_q: 39.424381\n",
            " 27200/30000: episode: 4535, duration: 0.245s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1905.333 [872.000, 2985.000],  loss: 56.245178, mae: 12.482446, mean_q: 35.003334\n",
            " 27206/30000: episode: 4536, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 1680.167 [872.000, 3098.000],  loss: 35.569530, mae: 12.987069, mean_q: 34.732567\n",
            " 27212/30000: episode: 4537, duration: 0.200s, episode steps:   6, steps per second:  30, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2734.000 [1596.000, 4203.000],  loss: 47.484806, mae: 13.327388, mean_q: 36.841736\n",
            " 27218/30000: episode: 4538, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward: 55.000, mean reward:  9.167 [-5.000, 30.000], mean action: 2010.833 [1059.000, 3770.000],  loss: 82.128624, mae: 13.760383, mean_q: 37.640110\n",
            " 27224/30000: episode: 4539, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2764.167 [1067.000, 5717.000],  loss: 40.212528, mae: 13.296589, mean_q: 36.374928\n",
            " 27230/30000: episode: 4540, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3228.167 [140.000, 5656.000],  loss: 50.819469, mae: 13.634437, mean_q: 37.754513\n",
            " 27236/30000: episode: 4541, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 4239.667 [2985.000, 4616.000],  loss: 53.684826, mae: 13.836871, mean_q: 37.524799\n",
            " 27242/30000: episode: 4542, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3545.500 [1596.000, 4852.000],  loss: 50.654205, mae: 13.241366, mean_q: 36.552639\n",
            " 27248/30000: episode: 4543, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2097.167 [580.000, 3817.000],  loss: 64.190773, mae: 12.521683, mean_q: 34.711384\n",
            " 27254/30000: episode: 4544, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 2985.000 [2985.000, 2985.000],  loss: 58.581127, mae: 13.306367, mean_q: 36.021271\n",
            " 27260/30000: episode: 4545, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2706.833 [132.000, 4683.000],  loss: 45.138515, mae: 13.121158, mean_q: 35.550495\n",
            " 27266/30000: episode: 4546, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3251.667 [2390.000, 4027.000],  loss: 64.132088, mae: 13.484048, mean_q: 36.435997\n",
            " 27272/30000: episode: 4547, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 2742.500 [1530.000, 2985.000],  loss: 60.172867, mae: 13.029438, mean_q: 36.023945\n",
            " 27278/30000: episode: 4548, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: 15.000, mean reward:  2.500 [-5.000, 15.000], mean action: 3676.667 [1596.000, 5129.000],  loss: 57.751423, mae: 13.654769, mean_q: 37.657215\n",
            " 27284/30000: episode: 4549, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2802.500 [1057.000, 4832.000],  loss: 49.174164, mae: 13.794541, mean_q: 36.567112\n",
            " 27290/30000: episode: 4550, duration: 0.189s, episode steps:   6, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3017.333 [1495.000, 4391.000],  loss: 42.512123, mae: 13.594673, mean_q: 37.259232\n",
            " 27296/30000: episode: 4551, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 3076.500 [1596.000, 4203.000],  loss: 50.059948, mae: 14.697192, mean_q: 38.599743\n",
            " 27302/30000: episode: 4552, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2686.000 [294.000, 5593.000],  loss: 64.658073, mae: 14.167248, mean_q: 37.677792\n",
            " 27308/30000: episode: 4553, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward: 20.000, mean reward:  3.333 [-5.000, 25.000], mean action: 3586.000 [1596.000, 5096.000],  loss: 68.986107, mae: 13.176145, mean_q: 34.936977\n",
            " 27314/30000: episode: 4554, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3207.333 [2112.000, 4841.000],  loss: 34.532772, mae: 14.027680, mean_q: 37.193718\n",
            " 27320/30000: episode: 4555, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 30.000, mean reward:  5.000 [ 0.000, 20.000], mean action: 3643.500 [1473.000, 5717.000],  loss: 50.620197, mae: 12.824340, mean_q: 34.321541\n",
            " 27326/30000: episode: 4556, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3522.833 [1282.000, 5418.000],  loss: 54.270351, mae: 14.306182, mean_q: 38.131008\n",
            " 27332/30000: episode: 4557, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2241.500 [437.000, 4353.000],  loss: 51.301941, mae: 14.503590, mean_q: 37.367371\n",
            " 27338/30000: episode: 4558, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2753.667 [581.000, 5233.000],  loss: 51.576267, mae: 14.160035, mean_q: 37.494614\n",
            " 27344/30000: episode: 4559, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 90.000, mean reward: 15.000 [ 0.000, 35.000], mean action: 2839.500 [581.000, 4379.000],  loss: 54.239971, mae: 12.654174, mean_q: 34.400944\n",
            " 27350/30000: episode: 4560, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1121.667 [138.000, 5597.000],  loss: 42.791912, mae: 13.096480, mean_q: 35.430153\n",
            " 27356/30000: episode: 4561, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1711.167 [581.000, 2480.000],  loss: 37.836113, mae: 13.299316, mean_q: 35.867352\n",
            " 27362/30000: episode: 4562, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2506.500 [238.000, 5296.000],  loss: 37.905972, mae: 12.712429, mean_q: 35.360523\n",
            " 27368/30000: episode: 4563, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1559.000 [1001.000, 2198.000],  loss: 70.260231, mae: 13.124146, mean_q: 35.814129\n",
            " 27374/30000: episode: 4564, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 145.000, mean reward: 24.167 [ 0.000, 35.000], mean action: 3315.000 [1554.000, 4185.000],  loss: 71.567879, mae: 13.852345, mean_q: 37.269878\n",
            " 27380/30000: episode: 4565, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3333.000 [1779.000, 4474.000],  loss: 34.989399, mae: 13.547011, mean_q: 37.269001\n",
            " 27386/30000: episode: 4566, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2832.833 [590.000, 5411.000],  loss: 48.535168, mae: 14.881336, mean_q: 38.954876\n",
            " 27392/30000: episode: 4567, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2734.667 [581.000, 5743.000],  loss: 28.696465, mae: 13.270162, mean_q: 36.187759\n",
            " 27398/30000: episode: 4568, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2451.167 [1438.000, 3731.000],  loss: 74.677574, mae: 14.565923, mean_q: 38.734528\n",
            " 27404/30000: episode: 4569, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 30.000, mean reward:  5.000 [-5.000, 20.000], mean action: 1152.167 [242.000, 1774.000],  loss: 48.785603, mae: 14.166875, mean_q: 37.244614\n",
            " 27410/30000: episode: 4570, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2011.667 [229.000, 4796.000],  loss: 23.776407, mae: 13.121090, mean_q: 35.387749\n",
            " 27416/30000: episode: 4571, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4007.333 [3103.000, 5629.000],  loss: 55.446945, mae: 13.987457, mean_q: 37.243305\n",
            " 27422/30000: episode: 4572, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [-5.000, 20.000], mean action: 2368.500 [1596.000, 2540.000],  loss: 62.657787, mae: 13.858105, mean_q: 36.985493\n",
            " 27428/30000: episode: 4573, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2577.167 [162.000, 4632.000],  loss: 54.816944, mae: 13.261379, mean_q: 35.657024\n",
            " 27434/30000: episode: 4574, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1886.333 [808.000, 4599.000],  loss: 63.017773, mae: 13.804329, mean_q: 36.859062\n",
            " 27440/30000: episode: 4575, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2812.000 [581.000, 4530.000],  loss: 24.212347, mae: 12.016938, mean_q: 33.345570\n",
            " 27446/30000: episode: 4576, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3094.333 [1202.000, 4826.000],  loss: 57.016800, mae: 13.755661, mean_q: 36.563385\n",
            " 27452/30000: episode: 4577, duration: 0.145s, episode steps:   6, steps per second:  41, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2380.833 [375.000, 4632.000],  loss: 59.379974, mae: 13.598174, mean_q: 36.643211\n",
            " 27458/30000: episode: 4578, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 2948.500 [385.000, 5062.000],  loss: 58.285007, mae: 13.480390, mean_q: 35.937698\n",
            " 27464/30000: episode: 4579, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2610.000 [872.000, 4767.000],  loss: 59.409134, mae: 13.224903, mean_q: 36.518360\n",
            " 27470/30000: episode: 4580, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3223.500 [156.000, 4632.000],  loss: 77.849319, mae: 13.265786, mean_q: 36.362988\n",
            " 27476/30000: episode: 4581, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 150.000, mean reward: 25.000 [ 0.000, 35.000], mean action: 2752.667 [1242.000, 4185.000],  loss: 50.626217, mae: 13.187278, mean_q: 35.638260\n",
            " 27482/30000: episode: 4582, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3431.167 [1554.000, 5689.000],  loss: 44.880112, mae: 12.929769, mean_q: 36.196285\n",
            " 27488/30000: episode: 4583, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2166.167 [1141.000, 3020.000],  loss: 43.727383, mae: 13.602367, mean_q: 37.027401\n",
            " 27494/30000: episode: 4584, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 3050.167 [477.000, 4397.000],  loss: 36.818165, mae: 13.791869, mean_q: 37.429306\n",
            " 27500/30000: episode: 4585, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3561.000 [560.000, 5716.000],  loss: 60.724178, mae: 13.680092, mean_q: 37.216850\n",
            " 27506/30000: episode: 4586, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3930.000 [3192.000, 4701.000],  loss: 39.146526, mae: 13.851818, mean_q: 37.200703\n",
            " 27512/30000: episode: 4587, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2357.667 [581.000, 4259.000],  loss: 59.592453, mae: 14.679038, mean_q: 38.998966\n",
            " 27518/30000: episode: 4588, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: 20.000, mean reward:  3.333 [-5.000, 15.000], mean action: 3646.667 [1596.000, 4826.000],  loss: 43.208553, mae: 12.771081, mean_q: 34.619678\n",
            " 27524/30000: episode: 4589, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: 20.000, mean reward:  3.333 [-5.000, 25.000], mean action: 4470.000 [1606.000, 5656.000],  loss: 70.245247, mae: 13.692287, mean_q: 37.239651\n",
            " 27530/30000: episode: 4590, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3892.500 [584.000, 5703.000],  loss: 52.246052, mae: 12.828122, mean_q: 35.804333\n",
            " 27536/30000: episode: 4591, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2384.667 [238.000, 4284.000],  loss: 41.671268, mae: 12.488208, mean_q: 34.539051\n",
            " 27542/30000: episode: 4592, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2672.333 [1305.000, 5017.000],  loss: 98.511162, mae: 14.739368, mean_q: 39.354572\n",
            " 27548/30000: episode: 4593, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 50.000, mean reward:  8.333 [ 0.000, 30.000], mean action: 3216.333 [803.000, 4632.000],  loss: 36.718307, mae: 13.778618, mean_q: 37.826691\n",
            " 27554/30000: episode: 4594, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2218.500 [27.000, 4632.000],  loss: 44.412277, mae: 12.187312, mean_q: 33.626350\n",
            " 27560/30000: episode: 4595, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3895.000 [86.000, 5668.000],  loss: 54.844067, mae: 13.993491, mean_q: 38.647327\n",
            " 27566/30000: episode: 4596, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2476.500 [875.000, 5716.000],  loss: 74.966377, mae: 14.451005, mean_q: 39.461235\n",
            " 27572/30000: episode: 4597, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4432.333 [3494.000, 5717.000],  loss: 43.276905, mae: 13.643532, mean_q: 37.202999\n",
            " 27578/30000: episode: 4598, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward: 30.000, mean reward:  5.000 [ 0.000, 25.000], mean action: 3247.333 [2448.000, 3797.000],  loss: 50.703384, mae: 13.722881, mean_q: 36.672752\n",
            " 27584/30000: episode: 4599, duration: 0.264s, episode steps:   6, steps per second:  23, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3269.000 [2221.000, 4632.000],  loss: 49.166737, mae: 13.618324, mean_q: 36.959656\n",
            " 27590/30000: episode: 4600, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward: -5.000, mean reward: -0.833 [-5.000, 15.000], mean action: 2809.000 [2448.000, 4614.000],  loss: 52.909794, mae: 12.644096, mean_q: 34.338909\n",
            " 27596/30000: episode: 4601, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2563.833 [648.000, 5743.000],  loss: 52.352314, mae: 13.240621, mean_q: 35.761547\n",
            " 27602/30000: episode: 4602, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2890.833 [608.000, 4632.000],  loss: 37.968090, mae: 13.848630, mean_q: 37.440220\n",
            " 27608/30000: episode: 4603, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5124.667 [2812.000, 5700.000],  loss: 74.163780, mae: 14.340287, mean_q: 37.924107\n",
            " 27614/30000: episode: 4604, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward: 45.000, mean reward:  7.500 [ 0.000, 15.000], mean action: 3600.167 [1596.000, 4742.000],  loss: 104.774445, mae: 14.133952, mean_q: 38.769505\n",
            " 27620/30000: episode: 4605, duration: 0.241s, episode steps:   6, steps per second:  25, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 1593.000 [269.000, 5270.000],  loss: 58.757938, mae: 14.193756, mean_q: 37.505955\n",
            " 27626/30000: episode: 4606, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4045.833 [575.000, 5629.000],  loss: 61.269169, mae: 12.043995, mean_q: 32.470661\n",
            " 27632/30000: episode: 4607, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 1893.500 [430.000, 3709.000],  loss: 49.893887, mae: 13.905003, mean_q: 36.887619\n",
            " 27638/30000: episode: 4608, duration: 0.259s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3087.333 [575.000, 5447.000],  loss: 49.374924, mae: 13.862434, mean_q: 37.680828\n",
            " 27644/30000: episode: 4609, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3129.333 [575.000, 5717.000],  loss: 69.846550, mae: 13.207025, mean_q: 35.330273\n",
            " 27650/30000: episode: 4610, duration: 0.192s, episode steps:   6, steps per second:  31, episode reward: 20.000, mean reward:  3.333 [-5.000, 25.000], mean action: 4670.833 [2985.000, 5616.000],  loss: 58.363712, mae: 13.003503, mean_q: 35.142235\n",
            " 27656/30000: episode: 4611, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 145.000, mean reward: 24.167 [ 0.000, 35.000], mean action: 2888.000 [1596.000, 4185.000],  loss: 32.860806, mae: 12.997536, mean_q: 35.789219\n",
            " 27662/30000: episode: 4612, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3528.167 [1003.000, 5629.000],  loss: 43.272739, mae: 12.683951, mean_q: 33.888493\n",
            " 27668/30000: episode: 4613, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2427.667 [138.000, 4789.000],  loss: 37.734547, mae: 12.824387, mean_q: 35.431637\n",
            " 27674/30000: episode: 4614, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4042.000 [2740.000, 4918.000],  loss: 47.159809, mae: 14.258597, mean_q: 37.987377\n",
            " 27680/30000: episode: 4615, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3290.833 [575.000, 5717.000],  loss: 63.036266, mae: 13.659783, mean_q: 37.326218\n",
            " 27686/30000: episode: 4616, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1689.667 [163.000, 2985.000],  loss: 49.424713, mae: 13.711692, mean_q: 36.957531\n",
            " 27692/30000: episode: 4617, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3963.833 [2795.000, 4632.000],  loss: 39.665012, mae: 13.295681, mean_q: 36.358532\n",
            " 27698/30000: episode: 4618, duration: 0.148s, episode steps:   6, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3321.167 [1654.000, 5671.000],  loss: 46.735226, mae: 12.996636, mean_q: 35.934822\n",
            " 27704/30000: episode: 4619, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3103.167 [462.000, 5456.000],  loss: 56.498325, mae: 13.453365, mean_q: 37.124390\n",
            " 27710/30000: episode: 4620, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3575.000 [1596.000, 5351.000],  loss: 59.448833, mae: 14.434957, mean_q: 38.833736\n",
            " 27716/30000: episode: 4621, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 40.000, mean reward:  6.667 [ 0.000, 20.000], mean action: 2720.500 [238.000, 5062.000],  loss: 39.554653, mae: 12.928424, mean_q: 36.382324\n",
            " 27722/30000: episode: 4622, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2960.500 [575.000, 5447.000],  loss: 49.541199, mae: 14.417023, mean_q: 39.281906\n",
            " 27728/30000: episode: 4623, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3012.333 [504.000, 5387.000],  loss: 49.915661, mae: 13.431293, mean_q: 36.529057\n",
            " 27734/30000: episode: 4624, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2660.667 [1573.000, 4185.000],  loss: 52.455307, mae: 13.114777, mean_q: 36.002644\n",
            " 27740/30000: episode: 4625, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3023.000 [1199.000, 4742.000],  loss: 46.179920, mae: 11.994077, mean_q: 33.168564\n",
            " 27746/30000: episode: 4626, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1799.833 [188.000, 3985.000],  loss: 35.481998, mae: 12.089260, mean_q: 34.215599\n",
            " 27752/30000: episode: 4627, duration: 0.179s, episode steps:   6, steps per second:  33, episode reward:  5.000, mean reward:  0.833 [-5.000, 15.000], mean action: 4315.500 [581.000, 5369.000],  loss: 36.310467, mae: 13.665996, mean_q: 37.751698\n",
            " 27758/30000: episode: 4628, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2047.000 [1530.000, 4632.000],  loss: 54.085194, mae: 14.023964, mean_q: 39.113384\n",
            " 27764/30000: episode: 4629, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3045.833 [331.000, 5204.000],  loss: 74.603401, mae: 13.012954, mean_q: 36.238590\n",
            " 27770/30000: episode: 4630, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4449.167 [2985.000, 4742.000],  loss: 51.930950, mae: 13.181259, mean_q: 35.761250\n",
            " 27776/30000: episode: 4631, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 1850.167 [158.000, 4247.000],  loss: 69.161179, mae: 14.271252, mean_q: 38.149448\n",
            " 27782/30000: episode: 4632, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1244.500 [183.000, 3311.000],  loss: 48.204620, mae: 12.841987, mean_q: 36.137573\n",
            " 27788/30000: episode: 4633, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2997.500 [903.000, 4826.000],  loss: 60.784672, mae: 12.333901, mean_q: 34.525593\n",
            " 27794/30000: episode: 4634, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3218.000 [580.000, 4742.000],  loss: 58.269821, mae: 12.629826, mean_q: 34.709469\n",
            " 27800/30000: episode: 4635, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [-5.000, 20.000], mean action: 2701.333 [1262.000, 3367.000],  loss: 56.635090, mae: 13.533993, mean_q: 36.819569\n",
            " 27806/30000: episode: 4636, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4079.833 [769.000, 4742.000],  loss: 38.172840, mae: 13.340947, mean_q: 36.360287\n",
            " 27812/30000: episode: 4637, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2602.500 [101.000, 4530.000],  loss: 66.929070, mae: 13.988224, mean_q: 38.356655\n",
            " 27818/30000: episode: 4638, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2255.500 [547.000, 4220.000],  loss: 49.133015, mae: 14.471166, mean_q: 38.516541\n",
            " 27824/30000: episode: 4639, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3267.167 [992.000, 5656.000],  loss: 44.643940, mae: 13.080986, mean_q: 35.165417\n",
            " 27830/30000: episode: 4640, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2329.500 [1530.000, 4163.000],  loss: 44.334122, mae: 13.516612, mean_q: 36.694363\n",
            " 27836/30000: episode: 4641, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [-5.000, 20.000], mean action: 3479.000 [1202.000, 4742.000],  loss: 53.331593, mae: 14.173383, mean_q: 38.134037\n",
            " 27842/30000: episode: 4642, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2363.167 [329.000, 5180.000],  loss: 72.840736, mae: 12.878593, mean_q: 36.526649\n",
            " 27848/30000: episode: 4643, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1713.167 [191.000, 3712.000],  loss: 75.267700, mae: 13.953359, mean_q: 37.868725\n",
            " 27854/30000: episode: 4644, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3237.333 [581.000, 5608.000],  loss: 39.739639, mae: 13.692142, mean_q: 37.670059\n",
            " 27860/30000: episode: 4645, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3000.500 [459.000, 5715.000],  loss: 35.336517, mae: 13.491307, mean_q: 36.607151\n",
            " 27866/30000: episode: 4646, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2380.500 [581.000, 4157.000],  loss: 61.670521, mae: 14.688850, mean_q: 38.953140\n",
            " 27872/30000: episode: 4647, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2570.833 [1573.000, 4185.000],  loss: 63.583134, mae: 13.643200, mean_q: 36.862106\n",
            " 27878/30000: episode: 4648, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2656.000 [273.000, 5057.000],  loss: 53.062122, mae: 13.104644, mean_q: 35.994164\n",
            " 27884/30000: episode: 4649, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  5.000], mean action: 2276.667 [575.000, 5447.000],  loss: 84.191292, mae: 14.032969, mean_q: 37.822098\n",
            " 27890/30000: episode: 4650, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3742.833 [2879.000, 4742.000],  loss: 32.885643, mae: 13.069297, mean_q: 35.428406\n",
            " 27896/30000: episode: 4651, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1338.000 [581.000, 3249.000],  loss: 61.197208, mae: 12.779073, mean_q: 35.721371\n",
            " 27902/30000: episode: 4652, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2243.833 [581.000, 4140.000],  loss: 44.298306, mae: 13.127210, mean_q: 36.094101\n",
            " 27908/30000: episode: 4653, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 35.000, mean reward:  5.833 [ 0.000, 15.000], mean action: 2908.000 [1202.000, 4826.000],  loss: 34.605694, mae: 12.157616, mean_q: 33.993595\n",
            " 27914/30000: episode: 4654, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4183.667 [2357.000, 5722.000],  loss: 56.933247, mae: 12.639863, mean_q: 34.731365\n",
            " 27920/30000: episode: 4655, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2810.833 [2555.000, 4090.000],  loss: 63.049938, mae: 14.219650, mean_q: 38.898239\n",
            " 27926/30000: episode: 4656, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1429.667 [106.000, 3101.000],  loss: 53.700546, mae: 13.428543, mean_q: 36.807556\n",
            " 27932/30000: episode: 4657, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3808.000 [1596.000, 4992.000],  loss: 54.327930, mae: 12.261704, mean_q: 34.295021\n",
            " 27938/30000: episode: 4658, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3015.333 [581.000, 4407.000],  loss: 79.586754, mae: 13.531574, mean_q: 36.872673\n",
            " 27944/30000: episode: 4659, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3163.000 [1937.000, 4681.000],  loss: 49.015491, mae: 14.881053, mean_q: 40.180584\n",
            " 27950/30000: episode: 4660, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4096.333 [1160.000, 5719.000],  loss: 43.884693, mae: 13.250710, mean_q: 37.481159\n",
            " 27956/30000: episode: 4661, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 2787.667 [271.000, 4090.000],  loss: 62.929760, mae: 15.053180, mean_q: 41.098511\n",
            " 27962/30000: episode: 4662, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2521.333 [57.000, 4090.000],  loss: 69.492393, mae: 13.805076, mean_q: 37.959980\n",
            " 27968/30000: episode: 4663, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4416.000 [2540.000, 5433.000],  loss: 53.001968, mae: 14.354716, mean_q: 39.253727\n",
            " 27974/30000: episode: 4664, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3577.000 [238.000, 4567.000],  loss: 57.313946, mae: 14.423560, mean_q: 39.676556\n",
            " 27980/30000: episode: 4665, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2412.833 [820.000, 4090.000],  loss: 58.349384, mae: 14.846378, mean_q: 40.191631\n",
            " 27986/30000: episode: 4666, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3035.500 [62.000, 4826.000],  loss: 42.037281, mae: 14.321789, mean_q: 39.798801\n",
            " 27992/30000: episode: 4667, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3036.167 [542.000, 4229.000],  loss: 33.104263, mae: 12.704528, mean_q: 35.787678\n",
            " 27998/30000: episode: 4668, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3571.833 [1094.000, 5701.000],  loss: 48.001194, mae: 14.026042, mean_q: 38.649292\n",
            " 28004/30000: episode: 4669, duration: 0.183s, episode steps:   6, steps per second:  33, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2398.500 [363.000, 4909.000],  loss: 63.225712, mae: 12.396594, mean_q: 36.013988\n",
            " 28010/30000: episode: 4670, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 70.000, mean reward: 11.667 [-5.000, 30.000], mean action: 3823.667 [1596.000, 5608.000],  loss: 65.994781, mae: 13.106490, mean_q: 36.644661\n",
            " 28016/30000: episode: 4671, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2095.167 [375.000, 3838.000],  loss: 38.961784, mae: 11.757722, mean_q: 33.612888\n",
            " 28022/30000: episode: 4672, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3682.833 [927.000, 5579.000],  loss: 53.667469, mae: 11.956622, mean_q: 34.907093\n",
            " 28028/30000: episode: 4673, duration: 0.241s, episode steps:   6, steps per second:  25, episode reward: 40.000, mean reward:  6.667 [-5.000, 45.000], mean action: 3019.833 [238.000, 4090.000],  loss: 62.943539, mae: 13.438172, mean_q: 37.716286\n",
            " 28034/30000: episode: 4674, duration: 0.249s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2462.000 [1076.000, 4140.000],  loss: 51.766834, mae: 12.981595, mean_q: 37.209011\n",
            " 28040/30000: episode: 4675, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2891.833 [467.000, 4630.000],  loss: 39.973816, mae: 13.630141, mean_q: 38.565018\n",
            " 28046/30000: episode: 4676, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 1593.333 [1094.000, 4090.000],  loss: 39.904202, mae: 13.162563, mean_q: 38.133610\n",
            " 28052/30000: episode: 4677, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 4090.000 [4090.000, 4090.000],  loss: 54.538242, mae: 13.224235, mean_q: 37.727631\n",
            " 28058/30000: episode: 4678, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2641.667 [1202.000, 4806.000],  loss: 47.074612, mae: 12.537376, mean_q: 35.906563\n",
            " 28064/30000: episode: 4679, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1747.000 [309.000, 5156.000],  loss: 65.036034, mae: 12.827661, mean_q: 36.727528\n",
            " 28070/30000: episode: 4680, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3099.833 [1596.000, 4090.000],  loss: 55.208385, mae: 13.482894, mean_q: 38.327606\n",
            " 28076/30000: episode: 4681, duration: 0.246s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2685.833 [2021.000, 4090.000],  loss: 52.584385, mae: 12.596249, mean_q: 35.374077\n",
            " 28082/30000: episode: 4682, duration: 0.255s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2904.000 [1254.000, 4090.000],  loss: 61.375946, mae: 13.888499, mean_q: 39.080723\n",
            " 28088/30000: episode: 4683, duration: 0.209s, episode steps:   6, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2859.333 [1710.000, 4090.000],  loss: 90.515022, mae: 13.518074, mean_q: 38.537395\n",
            " 28094/30000: episode: 4684, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2517.667 [245.000, 4203.000],  loss: 62.904369, mae: 13.628544, mean_q: 38.225960\n",
            " 28100/30000: episode: 4685, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2458.000 [542.000, 4247.000],  loss: 38.774357, mae: 13.142350, mean_q: 37.154308\n",
            " 28106/30000: episode: 4686, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3495.000 [132.000, 5408.000],  loss: 51.170971, mae: 13.816249, mean_q: 38.550369\n",
            " 28112/30000: episode: 4687, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1707.000 [245.000, 4090.000],  loss: 67.876221, mae: 12.617050, mean_q: 36.121449\n",
            " 28118/30000: episode: 4688, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3982.167 [2446.000, 5598.000],  loss: 54.182018, mae: 12.693398, mean_q: 36.427383\n",
            " 28124/30000: episode: 4689, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3308.333 [915.000, 4652.000],  loss: 54.411617, mae: 13.846335, mean_q: 38.845768\n",
            " 28130/30000: episode: 4690, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3272.167 [1059.000, 5156.000],  loss: 61.690228, mae: 13.074536, mean_q: 36.379166\n",
            " 28136/30000: episode: 4691, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2553.833 [309.000, 5156.000],  loss: 81.485451, mae: 13.665391, mean_q: 38.103886\n",
            " 28142/30000: episode: 4692, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2541.667 [820.000, 4239.000],  loss: 53.889984, mae: 13.053769, mean_q: 37.261150\n",
            " 28148/30000: episode: 4693, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3863.667 [1596.000, 5722.000],  loss: 51.130871, mae: 14.687099, mean_q: 39.800030\n",
            " 28154/30000: episode: 4694, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3253.167 [459.000, 4614.000],  loss: 39.764858, mae: 13.709664, mean_q: 39.442760\n",
            " 28160/30000: episode: 4695, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2207.667 [237.000, 4090.000],  loss: 46.007092, mae: 12.919621, mean_q: 37.042801\n",
            " 28166/30000: episode: 4696, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 4264.667 [4090.000, 4614.000],  loss: 56.771351, mae: 14.517789, mean_q: 39.874012\n",
            " 28172/30000: episode: 4697, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 155.000, mean reward: 25.833 [10.000, 35.000], mean action: 3302.333 [1573.000, 4185.000],  loss: 42.027546, mae: 13.369985, mean_q: 37.082226\n",
            " 28178/30000: episode: 4698, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3191.833 [1596.000, 5156.000],  loss: 76.399895, mae: 13.720140, mean_q: 37.585323\n",
            " 28184/30000: episode: 4699, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2809.167 [974.000, 4614.000],  loss: 38.672817, mae: 13.166924, mean_q: 36.781334\n",
            " 28190/30000: episode: 4700, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4119.667 [1530.000, 4918.000],  loss: 27.979872, mae: 12.967971, mean_q: 35.980278\n",
            " 28196/30000: episode: 4701, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2868.333 [273.000, 5722.000],  loss: 45.171997, mae: 13.597821, mean_q: 37.453236\n",
            " 28202/30000: episode: 4702, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4415.500 [1887.000, 5473.000],  loss: 55.651867, mae: 13.360124, mean_q: 37.518570\n",
            " 28208/30000: episode: 4703, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 4614.000 [4614.000, 4614.000],  loss: 52.528301, mae: 13.332225, mean_q: 37.324642\n",
            " 28214/30000: episode: 4704, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3746.500 [1057.000, 5703.000],  loss: 82.527550, mae: 13.517282, mean_q: 36.928204\n",
            " 28220/30000: episode: 4705, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 4614.000 [4614.000, 4614.000],  loss: 58.627453, mae: 13.483472, mean_q: 37.096424\n",
            " 28226/30000: episode: 4706, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1531.333 [163.000, 4614.000],  loss: 30.700426, mae: 12.601649, mean_q: 34.920620\n",
            " 28232/30000: episode: 4707, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3924.667 [1530.000, 5742.000],  loss: 48.524036, mae: 13.249713, mean_q: 35.790985\n",
            " 28238/30000: episode: 4708, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3318.833 [1661.000, 5715.000],  loss: 59.998547, mae: 13.252767, mean_q: 36.554001\n",
            " 28244/30000: episode: 4709, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3390.333 [1599.000, 5656.000],  loss: 49.309555, mae: 14.009193, mean_q: 37.920559\n",
            " 28250/30000: episode: 4710, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 2438.167 [2003.000, 4614.000],  loss: 27.336159, mae: 12.751082, mean_q: 36.724651\n",
            " 28256/30000: episode: 4711, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4585.667 [4444.000, 4614.000],  loss: 58.962879, mae: 14.236004, mean_q: 38.533230\n",
            " 28262/30000: episode: 4712, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3060.000 [238.000, 5014.000],  loss: 69.307373, mae: 14.567113, mean_q: 39.395607\n",
            " 28268/30000: episode: 4713, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2349.667 [959.000, 4628.000],  loss: 53.647934, mae: 14.238208, mean_q: 38.296658\n",
            " 28274/30000: episode: 4714, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 4271.500 [4203.000, 4614.000],  loss: 52.176010, mae: 13.386645, mean_q: 36.967983\n",
            " 28280/30000: episode: 4715, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3313.667 [1253.000, 4819.000],  loss: 51.881119, mae: 12.426358, mean_q: 33.886517\n",
            " 28286/30000: episode: 4716, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3537.167 [1147.000, 5715.000],  loss: 42.787811, mae: 12.767170, mean_q: 35.117966\n",
            " 28292/30000: episode: 4717, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2961.333 [238.000, 5400.000],  loss: 46.481243, mae: 12.891942, mean_q: 35.094570\n",
            " 28298/30000: episode: 4718, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3388.667 [1596.000, 5117.000],  loss: 63.834366, mae: 13.115055, mean_q: 35.229359\n",
            " 28304/30000: episode: 4719, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3000.667 [808.000, 5283.000],  loss: 66.680901, mae: 13.796786, mean_q: 37.120266\n",
            " 28310/30000: episode: 4720, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4063.167 [145.000, 5722.000],  loss: 60.902695, mae: 13.801019, mean_q: 37.394352\n",
            " 28316/30000: episode: 4721, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2707.000 [291.000, 5703.000],  loss: 66.164177, mae: 14.085934, mean_q: 37.361240\n",
            " 28322/30000: episode: 4722, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 3298.333 [1596.000, 4444.000],  loss: 48.344711, mae: 13.948077, mean_q: 38.504200\n",
            " 28328/30000: episode: 4723, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2077.500 [1094.000, 4444.000],  loss: 48.557285, mae: 13.739868, mean_q: 37.454056\n",
            " 28334/30000: episode: 4724, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2267.333 [375.000, 4826.000],  loss: 83.439888, mae: 13.197268, mean_q: 36.163544\n",
            " 28340/30000: episode: 4725, duration: 0.185s, episode steps:   6, steps per second:  32, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2832.667 [1393.000, 5001.000],  loss: 44.906330, mae: 14.022426, mean_q: 37.794125\n",
            " 28346/30000: episode: 4726, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3436.333 [2237.000, 4444.000],  loss: 39.543594, mae: 13.395801, mean_q: 36.520535\n",
            " 28352/30000: episode: 4727, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4057.667 [229.000, 5742.000],  loss: 36.452332, mae: 11.967221, mean_q: 34.219109\n",
            " 28358/30000: episode: 4728, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2204.500 [145.000, 4582.000],  loss: 54.086639, mae: 14.029682, mean_q: 38.390736\n",
            " 28364/30000: episode: 4729, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2350.333 [224.000, 4742.000],  loss: 48.617504, mae: 13.597366, mean_q: 37.195683\n",
            " 28370/30000: episode: 4730, duration: 0.146s, episode steps:   6, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3501.000 [1530.000, 4614.000],  loss: 45.938351, mae: 14.688752, mean_q: 39.534252\n",
            " 28376/30000: episode: 4731, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2207.167 [238.000, 4444.000],  loss: 64.022499, mae: 13.778234, mean_q: 37.590366\n",
            " 28382/30000: episode: 4732, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2861.667 [238.000, 5373.000],  loss: 44.771790, mae: 12.161315, mean_q: 34.533314\n",
            " 28388/30000: episode: 4733, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 10.000], mean action: 3032.333 [1530.000, 5703.000],  loss: 41.052116, mae: 12.752309, mean_q: 34.840672\n",
            " 28394/30000: episode: 4734, duration: 0.147s, episode steps:   6, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2351.333 [163.000, 4159.000],  loss: 41.943096, mae: 14.206647, mean_q: 37.454128\n",
            " 28400/30000: episode: 4735, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 50.000, mean reward:  8.333 [ 0.000, 40.000], mean action: 4117.833 [2518.000, 5474.000],  loss: 60.772594, mae: 13.656068, mean_q: 36.244717\n",
            " 28406/30000: episode: 4736, duration: 0.156s, episode steps:   6, steps per second:  39, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 4001.833 [1567.000, 4683.000],  loss: 54.582584, mae: 13.445564, mean_q: 35.741409\n",
            " 28412/30000: episode: 4737, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1931.167 [72.000, 5348.000],  loss: 68.888618, mae: 13.722350, mean_q: 37.259045\n",
            " 28418/30000: episode: 4738, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  5.000], mean action: 4180.000 [1495.000, 5373.000],  loss: 67.980713, mae: 13.496078, mean_q: 35.903805\n",
            " 28424/30000: episode: 4739, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2378.167 [156.000, 4614.000],  loss: 62.778667, mae: 12.900757, mean_q: 36.063663\n",
            " 28430/30000: episode: 4740, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3595.667 [186.000, 5715.000],  loss: 38.349575, mae: 12.139949, mean_q: 33.856739\n",
            " 28436/30000: episode: 4741, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2456.833 [238.000, 5090.000],  loss: 32.356663, mae: 12.709916, mean_q: 35.268848\n",
            " 28442/30000: episode: 4742, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3271.333 [1532.000, 5666.000],  loss: 50.417496, mae: 13.137456, mean_q: 36.727215\n",
            " 28448/30000: episode: 4743, duration: 0.201s, episode steps:   6, steps per second:  30, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1864.500 [229.000, 2357.000],  loss: 48.633121, mae: 13.811447, mean_q: 37.775974\n",
            " 28454/30000: episode: 4744, duration: 0.259s, episode steps:   6, steps per second:  23, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3437.667 [1074.000, 5283.000],  loss: 56.705738, mae: 11.856548, mean_q: 33.352890\n",
            " 28460/30000: episode: 4745, duration: 0.239s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3326.333 [238.000, 5456.000],  loss: 56.238129, mae: 13.157073, mean_q: 35.256516\n",
            " 28466/30000: episode: 4746, duration: 0.235s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3419.333 [1774.000, 5283.000],  loss: 72.050842, mae: 13.282691, mean_q: 35.729000\n",
            " 28472/30000: episode: 4747, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 1641.000 [158.000, 4444.000],  loss: 41.141159, mae: 13.385386, mean_q: 35.891605\n",
            " 28478/30000: episode: 4748, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 4305.167 [3235.000, 4918.000],  loss: 60.639866, mae: 13.530598, mean_q: 35.988747\n",
            " 28484/30000: episode: 4749, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 4313.500 [3921.000, 4444.000],  loss: 43.701626, mae: 13.189624, mean_q: 35.918850\n",
            " 28490/30000: episode: 4750, duration: 0.238s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2487.333 [429.000, 5303.000],  loss: 47.463959, mae: 14.414031, mean_q: 38.234314\n",
            " 28496/30000: episode: 4751, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2608.167 [1513.000, 3957.000],  loss: 52.887043, mae: 14.066255, mean_q: 38.153362\n",
            " 28502/30000: episode: 4752, duration: 0.251s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3582.000 [2192.000, 5211.000],  loss: 40.662258, mae: 12.741585, mean_q: 34.864170\n",
            " 28508/30000: episode: 4753, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [-5.000, 15.000], mean action: 4744.667 [3921.000, 5579.000],  loss: 110.121162, mae: 14.637943, mean_q: 38.335876\n",
            " 28514/30000: episode: 4754, duration: 0.266s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2540.333 [1530.000, 3529.000],  loss: 68.112091, mae: 14.421890, mean_q: 38.682262\n",
            " 28520/30000: episode: 4755, duration: 0.250s, episode steps:   6, steps per second:  24, episode reward: 55.000, mean reward:  9.167 [ 0.000, 25.000], mean action: 2686.333 [1202.000, 4789.000],  loss: 49.698757, mae: 13.618198, mean_q: 37.512531\n",
            " 28526/30000: episode: 4756, duration: 0.206s, episode steps:   6, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3115.333 [590.000, 4828.000],  loss: 63.064056, mae: 13.661195, mean_q: 37.812225\n",
            " 28532/30000: episode: 4757, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3286.500 [442.000, 5656.000],  loss: 41.508652, mae: 12.816762, mean_q: 34.973492\n",
            " 28538/30000: episode: 4758, duration: 0.174s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2501.333 [1141.000, 3712.000],  loss: 55.433548, mae: 14.089706, mean_q: 38.116066\n",
            " 28544/30000: episode: 4759, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3038.000 [1499.000, 4701.000],  loss: 71.818581, mae: 13.620376, mean_q: 36.649197\n",
            " 28550/30000: episode: 4760, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3280.000 [1573.000, 4444.000],  loss: 39.807358, mae: 13.825994, mean_q: 36.608410\n",
            " 28556/30000: episode: 4761, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2632.167 [273.000, 5283.000],  loss: 78.575829, mae: 13.936432, mean_q: 37.435581\n",
            " 28562/30000: episode: 4762, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2976.667 [1530.000, 5473.000],  loss: 50.592587, mae: 13.152856, mean_q: 35.474880\n",
            " 28568/30000: episode: 4763, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3871.667 [1622.000, 4474.000],  loss: 64.231659, mae: 14.021228, mean_q: 38.149181\n",
            " 28574/30000: episode: 4764, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2863.500 [1180.000, 5607.000],  loss: 60.770878, mae: 13.264447, mean_q: 36.071156\n",
            " 28580/30000: episode: 4765, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2161.000 [605.000, 4871.000],  loss: 41.766479, mae: 14.024224, mean_q: 37.089832\n",
            " 28586/30000: episode: 4766, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2345.667 [30.000, 5233.000],  loss: 65.514076, mae: 13.939801, mean_q: 37.352230\n",
            " 28592/30000: episode: 4767, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward: 135.000, mean reward: 22.500 [ 0.000, 40.000], mean action: 4527.667 [3712.000, 5616.000],  loss: 67.671692, mae: 13.402812, mean_q: 37.005684\n",
            " 28598/30000: episode: 4768, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2470.500 [138.000, 5218.000],  loss: 53.566433, mae: 13.761111, mean_q: 36.872768\n",
            " 28604/30000: episode: 4769, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2023.167 [1243.000, 3712.000],  loss: 53.545959, mae: 15.005040, mean_q: 39.647789\n",
            " 28610/30000: episode: 4770, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1750.500 [961.000, 1958.000],  loss: 49.221378, mae: 13.999387, mean_q: 37.195847\n",
            " 28616/30000: episode: 4771, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1761.667 [575.000, 4444.000],  loss: 33.530022, mae: 13.985951, mean_q: 36.773418\n",
            " 28622/30000: episode: 4772, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1236.000 [961.000, 1348.000],  loss: 50.757572, mae: 14.013225, mean_q: 37.498974\n",
            " 28628/30000: episode: 4773, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1206.500 [575.000, 1530.000],  loss: 52.351715, mae: 13.519816, mean_q: 37.355850\n",
            " 28634/30000: episode: 4774, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3538.333 [1160.000, 5129.000],  loss: 50.454807, mae: 12.169534, mean_q: 33.786762\n",
            " 28640/30000: episode: 4775, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2955.167 [1023.000, 5443.000],  loss: 38.872742, mae: 13.377632, mean_q: 36.662281\n",
            " 28646/30000: episode: 4776, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward: 40.000, mean reward:  6.667 [ 0.000, 40.000], mean action: 2356.667 [1133.000, 3542.000],  loss: 46.818691, mae: 14.000282, mean_q: 37.463821\n",
            " 28652/30000: episode: 4777, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3096.000 [513.000, 5658.000],  loss: 36.208241, mae: 13.368127, mean_q: 36.589478\n",
            " 28658/30000: episode: 4778, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3453.500 [1988.000, 5107.000],  loss: 48.492550, mae: 14.566986, mean_q: 38.828903\n",
            " 28664/30000: episode: 4779, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3411.333 [1253.000, 5433.000],  loss: 64.466873, mae: 13.148351, mean_q: 36.844189\n",
            " 28670/30000: episode: 4780, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 1678.167 [156.000, 3299.000],  loss: 57.458790, mae: 13.072845, mean_q: 36.322102\n",
            " 28676/30000: episode: 4781, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward: 35.000, mean reward:  5.833 [ 0.000, 15.000], mean action: 3184.667 [1190.000, 5233.000],  loss: 33.384541, mae: 12.927588, mean_q: 34.793404\n",
            " 28682/30000: episode: 4782, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3786.833 [2125.000, 5494.000],  loss: 63.647274, mae: 13.814372, mean_q: 36.811676\n",
            " 28688/30000: episode: 4783, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3621.500 [1348.000, 5629.000],  loss: 59.280064, mae: 14.104442, mean_q: 36.849510\n",
            " 28694/30000: episode: 4784, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3346.333 [375.000, 5626.000],  loss: 41.133938, mae: 12.945183, mean_q: 35.953785\n",
            " 28700/30000: episode: 4785, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2398.667 [1172.000, 5010.000],  loss: 51.956226, mae: 14.648280, mean_q: 38.320843\n",
            " 28706/30000: episode: 4786, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 45.000, mean reward:  7.500 [ 0.000, 45.000], mean action: 2625.833 [1253.000, 5433.000],  loss: 80.273376, mae: 14.227863, mean_q: 39.335102\n",
            " 28712/30000: episode: 4787, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2741.333 [1421.000, 3685.000],  loss: 90.501564, mae: 14.163246, mean_q: 37.892303\n",
            " 28718/30000: episode: 4788, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 3781.333 [542.000, 5703.000],  loss: 42.835331, mae: 12.508430, mean_q: 34.295933\n",
            " 28724/30000: episode: 4789, duration: 0.148s, episode steps:   6, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2347.000 [1133.000, 5129.000],  loss: 51.444538, mae: 13.130916, mean_q: 36.386753\n",
            " 28730/30000: episode: 4790, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 25.000, mean reward:  4.167 [ 0.000, 10.000], mean action: 3170.167 [1530.000, 5364.000],  loss: 101.046059, mae: 14.256424, mean_q: 38.966209\n",
            " 28736/30000: episode: 4791, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 30.000, mean reward:  5.000 [ 0.000, 30.000], mean action: 2419.167 [1133.000, 4693.000],  loss: 45.052212, mae: 14.293942, mean_q: 38.484913\n",
            " 28742/30000: episode: 4792, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2679.167 [551.000, 4677.000],  loss: 58.780949, mae: 13.932252, mean_q: 37.611053\n",
            " 28748/30000: episode: 4793, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3063.500 [742.000, 4500.000],  loss: 71.592033, mae: 13.466586, mean_q: 35.967712\n",
            " 28754/30000: episode: 4794, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3890.833 [2323.000, 5466.000],  loss: 60.032532, mae: 13.371722, mean_q: 36.435394\n",
            " 28760/30000: episode: 4795, duration: 0.147s, episode steps:   6, steps per second:  41, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2756.333 [1421.000, 4616.000],  loss: 58.986279, mae: 14.362903, mean_q: 38.411625\n",
            " 28766/30000: episode: 4796, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3620.833 [1421.000, 5476.000],  loss: 47.492268, mae: 14.561584, mean_q: 38.664333\n",
            " 28772/30000: episode: 4797, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2193.833 [150.000, 4683.000],  loss: 48.310516, mae: 13.498164, mean_q: 36.755005\n",
            " 28778/30000: episode: 4798, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1210.167 [238.000, 2861.000],  loss: 71.986420, mae: 14.545991, mean_q: 39.538662\n",
            " 28784/30000: episode: 4799, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2433.000 [238.000, 3935.000],  loss: 42.790066, mae: 12.074756, mean_q: 33.751453\n",
            " 28790/30000: episode: 4800, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: 50.000, mean reward:  8.333 [ 0.000, 25.000], mean action: 2601.500 [183.000, 5608.000],  loss: 42.707718, mae: 12.575553, mean_q: 34.941521\n",
            " 28796/30000: episode: 4801, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3794.500 [2076.000, 4918.000],  loss: 56.269665, mae: 14.508293, mean_q: 39.551449\n",
            " 28802/30000: episode: 4802, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3779.167 [34.000, 5626.000],  loss: 66.554649, mae: 14.019221, mean_q: 37.854355\n",
            " 28808/30000: episode: 4803, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2700.500 [339.000, 5579.000],  loss: 51.396240, mae: 14.050460, mean_q: 38.427544\n",
            " 28814/30000: episode: 4804, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2631.000 [145.000, 4925.000],  loss: 74.448898, mae: 13.976932, mean_q: 37.971584\n",
            " 28820/30000: episode: 4805, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2822.500 [1127.000, 5476.000],  loss: 69.905663, mae: 13.855053, mean_q: 37.210934\n",
            " 28826/30000: episode: 4806, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 4487.500 [3214.000, 5651.000],  loss: 75.198021, mae: 14.162410, mean_q: 38.176586\n",
            " 28832/30000: episode: 4807, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2884.000 [363.000, 4344.000],  loss: 70.449890, mae: 12.305856, mean_q: 34.021709\n",
            " 28838/30000: episode: 4808, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2310.667 [1242.000, 4344.000],  loss: 60.106903, mae: 11.916183, mean_q: 33.091068\n",
            " 28844/30000: episode: 4809, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3166.500 [1596.000, 4945.000],  loss: 63.463207, mae: 12.809120, mean_q: 35.482906\n",
            " 28850/30000: episode: 4810, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3553.667 [1596.000, 4444.000],  loss: 78.249657, mae: 13.370763, mean_q: 36.392124\n",
            " 28856/30000: episode: 4811, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3645.333 [1141.000, 4945.000],  loss: 49.912151, mae: 12.916752, mean_q: 35.254162\n",
            " 28862/30000: episode: 4812, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward: 35.000, mean reward:  5.833 [ 0.000, 35.000], mean action: 3084.833 [1421.000, 4806.000],  loss: 59.031799, mae: 12.988467, mean_q: 36.104099\n",
            " 28868/30000: episode: 4813, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2030.167 [135.000, 4820.000],  loss: 62.786251, mae: 13.418489, mean_q: 37.590549\n",
            " 28874/30000: episode: 4814, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 3156.000 [1421.000, 5564.000],  loss: 53.655151, mae: 12.245618, mean_q: 34.020374\n",
            " 28880/30000: episode: 4815, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3448.167 [542.000, 5652.000],  loss: 55.033634, mae: 14.923442, mean_q: 40.665379\n",
            " 28886/30000: episode: 4816, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3412.000 [1596.000, 4871.000],  loss: 80.529381, mae: 14.123417, mean_q: 38.039295\n",
            " 28892/30000: episode: 4817, duration: 0.230s, episode steps:   6, steps per second:  26, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 2447.167 [590.000, 5447.000],  loss: 59.364304, mae: 14.267083, mean_q: 39.070866\n",
            " 28898/30000: episode: 4818, duration: 0.268s, episode steps:   6, steps per second:  22, episode reward: 105.000, mean reward: 17.500 [ 0.000, 40.000], mean action: 2441.333 [86.000, 4652.000],  loss: 64.132080, mae: 14.288532, mean_q: 39.057465\n",
            " 28904/30000: episode: 4819, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 3377.833 [1596.000, 4871.000],  loss: 36.996204, mae: 12.325787, mean_q: 34.667854\n",
            " 28910/30000: episode: 4820, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2123.000 [1253.000, 4871.000],  loss: 60.438992, mae: 12.864777, mean_q: 35.097691\n",
            " 28916/30000: episode: 4821, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2593.167 [198.000, 5212.000],  loss: 41.386414, mae: 13.250237, mean_q: 35.978092\n",
            " 28922/30000: episode: 4822, duration: 0.265s, episode steps:   6, steps per second:  23, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 4148.833 [1596.000, 5331.000],  loss: 45.755726, mae: 12.691892, mean_q: 35.540359\n",
            " 28928/30000: episode: 4823, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward:  5.000, mean reward:  0.833 [-5.000,  5.000], mean action: 3490.167 [1253.000, 5456.000],  loss: 47.634735, mae: 13.915806, mean_q: 37.452770\n",
            " 28934/30000: episode: 4824, duration: 0.239s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3505.833 [262.000, 4871.000],  loss: 73.603111, mae: 13.967097, mean_q: 37.535511\n",
            " 28940/30000: episode: 4825, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3457.833 [1495.000, 5023.000],  loss: 49.097610, mae: 12.398152, mean_q: 34.844578\n",
            " 28946/30000: episode: 4826, duration: 0.252s, episode steps:   6, steps per second:  24, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2744.500 [1632.000, 4407.000],  loss: 66.849739, mae: 14.550174, mean_q: 40.162762\n",
            " 28952/30000: episode: 4827, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2337.333 [1180.000, 3192.000],  loss: 73.642235, mae: 13.402644, mean_q: 36.793682\n",
            " 28958/30000: episode: 4828, duration: 0.270s, episode steps:   6, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4666.500 [2831.000, 5656.000],  loss: 41.089489, mae: 14.215800, mean_q: 38.398014\n",
            " 28964/30000: episode: 4829, duration: 0.197s, episode steps:   6, steps per second:  30, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3859.833 [1530.000, 5597.000],  loss: 46.343628, mae: 13.238204, mean_q: 36.883610\n",
            " 28970/30000: episode: 4830, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 4196.167 [3699.000, 4871.000],  loss: 54.867748, mae: 13.979917, mean_q: 38.756565\n",
            " 28976/30000: episode: 4831, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3536.000 [1345.000, 5708.000],  loss: 56.313877, mae: 12.831861, mean_q: 35.872242\n",
            " 28982/30000: episode: 4832, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 1814.667 [262.000, 3343.000],  loss: 63.198364, mae: 13.766747, mean_q: 37.854774\n",
            " 28988/30000: episode: 4833, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1742.500 [262.000, 3311.000],  loss: 49.803143, mae: 13.447415, mean_q: 37.106712\n",
            " 28994/30000: episode: 4834, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1561.167 [262.000, 2988.000],  loss: 34.355198, mae: 12.565532, mean_q: 35.306019\n",
            " 29000/30000: episode: 4835, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 4203.667 [677.000, 5703.000],  loss: 63.705135, mae: 13.427726, mean_q: 37.501614\n",
            " 29006/30000: episode: 4836, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 45.000, mean reward:  7.500 [ 0.000, 25.000], mean action: 2141.333 [262.000, 5129.000],  loss: 74.977776, mae: 14.173610, mean_q: 37.576508\n",
            " 29012/30000: episode: 4837, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 1511.833 [262.000, 3696.000],  loss: 51.526722, mae: 13.608616, mean_q: 37.291718\n",
            " 29018/30000: episode: 4838, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 2209.500 [1239.000, 4871.000],  loss: 48.719830, mae: 13.225433, mean_q: 36.123211\n",
            " 29024/30000: episode: 4839, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3648.667 [1530.000, 5656.000],  loss: 50.205814, mae: 13.310338, mean_q: 36.087776\n",
            " 29030/30000: episode: 4840, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2798.333 [1133.000, 4486.000],  loss: 45.967846, mae: 12.448223, mean_q: 34.361713\n",
            " 29036/30000: episode: 4841, duration: 0.148s, episode steps:   6, steps per second:  40, episode reward: 70.000, mean reward: 11.667 [ 0.000, 20.000], mean action: 1751.833 [356.000, 4344.000],  loss: 55.246262, mae: 13.134366, mean_q: 36.812054\n",
            " 29042/30000: episode: 4842, duration: 0.175s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3782.667 [1634.000, 5616.000],  loss: 55.389515, mae: 12.883222, mean_q: 35.308517\n",
            " 29048/30000: episode: 4843, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3604.000 [1074.000, 5372.000],  loss: 54.726643, mae: 12.992934, mean_q: 35.524429\n",
            " 29054/30000: episode: 4844, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [-5.000, 20.000], mean action: 2465.500 [238.000, 3299.000],  loss: 39.052410, mae: 12.451512, mean_q: 35.120281\n",
            " 29060/30000: episode: 4845, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2225.167 [756.000, 4344.000],  loss: 54.374420, mae: 12.969718, mean_q: 35.445683\n",
            " 29066/30000: episode: 4846, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 35.000, mean reward:  5.833 [ 0.000, 30.000], mean action: 3715.000 [2003.000, 5656.000],  loss: 54.861450, mae: 14.159852, mean_q: 38.631367\n",
            " 29072/30000: episode: 4847, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2518.667 [1242.000, 5036.000],  loss: 50.019230, mae: 14.525299, mean_q: 38.418884\n",
            " 29078/30000: episode: 4848, duration: 0.179s, episode steps:   6, steps per second:  33, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2684.167 [1253.000, 5188.000],  loss: 59.573334, mae: 13.207867, mean_q: 36.810856\n",
            " 29084/30000: episode: 4849, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 35.000, mean reward:  5.833 [ 0.000, 35.000], mean action: 3738.833 [262.000, 5602.000],  loss: 58.601379, mae: 12.288369, mean_q: 34.202976\n",
            " 29090/30000: episode: 4850, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2123.667 [1421.000, 4616.000],  loss: 47.865570, mae: 13.009472, mean_q: 35.628628\n",
            " 29096/30000: episode: 4851, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3926.167 [1253.000, 5259.000],  loss: 49.692822, mae: 11.843315, mean_q: 33.615356\n",
            " 29102/30000: episode: 4852, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 65.000, mean reward: 10.833 [ 0.000, 35.000], mean action: 3650.333 [2544.000, 4397.000],  loss: 52.440159, mae: 12.233598, mean_q: 34.535202\n",
            " 29108/30000: episode: 4853, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2340.833 [547.000, 3685.000],  loss: 57.051697, mae: 12.816161, mean_q: 34.726398\n",
            " 29114/30000: episode: 4854, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2611.167 [846.000, 3921.000],  loss: 53.190033, mae: 12.700352, mean_q: 35.531563\n",
            " 29120/30000: episode: 4855, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3211.167 [1958.000, 4397.000],  loss: 45.307842, mae: 13.049499, mean_q: 36.605270\n",
            " 29126/30000: episode: 4856, duration: 0.184s, episode steps:   6, steps per second:  33, episode reward: 30.000, mean reward:  5.000 [-5.000, 35.000], mean action: 1765.500 [796.000, 4005.000],  loss: 53.399258, mae: 14.345942, mean_q: 39.448643\n",
            " 29132/30000: episode: 4857, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3181.333 [1421.000, 5598.000],  loss: 49.784359, mae: 13.307571, mean_q: 37.601501\n",
            " 29138/30000: episode: 4858, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2237.500 [973.000, 4871.000],  loss: 65.912804, mae: 14.687092, mean_q: 40.842518\n",
            " 29144/30000: episode: 4859, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2328.500 [79.000, 5096.000],  loss: 60.981689, mae: 14.119629, mean_q: 39.925327\n",
            " 29150/30000: episode: 4860, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 3175.000 [1421.000, 5608.000],  loss: 66.759117, mae: 14.882111, mean_q: 41.378551\n",
            " 29156/30000: episode: 4861, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2972.500 [1253.000, 4075.000],  loss: 57.273407, mae: 15.204651, mean_q: 41.795338\n",
            " 29162/30000: episode: 4862, duration: 0.182s, episode steps:   6, steps per second:  33, episode reward: 45.000, mean reward:  7.500 [ 0.000, 25.000], mean action: 3867.000 [1421.000, 5598.000],  loss: 47.095554, mae: 13.720528, mean_q: 38.984550\n",
            " 29168/30000: episode: 4863, duration: 0.152s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3098.333 [1076.000, 4979.000],  loss: 78.413338, mae: 14.537177, mean_q: 39.962772\n",
            " 29174/30000: episode: 4864, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3345.000 [1421.000, 5447.000],  loss: 49.681595, mae: 12.484687, mean_q: 34.875340\n",
            " 29180/30000: episode: 4865, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3550.500 [905.000, 5708.000],  loss: 56.125233, mae: 13.887299, mean_q: 37.974869\n",
            " 29186/30000: episode: 4866, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2850.667 [1421.000, 4176.000],  loss: 78.587273, mae: 12.362918, mean_q: 34.764309\n",
            " 29192/30000: episode: 4867, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3212.167 [1421.000, 5598.000],  loss: 39.774796, mae: 13.230415, mean_q: 36.302505\n",
            " 29198/30000: episode: 4868, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2925.833 [958.000, 5628.000],  loss: 55.881977, mae: 14.217532, mean_q: 38.463345\n",
            " 29204/30000: episode: 4869, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3027.833 [1693.000, 5339.000],  loss: 40.481266, mae: 13.914010, mean_q: 37.248215\n",
            " 29210/30000: episode: 4870, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 1954.333 [267.000, 3921.000],  loss: 55.877838, mae: 13.538948, mean_q: 36.564087\n",
            " 29216/30000: episode: 4871, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2159.500 [1145.000, 3293.000],  loss: 78.207329, mae: 13.659485, mean_q: 38.122303\n",
            " 29222/30000: episode: 4872, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 3094.667 [2558.000, 3202.000],  loss: 72.098320, mae: 14.053241, mean_q: 38.175098\n",
            " 29228/30000: episode: 4873, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 25.000, mean reward:  4.167 [-5.000, 25.000], mean action: 3147.500 [69.000, 5466.000],  loss: 41.416317, mae: 12.459382, mean_q: 34.937283\n",
            " 29234/30000: episode: 4874, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1193.000 [161.000, 2454.000],  loss: 52.234024, mae: 13.201495, mean_q: 36.433689\n",
            " 29240/30000: episode: 4875, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2907.333 [1514.000, 5269.000],  loss: 49.155071, mae: 14.315915, mean_q: 38.012264\n",
            " 29246/30000: episode: 4876, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2825.833 [2263.000, 3921.000],  loss: 32.668552, mae: 12.476499, mean_q: 34.724720\n",
            " 29252/30000: episode: 4877, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2018.667 [476.000, 3497.000],  loss: 49.770496, mae: 13.571584, mean_q: 37.273479\n",
            " 29258/30000: episode: 4878, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 30.000, mean reward:  5.000 [ 0.000, 15.000], mean action: 1984.333 [1059.000, 2558.000],  loss: 42.649441, mae: 12.923428, mean_q: 35.635021\n",
            " 29264/30000: episode: 4879, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2638.500 [729.000, 3687.000],  loss: 55.359173, mae: 13.138947, mean_q: 36.555546\n",
            " 29270/30000: episode: 4880, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [-5.000,  5.000], mean action: 2289.333 [1560.000, 4185.000],  loss: 50.683430, mae: 13.569146, mean_q: 37.707027\n",
            " 29276/30000: episode: 4881, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 4121.333 [1710.000, 5259.000],  loss: 57.706036, mae: 14.007874, mean_q: 37.677761\n",
            " 29282/30000: episode: 4882, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 2558.000 [2558.000, 2558.000],  loss: 48.392349, mae: 13.349717, mean_q: 37.614136\n",
            " 29288/30000: episode: 4883, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2521.167 [2277.000, 2618.000],  loss: 62.637772, mae: 12.971614, mean_q: 36.651440\n",
            " 29294/30000: episode: 4884, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 10.000, mean reward:  1.667 [-5.000, 10.000], mean action: 4116.667 [2558.000, 5667.000],  loss: 82.173958, mae: 13.698440, mean_q: 38.239231\n",
            " 29300/30000: episode: 4885, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3097.833 [1253.000, 4871.000],  loss: 56.789455, mae: 11.834129, mean_q: 34.234394\n",
            " 29306/30000: episode: 4886, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3928.667 [2558.000, 5656.000],  loss: 45.955490, mae: 14.232104, mean_q: 39.312252\n",
            " 29312/30000: episode: 4887, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3990.333 [2558.000, 5656.000],  loss: 61.020123, mae: 13.851158, mean_q: 37.982700\n",
            " 29318/30000: episode: 4888, duration: 0.169s, episode steps:   6, steps per second:  35, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2150.167 [375.000, 4220.000],  loss: 68.679718, mae: 13.903407, mean_q: 38.182438\n",
            " 29324/30000: episode: 4889, duration: 0.149s, episode steps:   6, steps per second:  40, episode reward: 35.000, mean reward:  5.833 [ 0.000, 15.000], mean action: 2830.667 [1596.000, 5096.000],  loss: 78.968712, mae: 12.649879, mean_q: 35.421997\n",
            " 29330/30000: episode: 4890, duration: 0.219s, episode steps:   6, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3221.000 [2263.000, 5140.000],  loss: 38.526615, mae: 12.388317, mean_q: 34.301868\n",
            " 29336/30000: episode: 4891, duration: 0.256s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2723.000 [195.000, 4871.000],  loss: 71.813423, mae: 13.631073, mean_q: 37.476170\n",
            " 29342/30000: episode: 4892, duration: 0.268s, episode steps:   6, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2876.833 [775.000, 5532.000],  loss: 57.851696, mae: 13.814975, mean_q: 37.460289\n",
            " 29348/30000: episode: 4893, duration: 0.242s, episode steps:   6, steps per second:  25, episode reward: 120.000, mean reward: 20.000 [ 0.000, 35.000], mean action: 3012.667 [1596.000, 4185.000],  loss: 64.061363, mae: 12.622773, mean_q: 34.565750\n",
            " 29354/30000: episode: 4894, duration: 0.244s, episode steps:   6, steps per second:  25, episode reward: 55.000, mean reward:  9.167 [ 0.000, 20.000], mean action: 3095.500 [1083.000, 5708.000],  loss: 66.614380, mae: 13.197795, mean_q: 35.434628\n",
            " 29360/30000: episode: 4895, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2061.833 [363.000, 3712.000],  loss: 69.930244, mae: 14.159566, mean_q: 38.035038\n",
            " 29366/30000: episode: 4896, duration: 0.253s, episode steps:   6, steps per second:  24, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3080.333 [355.000, 4656.000],  loss: 69.763588, mae: 13.919375, mean_q: 37.972786\n",
            " 29372/30000: episode: 4897, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4596.833 [3685.000, 5656.000],  loss: 63.221146, mae: 13.398113, mean_q: 36.896961\n",
            " 29378/30000: episode: 4898, duration: 0.232s, episode steps:   6, steps per second:  26, episode reward: -10.000, mean reward: -1.667 [-5.000,  5.000], mean action: 4586.833 [3249.000, 5140.000],  loss: 48.602024, mae: 13.469330, mean_q: 37.418468\n",
            " 29384/30000: episode: 4899, duration: 0.272s, episode steps:   6, steps per second:  22, episode reward: 25.000, mean reward:  4.167 [ 0.000, 20.000], mean action: 3752.333 [2652.000, 4497.000],  loss: 44.910015, mae: 13.085983, mean_q: 37.229450\n",
            " 29390/30000: episode: 4900, duration: 0.248s, episode steps:   6, steps per second:  24, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2786.667 [1145.000, 3712.000],  loss: 58.204128, mae: 14.222493, mean_q: 38.124546\n",
            " 29396/30000: episode: 4901, duration: 0.269s, episode steps:   6, steps per second:  22, episode reward: 170.000, mean reward: 28.333 [20.000, 35.000], mean action: 3919.833 [1596.000, 5656.000],  loss: 43.803604, mae: 13.125791, mean_q: 36.318695\n",
            " 29402/30000: episode: 4902, duration: 0.227s, episode steps:   6, steps per second:  26, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 3079.167 [1216.000, 5107.000],  loss: 33.828152, mae: 12.344630, mean_q: 35.336533\n",
            " 29408/30000: episode: 4903, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 40.000, mean reward:  6.667 [ 0.000, 40.000], mean action: 2710.167 [814.000, 4185.000],  loss: 52.050762, mae: 13.371803, mean_q: 36.889256\n",
            " 29414/30000: episode: 4904, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2910.667 [334.000, 4616.000],  loss: 45.717407, mae: 12.552231, mean_q: 34.547039\n",
            " 29420/30000: episode: 4905, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3180.333 [61.000, 5148.000],  loss: 52.091488, mae: 11.895051, mean_q: 34.258316\n",
            " 29426/30000: episode: 4906, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2219.167 [140.000, 4176.000],  loss: 41.171093, mae: 13.132030, mean_q: 36.050472\n",
            " 29432/30000: episode: 4907, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4317.167 [2378.000, 5628.000],  loss: 67.747017, mae: 13.506839, mean_q: 37.963867\n",
            " 29438/30000: episode: 4908, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3331.833 [537.000, 5622.000],  loss: 36.542324, mae: 13.096493, mean_q: 36.615917\n",
            " 29444/30000: episode: 4909, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: -15.000, mean reward: -2.500 [-5.000,  5.000], mean action: 4338.667 [3712.000, 4464.000],  loss: 43.905518, mae: 13.098941, mean_q: 35.980190\n",
            " 29450/30000: episode: 4910, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2575.333 [375.000, 4442.000],  loss: 80.938713, mae: 14.349144, mean_q: 38.220428\n",
            " 29456/30000: episode: 4911, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 4237.500 [3070.000, 5011.000],  loss: 75.849533, mae: 13.937129, mean_q: 37.457821\n",
            " 29462/30000: episode: 4912, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward:  5.000, mean reward:  0.833 [-5.000, 15.000], mean action: 1903.833 [183.000, 4616.000],  loss: 58.646347, mae: 12.802634, mean_q: 35.476284\n",
            " 29468/30000: episode: 4913, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2359.500 [1426.000, 4530.000],  loss: 66.112198, mae: 14.301934, mean_q: 38.751987\n",
            " 29474/30000: episode: 4914, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2224.333 [946.000, 3921.000],  loss: 53.329758, mae: 12.998280, mean_q: 36.179268\n",
            " 29480/30000: episode: 4915, duration: 0.158s, episode steps:   6, steps per second:  38, episode reward: 40.000, mean reward:  6.667 [ 0.000, 40.000], mean action: 1781.667 [140.000, 3221.000],  loss: 51.113682, mae: 13.789018, mean_q: 37.719311\n",
            " 29486/30000: episode: 4916, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3398.000 [1212.000, 5619.000],  loss: 56.580811, mae: 12.429748, mean_q: 34.089184\n",
            " 29492/30000: episode: 4917, duration: 0.156s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3874.333 [1438.000, 5147.000],  loss: 41.421474, mae: 12.894938, mean_q: 35.599396\n",
            " 29498/30000: episode: 4918, duration: 0.181s, episode steps:   6, steps per second:  33, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 4238.333 [3311.000, 4681.000],  loss: 51.267040, mae: 12.279305, mean_q: 34.962708\n",
            " 29504/30000: episode: 4919, duration: 0.150s, episode steps:   6, steps per second:  40, episode reward: 45.000, mean reward:  7.500 [ 0.000, 45.000], mean action: 2144.833 [814.000, 4616.000],  loss: 66.735443, mae: 12.958427, mean_q: 35.585098\n",
            " 29510/30000: episode: 4920, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3093.333 [2455.000, 4992.000],  loss: 32.795212, mae: 13.208327, mean_q: 36.953869\n",
            " 29516/30000: episode: 4921, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3086.167 [733.000, 5096.000],  loss: 36.379745, mae: 14.488473, mean_q: 38.535938\n",
            " 29522/30000: episode: 4922, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 3580.167 [1160.000, 4783.000],  loss: 64.630295, mae: 13.520520, mean_q: 36.831646\n",
            " 29528/30000: episode: 4923, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2559.000 [138.000, 5656.000],  loss: 44.081188, mae: 14.020772, mean_q: 38.539532\n",
            " 29534/30000: episode: 4924, duration: 0.179s, episode steps:   6, steps per second:  33, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 2866.667 [30.000, 4826.000],  loss: 47.912037, mae: 14.334144, mean_q: 39.071815\n",
            " 29540/30000: episode: 4925, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 2848.167 [1573.000, 4203.000],  loss: 49.413540, mae: 14.923684, mean_q: 40.624828\n",
            " 29546/30000: episode: 4926, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3078.333 [238.000, 5466.000],  loss: 50.547134, mae: 13.982658, mean_q: 38.381504\n",
            " 29552/30000: episode: 4927, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3393.167 [2455.000, 5656.000],  loss: 36.843044, mae: 12.918915, mean_q: 35.641621\n",
            " 29558/30000: episode: 4928, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 15.000, mean reward:  2.500 [-5.000, 15.000], mean action: 2679.500 [1596.000, 4185.000],  loss: 25.850286, mae: 12.877419, mean_q: 35.640694\n",
            " 29564/30000: episode: 4929, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3610.333 [429.000, 5616.000],  loss: 71.166786, mae: 13.157363, mean_q: 36.092758\n",
            " 29570/30000: episode: 4930, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3806.833 [2585.000, 5348.000],  loss: 45.850052, mae: 12.101883, mean_q: 34.057705\n",
            " 29576/30000: episode: 4931, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [-5.000, 20.000], mean action: 3116.667 [1596.000, 5656.000],  loss: 69.289101, mae: 13.223187, mean_q: 36.508663\n",
            " 29582/30000: episode: 4932, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 35.000, mean reward:  5.833 [ 0.000, 20.000], mean action: 2023.167 [946.000, 4109.000],  loss: 42.986240, mae: 11.803472, mean_q: 33.380005\n",
            " 29588/30000: episode: 4933, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 4473.833 [3712.000, 4681.000],  loss: 44.236797, mae: 13.301666, mean_q: 36.028332\n",
            " 29594/30000: episode: 4934, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3674.333 [1599.000, 5107.000],  loss: 67.895760, mae: 13.791367, mean_q: 37.563114\n",
            " 29600/30000: episode: 4935, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward: 25.000, mean reward:  4.167 [ 0.000, 25.000], mean action: 4206.167 [1495.000, 5466.000],  loss: 51.144863, mae: 13.763977, mean_q: 37.283756\n",
            " 29606/30000: episode: 4936, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2363.000 [238.000, 3712.000],  loss: 43.451244, mae: 13.320647, mean_q: 36.655819\n",
            " 29612/30000: episode: 4937, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2987.500 [818.000, 5582.000],  loss: 81.598061, mae: 13.975524, mean_q: 37.794819\n",
            " 29618/30000: episode: 4938, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3602.000 [537.000, 5675.000],  loss: 39.682739, mae: 13.392990, mean_q: 37.220112\n",
            " 29624/30000: episode: 4939, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4297.500 [2852.000, 5651.000],  loss: 66.526894, mae: 13.525021, mean_q: 36.133125\n",
            " 29630/30000: episode: 4940, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3191.833 [2173.000, 4806.000],  loss: 43.250332, mae: 13.342828, mean_q: 37.157608\n",
            " 29636/30000: episode: 4941, duration: 0.159s, episode steps:   6, steps per second:  38, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2597.000 [30.000, 4967.000],  loss: 83.981430, mae: 13.151824, mean_q: 36.989666\n",
            " 29642/30000: episode: 4942, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3054.000 [2008.000, 4444.000],  loss: 58.966705, mae: 13.674743, mean_q: 37.698780\n",
            " 29648/30000: episode: 4943, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4294.000 [2357.000, 5359.000],  loss: 44.503613, mae: 13.345708, mean_q: 37.845692\n",
            " 29654/30000: episode: 4944, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3163.833 [245.000, 5708.000],  loss: 28.003992, mae: 12.266326, mean_q: 35.227734\n",
            " 29660/30000: episode: 4945, duration: 0.157s, episode steps:   6, steps per second:  38, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 3344.500 [939.000, 5486.000],  loss: 56.105488, mae: 11.689990, mean_q: 34.162777\n",
            " 29666/30000: episode: 4946, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward: 40.000, mean reward:  6.667 [ 0.000, 25.000], mean action: 3056.667 [592.000, 3712.000],  loss: 71.027077, mae: 13.285192, mean_q: 38.135799\n",
            " 29672/30000: episode: 4947, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3605.000 [3070.000, 3712.000],  loss: 47.693420, mae: 12.777858, mean_q: 37.219898\n",
            " 29678/30000: episode: 4948, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3654.333 [1145.000, 5736.000],  loss: 69.168297, mae: 13.618817, mean_q: 38.634232\n",
            " 29684/30000: episode: 4949, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [ 0.000,  5.000], mean action: 3060.167 [733.000, 4397.000],  loss: 50.572281, mae: 13.586644, mean_q: 39.179016\n",
            " 29690/30000: episode: 4950, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: 25.000, mean reward:  4.167 [ 0.000, 15.000], mean action: 2639.000 [560.000, 5619.000],  loss: 32.583252, mae: 14.354280, mean_q: 40.143101\n",
            " 29696/30000: episode: 4951, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 2652.167 [145.000, 4005.000],  loss: 41.884708, mae: 13.531384, mean_q: 37.683140\n",
            " 29702/30000: episode: 4952, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1420.500 [140.000, 3712.000],  loss: 51.222748, mae: 12.587883, mean_q: 35.877563\n",
            " 29708/30000: episode: 4953, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3170.000 [35.000, 4244.000],  loss: 33.484913, mae: 12.677414, mean_q: 36.336445\n",
            " 29714/30000: episode: 4954, duration: 0.167s, episode steps:   6, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3943.000 [1530.000, 5619.000],  loss: 58.513264, mae: 12.852479, mean_q: 36.465145\n",
            " 29720/30000: episode: 4955, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2551.500 [733.000, 4616.000],  loss: 64.931526, mae: 12.940854, mean_q: 37.426453\n",
            " 29726/30000: episode: 4956, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 3776.167 [3712.000, 4097.000],  loss: 76.999878, mae: 13.448813, mean_q: 37.618176\n",
            " 29732/30000: episode: 4957, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2790.167 [525.000, 4656.000],  loss: 51.396656, mae: 12.884393, mean_q: 37.233761\n",
            " 29738/30000: episode: 4958, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3672.000 [650.000, 5607.000],  loss: 32.186821, mae: 12.916198, mean_q: 37.178036\n",
            " 29744/30000: episode: 4959, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2710.833 [663.000, 4185.000],  loss: 48.810791, mae: 12.944097, mean_q: 36.719852\n",
            " 29750/30000: episode: 4960, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: -5.000, mean reward: -0.833 [-5.000, 15.000], mean action: 1864.500 [1495.000, 3712.000],  loss: 102.134956, mae: 14.608307, mean_q: 40.127956\n",
            " 29756/30000: episode: 4961, duration: 0.161s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [-5.000, 20.000], mean action: 3517.000 [1560.000, 5708.000],  loss: 54.105244, mae: 13.111674, mean_q: 36.968849\n",
            " 29762/30000: episode: 4962, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 3712.000 [3712.000, 3712.000],  loss: 106.826317, mae: 12.825133, mean_q: 37.714909\n",
            " 29768/30000: episode: 4963, duration: 0.235s, episode steps:   6, steps per second:  26, episode reward:  5.000, mean reward:  0.833 [ 0.000,  5.000], mean action: 3800.833 [1180.000, 5717.000],  loss: 51.129684, mae: 13.298779, mean_q: 37.211788\n",
            " 29774/30000: episode: 4964, duration: 0.260s, episode steps:   6, steps per second:  23, episode reward: -25.000, mean reward: -4.167 [-5.000,  0.000], mean action: 3712.000 [3712.000, 3712.000],  loss: 54.561207, mae: 13.632729, mean_q: 38.034676\n",
            " 29780/30000: episode: 4965, duration: 0.254s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3380.333 [1632.000, 5708.000],  loss: 82.763222, mae: 14.196004, mean_q: 38.559875\n",
            " 29786/30000: episode: 4966, duration: 0.257s, episode steps:   6, steps per second:  23, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2608.833 [608.000, 5034.000],  loss: 73.729683, mae: 12.619620, mean_q: 36.314030\n",
            " 29792/30000: episode: 4967, duration: 0.247s, episode steps:   6, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2954.333 [1002.000, 5717.000],  loss: 57.734203, mae: 14.129483, mean_q: 38.499020\n",
            " 29798/30000: episode: 4968, duration: 0.265s, episode steps:   6, steps per second:  23, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 3470.000 [852.000, 5199.000],  loss: 41.878529, mae: 12.433578, mean_q: 36.112881\n",
            " 29804/30000: episode: 4969, duration: 0.245s, episode steps:   6, steps per second:  24, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 1735.500 [140.000, 3941.000],  loss: 58.586029, mae: 13.410405, mean_q: 37.400635\n",
            " 29810/30000: episode: 4970, duration: 0.264s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2202.333 [185.000, 5708.000],  loss: 63.272797, mae: 13.990982, mean_q: 37.994461\n",
            " 29816/30000: episode: 4971, duration: 0.265s, episode steps:   6, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2752.000 [534.000, 5708.000],  loss: 47.610851, mae: 14.275460, mean_q: 38.514576\n",
            " 29822/30000: episode: 4972, duration: 0.279s, episode steps:   6, steps per second:  22, episode reward: -20.000, mean reward: -3.333 [-5.000,  0.000], mean action: 1500.833 [1495.000, 1530.000],  loss: 63.297497, mae: 13.193822, mean_q: 36.832481\n",
            " 29828/30000: episode: 4973, duration: 0.285s, episode steps:   6, steps per second:  21, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3556.167 [2008.000, 5051.000],  loss: 55.410736, mae: 13.878896, mean_q: 37.631001\n",
            " 29834/30000: episode: 4974, duration: 0.206s, episode steps:   6, steps per second:  29, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 2306.500 [1305.000, 5305.000],  loss: 77.828247, mae: 14.676921, mean_q: 39.589455\n",
            " 29840/30000: episode: 4975, duration: 0.177s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 3525.167 [1596.000, 4573.000],  loss: 52.432713, mae: 12.492080, mean_q: 35.427109\n",
            " 29846/30000: episode: 4976, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1893.667 [1530.000, 3712.000],  loss: 55.459499, mae: 13.043830, mean_q: 36.852459\n",
            " 29852/30000: episode: 4977, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1893.667 [1530.000, 3712.000],  loss: 46.915722, mae: 13.390758, mean_q: 37.795322\n",
            " 29858/30000: episode: 4978, duration: 0.172s, episode steps:   6, steps per second:  35, episode reward: -5.000, mean reward: -0.833 [-5.000,  0.000], mean action: 3535.000 [1596.000, 5656.000],  loss: 32.973099, mae: 12.489609, mean_q: 35.907124\n",
            " 29864/30000: episode: 4979, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2071.167 [1018.000, 5212.000],  loss: 62.868999, mae: 13.675153, mean_q: 38.753292\n",
            " 29870/30000: episode: 4980, duration: 0.174s, episode steps:   6, steps per second:  34, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 2695.667 [1530.000, 5708.000],  loss: 54.084316, mae: 13.272281, mean_q: 37.401875\n",
            " 29876/30000: episode: 4981, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward: -15.000, mean reward: -2.500 [-5.000,  0.000], mean action: 1822.167 [1530.000, 2861.000],  loss: 60.225163, mae: 13.688117, mean_q: 38.128139\n",
            " 29882/30000: episode: 4982, duration: 0.171s, episode steps:   6, steps per second:  35, episode reward: 35.000, mean reward:  5.833 [-5.000, 40.000], mean action: 2912.833 [1530.000, 4491.000],  loss: 51.132000, mae: 12.925240, mean_q: 36.384762\n",
            " 29888/30000: episode: 4983, duration: 0.154s, episode steps:   6, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3330.333 [1191.000, 5656.000],  loss: 44.783081, mae: 13.414504, mean_q: 37.791042\n",
            " 29894/30000: episode: 4984, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward: 20.000, mean reward:  3.333 [ 0.000, 15.000], mean action: 2808.333 [1094.000, 5554.000],  loss: 62.476719, mae: 14.548028, mean_q: 40.307304\n",
            " 29900/30000: episode: 4985, duration: 0.145s, episode steps:   6, steps per second:  41, episode reward: 20.000, mean reward:  3.333 [-5.000, 25.000], mean action: 3094.000 [870.000, 4444.000],  loss: 51.930737, mae: 12.337535, mean_q: 34.921600\n",
            " 29906/30000: episode: 4986, duration: 0.180s, episode steps:   6, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3264.333 [1495.000, 5019.000],  loss: 38.603588, mae: 13.068096, mean_q: 37.129787\n",
            " 29912/30000: episode: 4987, duration: 0.243s, episode steps:   6, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2028.167 [568.000, 3249.000],  loss: 66.278152, mae: 13.892056, mean_q: 38.053802\n",
            " 29918/30000: episode: 4988, duration: 0.367s, episode steps:   6, steps per second:  16, episode reward:  5.000, mean reward:  0.833 [-5.000, 10.000], mean action: 2910.500 [2277.000, 3712.000],  loss: 42.844440, mae: 13.342907, mean_q: 37.872505\n",
            " 29924/30000: episode: 4989, duration: 0.169s, episode steps:   6, steps per second:  36, episode reward: 30.000, mean reward:  5.000 [ 0.000, 30.000], mean action: 2824.167 [334.000, 5163.000],  loss: 42.916157, mae: 13.431399, mean_q: 37.358944\n",
            " 29930/30000: episode: 4990, duration: 0.185s, episode steps:   6, steps per second:  33, episode reward: 20.000, mean reward:  3.333 [ 0.000, 10.000], mean action: 3662.667 [2823.000, 5096.000],  loss: 51.417004, mae: 13.622359, mean_q: 38.117149\n",
            " 29936/30000: episode: 4991, duration: 0.155s, episode steps:   6, steps per second:  39, episode reward: -10.000, mean reward: -1.667 [-5.000,  0.000], mean action: 3385.000 [1212.000, 4871.000],  loss: 37.787476, mae: 12.415103, mean_q: 34.666424\n",
            " 29942/30000: episode: 4992, duration: 0.183s, episode steps:   6, steps per second:  33, episode reward: 15.000, mean reward:  2.500 [ 0.000, 15.000], mean action: 2722.000 [140.000, 4576.000],  loss: 75.320122, mae: 13.728367, mean_q: 37.542400\n",
            " 29948/30000: episode: 4993, duration: 0.160s, episode steps:   6, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2486.833 [541.000, 5369.000],  loss: 106.335762, mae: 13.163254, mean_q: 37.696243\n",
            " 29954/30000: episode: 4994, duration: 0.176s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3521.667 [1710.000, 5140.000],  loss: 71.366692, mae: 14.347153, mean_q: 39.211758\n",
            " 29960/30000: episode: 4995, duration: 0.152s, episode steps:   6, steps per second:  40, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 4223.833 [2150.000, 4913.000],  loss: 46.851925, mae: 13.596326, mean_q: 37.815472\n",
            " 29966/30000: episode: 4996, duration: 0.178s, episode steps:   6, steps per second:  34, episode reward: 10.000, mean reward:  1.667 [ 0.000, 10.000], mean action: 2609.333 [316.000, 4840.000],  loss: 64.678719, mae: 13.304950, mean_q: 37.112514\n",
            " 29972/30000: episode: 4997, duration: 0.151s, episode steps:   6, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4349.833 [3618.000, 5747.000],  loss: 62.206013, mae: 13.423583, mean_q: 37.269772\n",
            " 29978/30000: episode: 4998, duration: 0.179s, episode steps:   6, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3277.667 [587.000, 4911.000],  loss: 59.211529, mae: 13.286514, mean_q: 36.623264\n",
            " 29984/30000: episode: 4999, duration: 0.153s, episode steps:   6, steps per second:  39, episode reward: 45.000, mean reward:  7.500 [-5.000, 35.000], mean action: 2902.333 [1202.000, 5001.000],  loss: 52.900471, mae: 12.101757, mean_q: 33.581905\n",
            " 29990/30000: episode: 5000, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward: 10.000, mean reward:  1.667 [-5.000, 15.000], mean action: 4404.833 [2277.000, 5619.000],  loss: 46.331585, mae: 11.963632, mean_q: 34.034836\n",
            " 29996/30000: episode: 5001, duration: 0.160s, episode steps:   6, steps per second:  37, episode reward: 10.000, mean reward:  1.667 [-5.000, 20.000], mean action: 2286.667 [608.000, 4185.000],  loss: 55.442562, mae: 13.752791, mean_q: 38.219688\n",
            "done, took 887.434 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cbaf994aaa0>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "dqn.fit(env, nb_steps=30000, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR9WJMpNGCHa"
      },
      "outputs": [],
      "source": [
        "# Save the DQN agent\n",
        "dqn.save_weights(\"dqn_weights.h5f\", overwrite=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXIkRq-eGDLr"
      },
      "outputs": [],
      "source": [
        "# Load the DQN agent\n",
        "dqn.load_weights(\"dqn_weights.h5f\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EYArQP7H_4Nf",
        "outputId": "7b176856-4e52-4016-a12c-da26c05015d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 100 episodes ...\n",
            "melee tawny\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0. -0.]\n",
            " [-0. -0. -0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee tawny\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0. -0. -0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee tawny\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee tawny\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee tawny\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee tawny\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 1: reward: -25.000, steps: 6\n",
            "melee murks\n",
            "[[1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "fells murks\n",
            "[[1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "gaped murks\n",
            "[[1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "gaped murks\n",
            "[[1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "gaped murks\n",
            "[[1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "gaped murks\n",
            "[[1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 2: reward: -15.000, steps: 6\n",
            "melee erupt\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "eerie erupt\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [ 1. -1. -1.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "seest erupt\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [ 1. -1. -1.  0. -1.]\n",
            " [ 0. -1. -1.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "paver erupt\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [ 1. -1. -1.  0. -1.]\n",
            " [ 0. -1. -1.  0.  1.]\n",
            " [-1.  0.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "beery erupt\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [ 1. -1. -1.  0. -1.]\n",
            " [ 0. -1. -1.  0.  1.]\n",
            " [-1.  0.  0. -1. -1.]\n",
            " [ 0. -1. -1. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "seeks erupt\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [ 1. -1. -1.  0. -1.]\n",
            " [ 0. -1. -1.  0.  1.]\n",
            " [-1.  0.  0. -1. -1.]\n",
            " [ 0. -1. -1. -1.  0.]\n",
            " [ 0. -1. -1.  0.  0.]]\n",
            "Episode 3: reward: 15.000, steps: 6\n",
            "melee night\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0. -0.  0. -0.]\n",
            " [ 0. -0. -0.  0.  0.]\n",
            " [-0.  0.  0. -0. -0.]\n",
            " [ 0. -0. -0. -0.  0.]\n",
            " [ 0. -0. -0.  0.  0.]]\n",
            "melee night\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0. -0.  0.  0.]\n",
            " [-0.  0.  0. -0. -0.]\n",
            " [ 0. -0. -0. -0.  0.]\n",
            " [ 0. -0. -0.  0.  0.]]\n",
            "melee night\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0. -0. -0.]\n",
            " [ 0. -0. -0. -0.  0.]\n",
            " [ 0. -0. -0.  0.  0.]]\n",
            "melee night\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0. -0. -0.  0.]\n",
            " [ 0. -0. -0.  0.  0.]]\n",
            "melee night\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0. -0.  0.  0.]]\n",
            "melee night\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 4: reward: -25.000, steps: 6\n",
            "melee rates\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "refly rates\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "octet rates\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  1.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "seeks rates\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  1.  1. -1.]\n",
            " [-1. -1. -1.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "finif rates\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  1.  1. -1.]\n",
            " [-1. -1. -1.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "finif rates\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  1.  1. -1.]\n",
            " [-1. -1. -1.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 5: reward: 15.000, steps: 6\n",
            "melee stele\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [-0. -0. -0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "abaca stele\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [-0. -0. -0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "abaca stele\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0. -0. -0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "abaca stele\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "abaca stele\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "abaca stele\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 6: reward: -5.000, steps: 6\n",
            "melee stabs\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee stabs\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 84: reward: -5.000, steps: 6\n",
            "melee talky\n",
            "[[0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "seeds talky\n",
            "[[0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "seeds talky\n",
            "[[0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "seeds talky\n",
            "[[0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "seeds talky\n",
            "[[0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "seeds talky\n",
            "[[0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 85: reward: -20.000, steps: 6\n",
            "melee chaos\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee chaos\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee chaos\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee chaos\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee chaos\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee chaos\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 86: reward: -25.000, steps: 6\n",
            "melee bares\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "refly bares\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "ruder bares\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "cedes bares\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1. -1.]\n",
            " [ 0. -1.  0.  1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "defog bares\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1. -1.]\n",
            " [ 0. -1.  0.  1.  1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "esses bares\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1. -1.]\n",
            " [ 0. -1.  0.  1.  1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [-1. -1. -1.  1.  1.]]\n",
            "Episode 87: reward: 40.000, steps: 6\n",
            "melee tiger\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-0. -0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0. -0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [-0. -0. -0.  0.  0.]]\n",
            "refly tiger\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-0.  0.  0.  0. -0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [-0. -0. -0.  0.  0.]]\n",
            "ruder tiger\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [-0. -0. -0.  0.  0.]]\n",
            "caped tiger\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [-0. -0. -0.  0.  0.]]\n",
            "cedes tiger\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0.  1.  0.]\n",
            " [-0. -0. -0.  0.  0.]]\n",
            "esses tiger\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0.  1.  0.]\n",
            " [-1.  0.  0.  1.  0.]]\n",
            "Episode 88: reward: 5.000, steps: 6\n",
            "melee snoot\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [-0. -0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]]\n",
            "melee snoot\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]]\n",
            "melee snoot\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]]\n",
            "melee snoot\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]]\n",
            "melee snoot\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]]\n",
            "melee snoot\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 89: reward: -25.000, steps: 6\n",
            "melee moors\n",
            "[[1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "fells moors\n",
            "[[1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "gaped moors\n",
            "[[1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "gaped moors\n",
            "[[1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "gaped moors\n",
            "[[1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "gaped moors\n",
            "[[1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 90: reward: -15.000, steps: 6\n",
            "melee vests\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "natty vests\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0. -1.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "humpy vests\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0. -1.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "humpy vests\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0. -1.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "humpy vests\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0. -1.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "humpy vests\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0. -1.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 91: reward: -15.000, steps: 6\n",
            "melee hands\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee hands\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee hands\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee hands\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee hands\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee hands\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 92: reward: -25.000, steps: 6\n",
            "melee cures\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "refly cures\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "ruder cures\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "snoot cures\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  1.  0.  1. -1.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "quill cures\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  1.  0.  1. -1.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 0.  1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "ruder cures\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  1.  0.  1. -1.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 0.  1.  0.  0.  0.]\n",
            " [-1.  1.  0.  1. -1.]]\n",
            "Episode 93: reward: 40.000, steps: 6\n",
            "melee clone\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [-0. -0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0. -0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0. -0.]]\n",
            "abaca clone\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [-0.  0.  0.  0. -0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0. -0.]]\n",
            "allay clone\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0.  1. -1.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0. -0.]]\n",
            "orbed clone\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0.  1. -1.  0.  0.]\n",
            " [-1.  0.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0. -0.]]\n",
            "fizzy clone\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0.  1. -1.  0.  0.]\n",
            " [-1.  0.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0. -0.]]\n",
            "fizzy clone\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0.  1. -1.  0.  0.]\n",
            " [-1.  0.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 94: reward: 10.000, steps: 6\n",
            "melee pumas\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [-0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "shoos pumas\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [-0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "pizza pumas\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 1.  0.  0.  0. -1.]\n",
            " [-0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "esses pumas\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 1.  0.  0.  0. -1.]\n",
            " [ 0. -1. -1.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "esses pumas\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 1.  0.  0.  0. -1.]\n",
            " [ 0. -1. -1.  0.  1.]\n",
            " [ 0. -1. -1.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "soles pumas\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 1.  0.  0.  0. -1.]\n",
            " [ 0. -1. -1.  0.  1.]\n",
            " [ 0. -1. -1.  0.  1.]\n",
            " [-1.  0.  0.  0.  1.]]\n",
            "Episode 95: reward: -5.000, steps: 6\n",
            "melee corer\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [ 0. -0. -0.  0.  0.]\n",
            " [ 0. -0. -0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]]\n",
            "refly corer\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [ 0. -0. -0.  0.  0.]\n",
            " [ 0. -0. -0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]]\n",
            "ruder corer\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [ 0. -0. -0.  0.  0.]\n",
            " [ 0. -0. -0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]]\n",
            "caped corer\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [ 1.  0.  0.  1.  0.]\n",
            " [ 0. -0. -0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]]\n",
            "esses corer\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [ 1.  0.  0.  1.  0.]\n",
            " [-1.  0.  0.  1.  0.]\n",
            " [-0.  0.  0.  0.  0.]]\n",
            "level corer\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [ 1.  0.  0.  1.  0.]\n",
            " [-1.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0.  1.  0.]]\n",
            "Episode 96: reward: 5.000, steps: 6\n",
            "melee usage\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [-0. -0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]]\n",
            "asses usage\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [-1.  1. -1. -1. -1.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]]\n",
            "rarer usage\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [-1.  1. -1. -1. -1.]\n",
            " [ 0. -1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]]\n",
            "bongo usage\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [-1.  1. -1. -1. -1.]\n",
            " [ 0. -1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]]\n",
            "jewel usage\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [-1.  1. -1. -1. -1.]\n",
            " [ 0. -1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0. -1.  0.]\n",
            " [ 0. -0.  0.  0.  0.]]\n",
            "doubt usage\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [-1.  1. -1. -1. -1.]\n",
            " [ 0. -1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0. -1.  0.]\n",
            " [ 0.  0. -1.  0.  0.]]\n",
            "Episode 97: reward: 30.000, steps: 6\n",
            "melee watts\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [-0.  0. -0. -0. -0.]\n",
            " [ 0. -0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]]\n",
            "melee watts\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]]\n",
            "melee watts\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]]\n",
            "melee watts\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]]\n",
            "melee watts\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]]\n",
            "melee watts\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 98: reward: -25.000, steps: 6\n",
            "melee franc\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee franc\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee franc\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee franc\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee franc\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee franc\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 99: reward: -25.000, steps: 6\n",
            "melee shale\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "abaca shale\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [-1.  0.  1.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "vizor shale\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [-1.  0.  1.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "vizor shale\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [-1.  0.  1.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "vizor shale\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [-1.  0.  1.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "vizor shale\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [-1.  0.  1.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 100: reward: 0.000, steps: 6\n",
            "Training for 1000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "  998/10000 [=>............................] - ETA: 32s - reward: 1.2074done, took 3.635 seconds\n",
            "Testing for 100 episodes ...\n",
            "melee spina\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [-0.  0. -0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee spina\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [-0.  0. -0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee spina\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0. -0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee spina\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee spina\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee spina\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 1: reward: -25.000, steps: 6\n",
            "melee bravo\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee bravo\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee bravo\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee bravo\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee bravo\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee bravo\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 2: reward: -25.000, steps: 6\n",
            "melee moult\n",
            "[[ 1.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "finif moult\n",
            "[[ 1.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "finif moult\n",
            "[[ 1.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "finif moult\n",
            "[[ 1.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "finif moult\n",
            "[[ 1.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "finif moult\n",
            "[[ 1.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 3: reward: -20.000, steps: 6\n",
            "melee monte\n",
            "[[ 1. -1.  0. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "stats monte\n",
            "[[ 1. -1.  0. -1.  1.]\n",
            " [ 0. -1.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "bucks monte\n",
            "[[ 1. -1.  0. -1.  1.]\n",
            " [ 0. -1.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "bucks monte\n",
            "[[ 1. -1.  0. -1.  1.]\n",
            " [ 0. -1.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "bucks monte\n",
            "[[ 1. -1.  0. -1.  1.]\n",
            " [ 0. -1.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "bucks monte\n",
            "[[ 1. -1.  0. -1.  1.]\n",
            " [ 0. -1.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 4: reward: 5.000, steps: 6\n",
            "melee scorn\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee scorn\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee scorn\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee scorn\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee scorn\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee scorn\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 5: reward: -25.000, steps: 6\n",
            "melee vitas\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee vitas\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee vitas\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee vitas\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee vitas\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee vitas\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 6: reward: -25.000, steps: 6\n",
            "melee harps\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee harps\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee harps\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee harps\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee harps\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee harps\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 7: reward: -25.000, steps: 6\n",
            "melee mutes\n",
            "[[ 1. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "refly mutes\n",
            "[[ 1. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "ruder mutes\n",
            "[[ 1. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  1.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "verve mutes\n",
            "[[ 1. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  1.  0.  1.  0.]\n",
            " [ 0. -1.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "snugs mutes\n",
            "[[ 1. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  1.  0.  1.  0.]\n",
            " [ 0. -1.  0.  0. -1.]\n",
            " [-1.  0. -1.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "seeks mutes\n",
            "[[ 1. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  1.  0.  1.  0.]\n",
            " [ 0. -1.  0.  0. -1.]\n",
            " [-1.  0. -1.  0.  1.]\n",
            " [-1. -1. -1.  0.  1.]]\n",
            "Episode 8: reward: 35.000, steps: 6\n",
            "melee towed\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0. -0.]\n",
            " [-0.  0. -0.  0.  0.]\n",
            " [-0. -0. -0.  0.  0.]]\n",
            "refly towed\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0. -0.]\n",
            " [-0.  0. -0.  0.  0.]\n",
            " [-0. -0. -0.  0.  0.]]\n",
            "rills towed\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0. -0.]\n",
            " [-0.  0. -0.  0.  0.]\n",
            " [-0. -0. -0.  0.  0.]]\n",
            "rills towed\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0. -0.  0.  0.]\n",
            " [-0. -0. -0.  0.  0.]]\n",
            "rills towed\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0. -0. -0.  0.  0.]]\n",
            "rills towed\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 9: reward: -15.000, steps: 6\n",
            "melee creed\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "refly creed\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "ruder creed\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0. -1.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "cedes creed\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0. -1.  1. -1.]\n",
            " [ 1. -1. -1.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "esses creed\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0. -1.  1. -1.]\n",
            " [ 1. -1. -1.  1.  0.]\n",
            " [-1.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "asses creed\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0. -1.  1. -1.]\n",
            " [ 1. -1. -1.  1.  0.]\n",
            " [-1.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0.  1.  0.]]\n",
            "Episode 10: reward: 35.000, steps: 6\n",
            "melee rites\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-0. -0.  0.  0.  0.]\n",
            " [-0.  0. -0.  0. -0.]\n",
            " [ 0. -0. -0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "refly rites\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 1. -1.  0.  0.  0.]\n",
            " [-0.  0. -0.  0. -0.]\n",
            " [ 0. -0. -0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "octet rites\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  1.  1. -1.]\n",
            " [ 0. -0. -0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "seeks rites\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  1.  1. -1.]\n",
            " [-1. -1. -1.  0.  1.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "finif rites\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  1.  1. -1.]\n",
            " [-1. -1. -1.  0.  1.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "dotes rites\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  1.  1. -1.]\n",
            " [-1. -1. -1.  0.  1.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [ 0.  0.  1.  1.  1.]]\n",
            "Episode 11: reward: 30.000, steps: 6\n",
            "melee tacos\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [-0. -0. -0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee tacos\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [-0. -0. -0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee tacos\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0. -0. -0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee tacos\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee tacos\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee tacos\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 12: reward: -25.000, steps: 6\n",
            "melee tacky\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee tacky\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee tacky\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee tacky\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee tacky\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee tacky\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 13: reward: -25.000, steps: 6\n",
            "melee based\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "refly based\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "rills based\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "piled based\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0.  0.  0.  1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "nicad based\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0.  0.  0.  1.  1.]\n",
            " [ 0.  0.  0. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "esses based\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0.  0.  0.  1.  1.]\n",
            " [ 0.  0.  0. -1.  1.]\n",
            " [-1. -1.  1.  1. -1.]]\n",
            "Episode 14: reward: 35.000, steps: 6\n",
            "melee rinds\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [-0. -0.  0.  0. -0.]]\n",
            "melee rinds\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [-0. -0.  0.  0. -0.]]\n",
            "melee rinds\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [-0. -0.  0.  0. -0.]]\n",
            "melee rinds\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [-0. -0.  0.  0. -0.]]\n",
            "melee rinds\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0. -0.  0.  0. -0.]]\n",
            "melee rinds\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 15: reward: -25.000, steps: 6\n",
            "melee wists\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee wists\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee wists\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee wists\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee wists\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee wists\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 16: reward: -25.000, steps: 6\n",
            "melee sleet\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "paver sleet\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "cedes sleet\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "ceded sleet\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "jells sleet\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  1.  0.]\n",
            " [ 0. -1. -1. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "brans sleet\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  1.  0.]\n",
            " [ 0. -1. -1. -1. -1.]\n",
            " [ 0.  0.  0.  0. -1.]]\n",
            "Episode 17: reward: 25.000, steps: 6\n",
            "melee fetch\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0. -0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0. -0. -0. -0. -0.]\n",
            " [ 0.  0.  0.  0. -0.]]\n",
            "natty fetch\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0.  1. -1.  0.]\n",
            " [ 0. -0.  0.  0. -0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0. -0. -0. -0. -0.]\n",
            " [ 0.  0.  0.  0. -0.]]\n",
            "level fetch\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0.  1. -1.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0. -0. -0. -0. -0.]\n",
            " [ 0.  0.  0.  0. -0.]]\n",
            "cirri fetch\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0.  1. -1.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 0. -0. -0. -0. -0.]\n",
            " [ 0.  0.  0.  0. -0.]]\n",
            "freer fetch\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0.  1. -1.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 1.  0. -1. -1.  0.]\n",
            " [ 0.  0.  0.  0. -0.]]\n",
            "lacks fetch\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0.  1. -1.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 1.  0. -1. -1.  0.]\n",
            " [ 0.  0. -1.  0.  0.]]\n",
            "Episode 18: reward: 0.000, steps: 6\n",
            "melee piety\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0. -0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]]\n",
            "eerie piety\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1.  0. -1. -1.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0. -0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]]\n",
            "yucky piety\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1.  0. -1. -1.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0. -0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]]\n",
            "nisei piety\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1.  0. -1. -1.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0. -0. -0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]]\n",
            "rarer piety\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1.  0. -1. -1.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0.  0. -0.  0.  0.]]\n",
            "hovel piety\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1.  0. -1. -1.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0.  0.  0. -1.  0.]]\n",
            "Episode 19: reward: 10.000, steps: 6\n",
            "melee tribe\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [-0. -0.  0. -0. -0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0. -0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]]\n",
            "asses tribe\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0. -0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]]\n",
            "cusps tribe\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0. -0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]]\n",
            "cusps tribe\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]]\n",
            "cusps tribe\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]]\n",
            "cusps tribe\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 20: reward: -15.000, steps: 6\n",
            "melee wanes\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "refly wanes\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "rills wanes\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "rills wanes\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "gunny wanes\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0.  1. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "amass wanes\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0.  1. -1.  0.]\n",
            " [-1.  0. -1. -1.  1.]]\n",
            "Episode 21: reward: 10.000, steps: 6\n",
            "melee dinky\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [-0.  0. -0. -0.  0.]]\n",
            "melee dinky\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [-0.  0. -0. -0.  0.]]\n",
            "melee dinky\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [-0.  0. -0. -0.  0.]]\n",
            "melee dinky\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [-0.  0. -0. -0.  0.]]\n",
            "melee dinky\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0. -0. -0.  0.]]\n",
            "melee dinky\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 22: reward: -25.000, steps: 6\n",
            "melee brews\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "eerie brews\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1. -1.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "cabal brews\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1. -1.  0. -1.]\n",
            " [ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "eerie brews\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1. -1.  0. -1.]\n",
            " [ 0.  0. -1.  0.  0.]\n",
            " [-1. -1. -1.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "biggy brews\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1. -1.  0. -1.]\n",
            " [ 0.  0. -1.  0.  0.]\n",
            " [-1. -1. -1.  0. -1.]\n",
            " [ 1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "memes brews\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1. -1.  0. -1.]\n",
            " [ 0.  0. -1.  0.  0.]\n",
            " [-1. -1. -1.  0. -1.]\n",
            " [ 1.  0.  0.  0.  0.]\n",
            " [ 0. -1.  0. -1.  1.]]\n",
            "Episode 23: reward: 20.000, steps: 6\n",
            "melee calks\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-0. -0. -0.  0. -0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [-0. -0. -0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]]\n",
            "seeds calks\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [-0. -0. -0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]]\n",
            "cagey calks\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 1.  1.  0.  0.  0.]\n",
            " [-0. -0. -0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]]\n",
            "abeam calks\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 1.  1.  0.  0.  0.]\n",
            " [-1.  0.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]]\n",
            "rarer calks\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 1.  1.  0.  0.  0.]\n",
            " [-1.  0.  0. -1.  0.]\n",
            " [ 0.  1.  0.  0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]]\n",
            "leper calks\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 1.  1.  0.  0.  0.]\n",
            " [-1.  0.  0. -1.  0.]\n",
            " [ 0.  1.  0.  0.  0.]\n",
            " [-1.  0.  0.  0.  0.]]\n",
            "Episode 24: reward: 0.000, steps: 6\n",
            "melee vower\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]]\n",
            "refly vower\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]]\n",
            "ruder vower\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [-0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]]\n",
            "caped vower\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]]\n",
            "cedes vower\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0.  1.  0.]\n",
            " [-0.  0.  0.  0.  0.]]\n",
            "esses vower\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0.  1.  0.]\n",
            " [-1.  0.  0.  1.  0.]]\n",
            "Episode 25: reward: 5.000, steps: 6\n",
            "melee magus\n",
            "[[ 1.  0.  0.  0.  0.]\n",
            " [-0. -0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]]\n",
            "fells magus\n",
            "[[ 1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]]\n",
            "gaped magus\n",
            "[[ 1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [-1.  1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]]\n",
            "belle magus\n",
            "[[ 1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [-1.  1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]]\n",
            "belle magus\n",
            "[[ 1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [-1.  1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]]\n",
            "belle magus\n",
            "[[ 1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [-1.  1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 26: reward: -10.000, steps: 6\n",
            "melee sorts\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee sorts\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee sorts\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee sorts\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee sorts\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee sorts\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 27: reward: -25.000, steps: 6\n",
            "melee nerds\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "natty nerds\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "genie nerds\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 1.  0.  0.  0.  0.]\n",
            " [ 0.  1. -1.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "sever nerds\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 1.  0.  0.  0.  0.]\n",
            " [ 0.  1. -1.  0. -1.]\n",
            " [-1.  1.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "first nerds\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 1.  0.  0.  0.  0.]\n",
            " [ 0.  1. -1.  0. -1.]\n",
            " [-1.  1.  0. -1. -1.]\n",
            " [ 0.  0.  1. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "fires nerds\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 1.  0.  0.  0.  0.]\n",
            " [ 0.  1. -1.  0. -1.]\n",
            " [-1.  1.  0. -1. -1.]\n",
            " [ 0.  0.  1. -1.  0.]\n",
            " [ 0.  0.  1. -1.  1.]]\n",
            "Episode 28: reward: 20.000, steps: 6\n",
            "melee seeps\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0.  0. -0.]\n",
            " [-0.  0.  0. -0. -0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]]\n",
            "natty seeps\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0.  0. -0.]\n",
            " [-0.  0.  0. -0. -0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]]\n",
            "natty seeps\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0. -0. -0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]]\n",
            "natty seeps\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]]\n",
            "natty seeps\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]]\n",
            "natty seeps\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 29: reward: -20.000, steps: 6\n",
            "melee nuder\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "refly nuder\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "ruder nuder\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  1.  1.  1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "caped nuder\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  1.  1.  1.  1.]\n",
            " [ 0.  0.  0.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "yawny nuder\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  1.  1.  1.  1.]\n",
            " [ 0.  0.  0.  1. -1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "geese nuder\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  1.  1.  1.  1.]\n",
            " [ 0.  0.  0.  1. -1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0. -1. -1.  0. -1.]]\n",
            "Episode 30: reward: 45.000, steps: 6\n",
            "melee delft\n",
            "[[ 0.  1.  1. -1. -1.]\n",
            " [-0. -0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0. -0. -0.  0. -0.]]\n",
            "fires delft\n",
            "[[ 0.  1.  1. -1. -1.]\n",
            " [-1.  0.  0. -1.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0. -0. -0.  0. -0.]]\n",
            "natty delft\n",
            "[[ 0.  1.  1. -1. -1.]\n",
            " [-1.  0.  0. -1.  0.]\n",
            " [ 0.  0. -1. -1.  0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0. -0. -0.  0. -0.]]\n",
            "batty delft\n",
            "[[ 0.  1.  1. -1. -1.]\n",
            " [-1.  0.  0. -1.  0.]\n",
            " [ 0.  0. -1. -1.  0.]\n",
            " [ 0.  0. -1. -1.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0. -0. -0.  0. -0.]]\n",
            "mosts delft\n",
            "[[ 0.  1.  1. -1. -1.]\n",
            " [-1.  0.  0. -1.  0.]\n",
            " [ 0.  0. -1. -1.  0.]\n",
            " [ 0.  0. -1. -1.  0.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0. -0. -0.  0. -0.]]\n",
            "spree delft\n",
            "[[ 0.  1.  1. -1. -1.]\n",
            " [-1.  0.  0. -1.  0.]\n",
            " [ 0.  0. -1. -1.  0.]\n",
            " [ 0.  0. -1. -1.  0.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0.  0.  0. -1. -1.]]\n",
            "Episode 31: reward: 20.000, steps: 6\n",
            "melee tombs\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [-0.  0.  0. -0.  0.]\n",
            " [ 0.  0. -0. -0.  0.]\n",
            " [ 0.  0. -0. -0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0. -0.]]\n",
            "shoos tombs\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [-1.  0. -1. -1.  1.]\n",
            " [ 0.  0. -0. -0.  0.]\n",
            " [ 0.  0. -0. -0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0. -0.]]\n",
            "anded tombs\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [-1.  0. -1. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0. -0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0. -0.]]\n",
            "anded tombs\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [-1.  0. -1. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0. -0.]]\n",
            "anded tombs\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [-1.  0. -1. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0. -0.]]\n",
            "anded tombs\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [-1.  0. -1. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 32: reward: 0.000, steps: 6\n",
            "melee litho\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-0.  0. -0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "absit litho\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "sepoy litho\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0. -1. -1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "algin litho\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0. -1. -1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0. -1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "curse litho\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0. -1. -1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0. -1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "curse litho\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0. -1. -1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0. -1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 33: reward: -5.000, steps: 6\n",
            "melee house\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0.  0.  0. -0. -0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "asses house\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0. -1. -1. -1. -1.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "bulls house\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0. -1. -1. -1. -1.]\n",
            " [ 0. -1.  0.  0. -1.]\n",
            " [ 0. -0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "memes house\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0. -1. -1. -1. -1.]\n",
            " [ 0. -1.  0.  0. -1.]\n",
            " [ 0. -1.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "gunny house\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0. -1. -1. -1. -1.]\n",
            " [ 0. -1.  0.  0. -1.]\n",
            " [ 0. -1.  0. -1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "width house\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0. -1. -1. -1. -1.]\n",
            " [ 0. -1.  0.  0. -1.]\n",
            " [ 0. -1.  0. -1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -1.]]\n",
            "Episode 34: reward: 10.000, steps: 6\n",
            "melee domed\n",
            "[[-1. -1.  0.  1. -1.]\n",
            " [ 0. -0. -0. -0. -0.]\n",
            " [ 0. -0.  0.  0. -0.]\n",
            " [ 0. -0.  0. -0. -0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]]\n",
            "paced domed\n",
            "[[-1. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  1.  1.]\n",
            " [ 0. -0.  0.  0. -0.]\n",
            " [ 0. -0.  0. -0. -0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]]\n",
            "books domed\n",
            "[[-1. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  1.  1.]\n",
            " [ 0.  1. -1.  0.  0.]\n",
            " [ 0. -0.  0. -0. -0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]]\n",
            "motor domed\n",
            "[[-1. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  1.  1.]\n",
            " [ 0.  1. -1.  0.  0.]\n",
            " [-1.  1.  0. -1.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]]\n",
            "wonky domed\n",
            "[[-1. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  1.  1.]\n",
            " [ 0.  1. -1.  0.  0.]\n",
            " [-1.  1.  0. -1.  0.]\n",
            " [ 0.  1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]]\n",
            "users domed\n",
            "[[-1. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  1.  1.]\n",
            " [ 0.  1. -1.  0.  0.]\n",
            " [-1.  1.  0. -1.  0.]\n",
            " [ 0.  1.  0.  0.  0.]\n",
            " [ 0.  0. -1.  0.  0.]]\n",
            "Episode 35: reward: 15.000, steps: 6\n",
            "melee brain\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [-0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]]\n",
            "melee brain\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [-0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]]\n",
            "melee brain\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]]\n",
            "melee brain\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]]\n",
            "melee brain\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]]\n",
            "melee brain\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 36: reward: -25.000, steps: 6\n",
            "melee diems\n",
            "[[-1. -1.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "serve diems\n",
            "[[-1. -1.  0. -1. -1.]\n",
            " [-1. -1.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "beery diems\n",
            "[[-1. -1.  0. -1. -1.]\n",
            " [-1. -1.  0.  0. -1.]\n",
            " [ 0. -1.  1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "asses diems\n",
            "[[-1. -1.  0. -1. -1.]\n",
            " [-1. -1.  0.  0. -1.]\n",
            " [ 0. -1.  1.  0.  0.]\n",
            " [ 0. -1. -1. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "asses diems\n",
            "[[-1. -1.  0. -1. -1.]\n",
            " [-1. -1.  0.  0. -1.]\n",
            " [ 0. -1.  1.  0.  0.]\n",
            " [ 0. -1. -1. -1.  1.]\n",
            " [ 0. -1. -1. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "seeks diems\n",
            "[[-1. -1.  0. -1. -1.]\n",
            " [-1. -1.  0.  0. -1.]\n",
            " [ 0. -1.  1.  0.  0.]\n",
            " [ 0. -1. -1. -1.  1.]\n",
            " [ 0. -1. -1. -1.  1.]\n",
            " [-1. -1.  1.  0.  1.]]\n",
            "Episode 37: reward: 55.000, steps: 6\n",
            "melee bands\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [-0. -0.  0.  0. -0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0. -0. -0. -0.  0.]\n",
            " [ 0. -0. -0. -0.  0.]\n",
            " [-0. -0.  0.  0.  0.]]\n",
            "melee bands\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0. -0. -0. -0.  0.]\n",
            " [ 0. -0. -0. -0.  0.]\n",
            " [-0. -0.  0.  0.  0.]]\n",
            "melee bands\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0. -0. -0.  0.]\n",
            " [ 0. -0. -0. -0.  0.]\n",
            " [-0. -0.  0.  0.  0.]]\n",
            "melee bands\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0. -0. -0.  0.]\n",
            " [-0. -0.  0.  0.  0.]]\n",
            "melee bands\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0. -0.  0.  0.  0.]]\n",
            "melee bands\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 38: reward: -25.000, steps: 6\n",
            "melee luvya\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "absit luvya\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "trots luvya\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "trots luvya\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "trots luvya\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "trots luvya\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 39: reward: -15.000, steps: 6\n",
            "melee treks\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "eerie treks\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1. -1.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "cabal treks\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1. -1.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "cabal treks\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1. -1.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "cabal treks\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1. -1.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "cabal treks\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1. -1.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 40: reward: -5.000, steps: 6\n",
            "melee slips\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-0. -0. -0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "absit slips\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0. -1. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "algin slips\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0. -1. -1.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "mommy slips\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0. -1. -1.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "mommy slips\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0. -1. -1.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "mommy slips\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0. -1. -1.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 41: reward: -10.000, steps: 6\n",
            "melee quail\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0. -0. -0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "absit quail\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-1.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "rarer quail\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-1.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "noose quail\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-1.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "noose quail\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-1.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "noose quail\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-1.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 42: reward: -10.000, steps: 6\n",
            "melee roach\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee roach\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee roach\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee roach\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee roach\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee roach\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 43: reward: -25.000, steps: 6\n",
            "melee forks\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee forks\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee forks\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee forks\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee forks\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee forks\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 44: reward: -25.000, steps: 6\n",
            "melee axman\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "shoos axman\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "shoos axman\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "shoos axman\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "shoos axman\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "shoos axman\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 45: reward: -20.000, steps: 6\n",
            "melee risks\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee risks\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee risks\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee risks\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee risks\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee risks\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 46: reward: -25.000, steps: 6\n",
            "melee wrack\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee wrack\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee wrack\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee wrack\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee wrack\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee wrack\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 47: reward: -25.000, steps: 6\n",
            "melee diced\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "refly diced\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "rills diced\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "nuked diced\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "nuked diced\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  1.  1.]\n",
            " [ 0.  0.  0.  1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "lamas diced\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  1.  1.]\n",
            " [ 0.  0.  0.  1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 48: reward: -5.000, steps: 6\n",
            "melee barns\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee barns\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee barns\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee barns\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee barns\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee barns\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 49: reward: -25.000, steps: 6\n",
            "melee curve\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "asses curve\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "cusps curve\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 1.  1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "wonky curve\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 1.  1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "wonky curve\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 1.  1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "wonky curve\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 1.  1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 50: reward: -10.000, steps: 6\n",
            "melee hoops\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee hoops\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee hoops\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee hoops\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee hoops\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee hoops\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 51: reward: -25.000, steps: 6\n",
            "melee reins\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "natty reins\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "level reins\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "level reins\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "lavas reins\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "bided reins\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0. -1.  0. -1.  0.]]\n",
            "Episode 52: reward: -5.000, steps: 6\n",
            "melee lites\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]]\n",
            "paver lites\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]]\n",
            "cedes lites\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0.  1.  1.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]]\n",
            "rider lites\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0.  1.  1.]\n",
            " [ 0.  1.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]]\n",
            "musos lites\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0.  1.  1.]\n",
            " [ 0.  1.  0.  1.  0.]\n",
            " [ 0.  0. -1.  0.  1.]\n",
            " [ 0. -0.  0. -0.  0.]]\n",
            "glass lites\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0.  1.  1.]\n",
            " [ 0.  1.  0.  1.  0.]\n",
            " [ 0.  0. -1.  0.  1.]\n",
            " [ 0. -1.  0. -1.  1.]]\n",
            "Episode 53: reward: 20.000, steps: 6\n",
            "melee pawls\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]]\n",
            "absit pawls\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-1.  0. -1.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]]\n",
            "civvy pawls\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-1.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]]\n",
            "civvy pawls\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-1.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]]\n",
            "civvy pawls\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-1.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]]\n",
            "civvy pawls\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-1.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 54: reward: -15.000, steps: 6\n",
            "melee klieg\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [-0.  0. -0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "paver klieg\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "cedes klieg\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "sties klieg\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0.  1.  0.]\n",
            " [ 0.  0.  1.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "paver klieg\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0.  1.  0.]\n",
            " [ 0.  0.  1.  1.  0.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "musos klieg\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0. -1.  0.  1.  0.]\n",
            " [ 0.  0.  1.  1.  0.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 55: reward: 15.000, steps: 6\n",
            "melee begun\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "natty begun\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "level begun\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "level begun\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "lavas begun\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "lavas begun\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [ 0.  1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 56: reward: -10.000, steps: 6\n",
            "melee walls\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "seeds walls\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "cagey walls\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 0.  1.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "brigs walls\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 0.  1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "peels walls\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 0.  1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "stows walls\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 0.  1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  1.  1.]\n",
            " [-1.  0.  0. -1.  1.]]\n",
            "Episode 57: reward: 0.000, steps: 6\n",
            "melee sedan\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0. -0.  0.]]\n",
            "natty sedan\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0. -0.  0.]]\n",
            "lolls sedan\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0. -0.  0.]]\n",
            "caked sedan\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0. -1.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0. -0.  0.]]\n",
            "rarer sedan\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0. -1.  0. -1. -1.]\n",
            " [ 0. -1.  0. -1.  0.]\n",
            " [-0.  0.  0. -0.  0.]]\n",
            "chums sedan\n",
            "[[ 0.  1.  0. -1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0. -1.  0. -1. -1.]\n",
            " [ 0. -1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0. -1.]]\n",
            "Episode 58: reward: 0.000, steps: 6\n",
            "melee mulch\n",
            "[[ 1.  0.  1.  0.  0.]\n",
            " [-0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [ 0. -0.  0. -0. -0.]\n",
            " [ 0. -0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]]\n",
            "verve mulch\n",
            "[[ 1.  0.  1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [ 0. -0.  0. -0. -0.]\n",
            " [ 0. -0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]]\n",
            "verve mulch\n",
            "[[ 1.  0.  1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0. -0. -0.]\n",
            " [ 0. -0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]]\n",
            "verve mulch\n",
            "[[ 1.  0.  1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]]\n",
            "verve mulch\n",
            "[[ 1.  0.  1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]]\n",
            "verve mulch\n",
            "[[1. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 59: reward: -20.000, steps: 6\n",
            "melee letup\n",
            "[[ 0.  1. -1. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "mocks letup\n",
            "[[ 0.  1. -1. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "mocks letup\n",
            "[[ 0.  1. -1. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "mocks letup\n",
            "[[ 0.  1. -1. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "mocks letup\n",
            "[[ 0.  1. -1. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "mocks letup\n",
            "[[ 0.  1. -1. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 60: reward: -5.000, steps: 6\n",
            "melee beams\n",
            "[[-1.  1.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "fires beams\n",
            "[[-1.  1.  0. -1. -1.]\n",
            " [ 0.  0.  0. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "pizza beams\n",
            "[[-1.  1.  0. -1. -1.]\n",
            " [ 0.  0.  0. -1.  1.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "pizza beams\n",
            "[[-1.  1.  0. -1. -1.]\n",
            " [ 0.  0.  0. -1.  1.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "amass beams\n",
            "[[-1.  1.  0. -1. -1.]\n",
            " [ 0.  0.  0. -1.  1.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [-1. -1.  1. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "totem beams\n",
            "[[-1.  1.  0. -1. -1.]\n",
            " [ 0.  0.  0. -1.  1.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [-1. -1.  1. -1.  1.]\n",
            " [ 0.  0.  0. -1. -1.]]\n",
            "Episode 61: reward: 45.000, steps: 6\n",
            "melee darks\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [-0. -0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0. -0.]]\n",
            "melee darks\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [-0. -0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0. -0.]]\n",
            "melee darks\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [-0. -0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0. -0.]]\n",
            "melee darks\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0. -0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0. -0.]]\n",
            "melee darks\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0. -0.]]\n",
            "melee darks\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 62: reward: -25.000, steps: 6\n",
            "melee siren\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "refly siren\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "ruder siren\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "cedes siren\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1. -1.]\n",
            " [ 0. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "phlox siren\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1. -1.]\n",
            " [ 0. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "phlox siren\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1. -1.]\n",
            " [ 0. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 63: reward: -5.000, steps: 6\n",
            "melee ogler\n",
            "[[ 0. -1.  1.  1. -1.]\n",
            " [-0. -0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0. -0.]\n",
            " [ 0. -0.  0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "arses ogler\n",
            "[[ 0. -1.  1.  1. -1.]\n",
            " [ 0. -1.  0.  1.  0.]\n",
            " [-0.  0.  0.  0. -0.]\n",
            " [ 0. -0.  0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "verve ogler\n",
            "[[ 0. -1.  1.  1. -1.]\n",
            " [ 0. -1.  0.  1.  0.]\n",
            " [ 0. -1. -1.  0. -1.]\n",
            " [ 0. -0.  0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "rarer ogler\n",
            "[[ 0. -1.  1.  1. -1.]\n",
            " [ 0. -1.  0.  1.  0.]\n",
            " [ 0. -1. -1.  0. -1.]\n",
            " [-1.  0. -1.  1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "queue ogler\n",
            "[[ 0. -1.  1.  1. -1.]\n",
            " [ 0. -1.  0.  1.  0.]\n",
            " [ 0. -1. -1.  0. -1.]\n",
            " [-1.  0. -1.  1.  1.]\n",
            " [ 0.  0. -1.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "asses ogler\n",
            "[[ 0. -1.  1.  1. -1.]\n",
            " [ 0. -1.  0.  1.  0.]\n",
            " [ 0. -1. -1.  0. -1.]\n",
            " [-1.  0. -1.  1.  1.]\n",
            " [ 0.  0. -1.  0. -1.]\n",
            " [ 0.  0.  0.  1.  0.]]\n",
            "Episode 64: reward: 40.000, steps: 6\n",
            "melee hopes\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0. -0. -0.  0. -0.]\n",
            " [-0.  0. -0.  0.  0.]\n",
            " [ 0.  0. -0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "refly hopes\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0. -0. -0.  0. -0.]\n",
            " [-0.  0. -0.  0.  0.]\n",
            " [ 0.  0. -0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "rills hopes\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [-0.  0. -0.  0.  0.]\n",
            " [ 0.  0. -0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "rills hopes\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0. -0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "gunny hopes\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "gunny hopes\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 65: reward: -10.000, steps: 6\n",
            "melee slots\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "absit slots\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0. -1.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "biter slots\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0. -1.  0. -1.]\n",
            " [ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "derby slots\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0. -1.  0. -1.]\n",
            " [ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "derby slots\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0. -1.  0. -1.]\n",
            " [ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "derby slots\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0. -1.  0. -1.]\n",
            " [ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 66: reward: -10.000, steps: 6\n",
            "melee kinks\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0.  0. -0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee kinks\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee kinks\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee kinks\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee kinks\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee kinks\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 67: reward: -25.000, steps: 6\n",
            "melee facer\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "refly facer\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "ruder facer\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1. -1.  0.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "esses facer\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1. -1.  0.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [-1.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "geese facer\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1. -1.  0.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [-1.  0.  0.  1.  0.]\n",
            " [ 0. -1. -1.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "spued facer\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1. -1.  0.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [-1.  0.  0.  1.  0.]\n",
            " [ 0. -1. -1.  0. -1.]\n",
            " [ 0.  0.  0.  1.  0.]]\n",
            "Episode 68: reward: 5.000, steps: 6\n",
            "melee paean\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-0. -0. -0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0. -0. -0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "eerie paean\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1.  0.  0. -1.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0. -0. -0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "snoot paean\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1.  0.  0. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0. -0. -0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "eerie paean\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1.  0.  0. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [-1. -1.  0.  0. -1.]\n",
            " [ 0. -0. -0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "puree paean\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1.  0.  0. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [-1. -1.  0.  0. -1.]\n",
            " [ 1.  0.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "spree paean\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1.  0.  0. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [-1. -1.  0.  0. -1.]\n",
            " [ 1.  0.  0. -1. -1.]\n",
            " [ 0. -1.  0. -1. -1.]]\n",
            "Episode 69: reward: 0.000, steps: 6\n",
            "melee cabal\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-0. -0.  0.  0. -0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [-0. -0.  0.  0. -0.]\n",
            " [ 0.  0.  0. -0. -0.]\n",
            " [ 0. -0.  0. -0. -0.]]\n",
            "absit cabal\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [-0. -0.  0.  0. -0.]\n",
            " [ 0.  0.  0. -0. -0.]\n",
            " [ 0. -0.  0. -0. -0.]]\n",
            "stuns cabal\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0. -0.  0.  0. -0.]\n",
            " [ 0.  0.  0. -0. -0.]\n",
            " [ 0. -0.  0. -0. -0.]]\n",
            "stuns cabal\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -0. -0.]\n",
            " [ 0. -0.  0. -0. -0.]]\n",
            "stuns cabal\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0. -0. -0.]]\n",
            "stuns cabal\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 70: reward: -15.000, steps: 6\n",
            "melee slope\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [-0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "abaca slope\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "abaca slope\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "abaca slope\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "abaca slope\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "abaca slope\n",
            "[[ 0. -1. -1. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 71: reward: -5.000, steps: 6\n",
            "melee girts\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee girts\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee girts\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee girts\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee girts\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee girts\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 72: reward: -25.000, steps: 6\n",
            "melee axles\n",
            "[[ 0. -1.  1.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "arses axles\n",
            "[[ 0. -1.  1.  1. -1.]\n",
            " [ 1.  0. -1.  1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "nuked axles\n",
            "[[ 0. -1.  1.  1. -1.]\n",
            " [ 1.  0. -1.  1.  1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "nuked axles\n",
            "[[ 0. -1.  1.  1. -1.]\n",
            " [ 1.  0. -1.  1.  1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "bitty axles\n",
            "[[ 0. -1.  1.  1. -1.]\n",
            " [ 1.  0. -1.  1.  1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "bitty axles\n",
            "[[ 0. -1.  1.  1. -1.]\n",
            " [ 1.  0. -1.  1.  1.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 73: reward: 35.000, steps: 6\n",
            "melee bulls\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "seeds bulls\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "cagey bulls\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "cagey bulls\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "cagey bulls\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "cagey bulls\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 74: reward: -15.000, steps: 6\n",
            "melee truer\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "refly truer\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "ruder truer\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1. -1.  0.  1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "esses truer\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1. -1.  0.  1.  1.]\n",
            " [-1.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "teens truer\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1. -1.  0.  1.  1.]\n",
            " [-1.  0.  0.  1.  0.]\n",
            " [ 1. -1. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "coons truer\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1. -1.  0.  1.  1.]\n",
            " [-1.  0.  0.  1.  0.]\n",
            " [ 1. -1. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 75: reward: 20.000, steps: 6\n",
            "melee belay\n",
            "[[ 0.  1.  1. -1. -1.]\n",
            " [-0. -0.  0.  0.  0.]\n",
            " [-0. -0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0. -0. -0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "fires belay\n",
            "[[ 0.  1.  1. -1. -1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [-0. -0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0. -0. -0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "seeks belay\n",
            "[[ 0.  1.  1. -1. -1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0.  1. -1.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0. -0. -0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "shill belay\n",
            "[[ 0.  1.  1. -1. -1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0.  1. -1.  0.  0.]\n",
            " [ 0.  0.  0. -1. -1.]\n",
            " [ 0. -0. -0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "hooks belay\n",
            "[[ 0.  1.  1. -1. -1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0.  1. -1.  0.  0.]\n",
            " [ 0.  0.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "hooks belay\n",
            "[[ 0.  1.  1. -1. -1.]\n",
            " [ 0.  0.  0. -1.  0.]\n",
            " [ 0.  1. -1.  0.  0.]\n",
            " [ 0.  0.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 76: reward: 15.000, steps: 6\n",
            "melee easel\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [ 0.  0.  0. -0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "paver easel\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  1.  0.  1.  0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [ 0.  0.  0. -0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "asses easel\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  1.  0.  1.  0.]\n",
            " [-1. -1.  1.  1. -1.]\n",
            " [ 0.  0.  0. -0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "motor easel\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  1.  0.  1.  0.]\n",
            " [-1. -1.  1.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "motor easel\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  1.  0.  1.  0.]\n",
            " [-1. -1.  1.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "motor easel\n",
            "[[ 0. -1. -1.  1. -1.]\n",
            " [ 0.  1.  0.  1.  0.]\n",
            " [-1. -1.  1.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 77: reward: 40.000, steps: 6\n",
            "melee hobos\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0. -0.  0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee hobos\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0. -0.  0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee hobos\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee hobos\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee hobos\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee hobos\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 78: reward: -25.000, steps: 6\n",
            "melee stoae\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "asses stoae\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [-1. -1. -1. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "mocks stoae\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [-1. -1. -1. -1. -1.]\n",
            " [ 0. -1.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "bulls stoae\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [-1. -1. -1. -1. -1.]\n",
            " [ 0. -1.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "delta stoae\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [-1. -1. -1. -1. -1.]\n",
            " [ 0. -1.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0. -1.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "asses stoae\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [-1. -1. -1. -1. -1.]\n",
            " [ 0. -1.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0. -1.  0. -1. -1.]\n",
            " [-1. -1. -1. -1. -1.]]\n",
            "Episode 79: reward: 50.000, steps: 6\n",
            "melee curbs\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [-0. -0. -0. -0. -0.]\n",
            " [ 0. -0.  0.  0. -0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [ 0. -0.  0. -0. -0.]\n",
            " [-0. -0. -0. -0. -0.]]\n",
            "melee curbs\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0. -0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [ 0. -0.  0. -0. -0.]\n",
            " [-0. -0. -0. -0. -0.]]\n",
            "melee curbs\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [ 0. -0.  0. -0. -0.]\n",
            " [-0. -0. -0. -0. -0.]]\n",
            "melee curbs\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0. -0. -0.]\n",
            " [-0. -0. -0. -0. -0.]]\n",
            "melee curbs\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0. -0. -0. -0. -0.]]\n",
            "melee curbs\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 80: reward: -25.000, steps: 6\n",
            "melee suety\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "eerie suety\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "snoot suety\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1.  0.  0. -1.]\n",
            " [ 1.  0.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "chums suety\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1.  0.  0. -1.]\n",
            " [ 1.  0.  0.  0. -1.]\n",
            " [ 0.  0. -1.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "sexed suety\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1.  0.  0. -1.]\n",
            " [ 1.  0.  0.  0. -1.]\n",
            " [ 0.  0. -1.  0. -1.]\n",
            " [ 1. -1.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "asses suety\n",
            "[[ 0. -1.  0. -1. -1.]\n",
            " [-1. -1.  0.  0. -1.]\n",
            " [ 1.  0.  0.  0. -1.]\n",
            " [ 0.  0. -1.  0. -1.]\n",
            " [ 1. -1.  0. -1.  0.]\n",
            " [ 0. -1. -1. -1. -1.]]\n",
            "Episode 81: reward: 10.000, steps: 6\n",
            "melee rises\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-0. -0.  0.  0. -0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [ 0.  0. -0.  0. -0.]\n",
            " [ 0. -0.  0. -0.  0.]\n",
            " [ 0. -0. -0. -0. -0.]]\n",
            "refly rises\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [ 0.  0. -0.  0. -0.]\n",
            " [ 0. -0.  0. -0.  0.]\n",
            " [ 0. -0. -0. -0. -0.]]\n",
            "octet rises\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0.  0. -0.  0. -0.]\n",
            " [ 0. -0.  0. -0.  0.]\n",
            " [ 0. -0. -0. -0. -0.]]\n",
            "mocks rises\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0. -0.  0. -0.  0.]\n",
            " [ 0. -0. -0. -0. -0.]]\n",
            "stash rises\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [-1.  0.  0. -1.  0.]\n",
            " [ 0. -0. -0. -0. -0.]]\n",
            "kebob rises\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  1.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [-1.  0.  0. -1.  0.]\n",
            " [ 0. -1.  0.  0.  0.]]\n",
            "Episode 82: reward: 0.000, steps: 6\n",
            "melee pager\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0. -0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]]\n",
            "refly pager\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0. -0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]]\n",
            "ruder pager\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0. -0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]]\n",
            "caped pager\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [ 0.  1. -1.  1.  0.]\n",
            " [-0.  0.  0. -0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]]\n",
            "ruder pager\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [ 0.  1. -1.  1.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [ 0. -0.  0.  0.  0.]]\n",
            "nisei pager\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [ 0.  1. -1.  1.  0.]\n",
            " [-1.  0.  0.  1.  1.]\n",
            " [ 0.  0.  0.  1.  0.]]\n",
            "Episode 83: reward: 15.000, steps: 6\n",
            "melee hiked\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-0. -0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "refly hiked\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "rills hiked\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  1.  0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "nuked hiked\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  1.  0.  0.  0.]\n",
            " [ 0.  0.  1.  1.  1.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "canon hiked\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  1.  0.  0.  0.]\n",
            " [ 0.  0.  1.  1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "canon hiked\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  1.  0.  0.  0.]\n",
            " [ 0.  0.  1.  1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 84: reward: 5.000, steps: 6\n",
            "melee punts\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee punts\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee punts\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee punts\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee punts\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee punts\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 85: reward: -25.000, steps: 6\n",
            "melee urges\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "refly urges\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "ruder urges\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "fists urges\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1. -1.  0.  1. -1.]\n",
            " [ 0.  0. -1.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "clunk urges\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1. -1.  0.  1. -1.]\n",
            " [ 0.  0. -1.  0.  1.]\n",
            " [ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "asses urges\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-1. -1.  0.  0.  0.]\n",
            " [-1. -1.  0.  1. -1.]\n",
            " [ 0.  0. -1.  0.  1.]\n",
            " [ 0.  0. -1.  0.  0.]\n",
            " [ 0. -1. -1.  1.  1.]]\n",
            "Episode 86: reward: 35.000, steps: 6\n",
            "melee spice\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [-0. -0.  0.  0.  0.]\n",
            " [-0. -0.  0.  0. -0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [ 0. -0. -0.  0.  0.]]\n",
            "asses spice\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0. -1. -1. -1. -1.]\n",
            " [-0. -0.  0.  0. -0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [ 0. -0. -0.  0.  0.]]\n",
            "bulls spice\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0. -1. -1. -1. -1.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [ 0. -0. -0.  0.  0.]]\n",
            "bulls spice\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0. -1. -1. -1. -1.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [ 0. -0. -0.  0.  0.]]\n",
            "brute spice\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0. -1. -1. -1. -1.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0. -0. -0.  0.  0.]]\n",
            "teats spice\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0. -1. -1. -1. -1.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0. -1.  0.  0. -1.]]\n",
            "Episode 87: reward: 5.000, steps: 6\n",
            "melee shoat\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0. -0. -0. -0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0. -0.]]\n",
            "melee shoat\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0. -0.]]\n",
            "melee shoat\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0. -0.]]\n",
            "melee shoat\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0. -0.]]\n",
            "melee shoat\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0. -0.]]\n",
            "melee shoat\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 88: reward: -25.000, steps: 6\n",
            "melee humps\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "shoos humps\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [-1. -1.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "caked humps\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [-1. -1.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "caked humps\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [-1. -1.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "caked humps\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [-1. -1.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "caked humps\n",
            "[[-1.  0.  0.  0.  0.]\n",
            " [-1. -1.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 89: reward: -15.000, steps: 6\n",
            "melee sages\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [-0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "refly sages\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "rills sages\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "rills sages\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "gunny sages\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "windy sages\n",
            "[[ 0. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  1.]\n",
            " [-1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 90: reward: -5.000, steps: 6\n",
            "melee nulls\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "seeds nulls\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "cagey nulls\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "cagey nulls\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "cagey nulls\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "cagey nulls\n",
            "[[ 0.  0.  1.  0.  0.]\n",
            " [-1.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 91: reward: -15.000, steps: 6\n",
            "melee gyppy\n",
            "[[ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "melee gyppy\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee gyppy\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee gyppy\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee gyppy\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee gyppy\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 92: reward: -25.000, steps: 6\n",
            "melee qualm\n",
            "[[-1.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "wryer qualm\n",
            "[[-1.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "wryer qualm\n",
            "[[-1.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "wryer qualm\n",
            "[[-1.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "wryer qualm\n",
            "[[-1.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "wryer qualm\n",
            "[[-1.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 93: reward: -20.000, steps: 6\n",
            "melee mazed\n",
            "[[ 1. -1.  0.  1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "refly mazed\n",
            "[[ 1. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "ruder mazed\n",
            "[[ 1. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0. -1.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "fists mazed\n",
            "[[ 1. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0. -1.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "fists mazed\n",
            "[[ 1. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0. -1.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "fists mazed\n",
            "[[ 1. -1.  0.  1. -1.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0.  0. -1.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 94: reward: 10.000, steps: 6\n",
            "melee aside\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "asses aside\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 1.  1. -1. -1. -1.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "coked aside\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 1.  1. -1. -1. -1.]\n",
            " [ 0.  0.  0. -1. -1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "algin aside\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 1.  1. -1. -1. -1.]\n",
            " [ 0.  0.  0. -1. -1.]\n",
            " [ 1.  0.  0. -1.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "yarns aside\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 1.  1. -1. -1. -1.]\n",
            " [ 0.  0.  0. -1. -1.]\n",
            " [ 1.  0.  0. -1.  0.]\n",
            " [ 0. -1.  0.  0. -1.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "lamas aside\n",
            "[[ 0. -1.  0. -1.  1.]\n",
            " [ 1.  1. -1. -1. -1.]\n",
            " [ 0.  0.  0. -1. -1.]\n",
            " [ 1.  0.  0. -1.  0.]\n",
            " [ 0. -1.  0.  0. -1.]\n",
            " [ 0. -1.  0. -1. -1.]]\n",
            "Episode 95: reward: 35.000, steps: 6\n",
            "melee purls\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0. -0. -0. -0.]\n",
            " [ 0.  0.  0. -0. -0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0. -0.  0.  0. -0.]\n",
            " [ 0. -0.  0. -0. -0.]]\n",
            "absit purls\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0. -0. -0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0. -0.  0.  0. -0.]\n",
            " [ 0. -0.  0. -0. -0.]]\n",
            "fiber purls\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0. -0.  0.  0. -0.]\n",
            " [ 0. -0.  0. -0. -0.]]\n",
            "sudsy purls\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [-1.  1.  0. -1.  0.]\n",
            " [ 0. -0.  0.  0. -0.]\n",
            " [ 0. -0.  0. -0. -0.]]\n",
            "algin purls\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [-1.  1.  0. -1.  0.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0. -0.  0. -0. -0.]]\n",
            "asses purls\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0. -1.]\n",
            " [-1.  1.  0. -1.  0.]\n",
            " [ 0. -1.  0.  0.  0.]\n",
            " [ 0. -1. -1.  0.  1.]]\n",
            "Episode 96: reward: 0.000, steps: 6\n",
            "melee quilt\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [-0.  0.  0. -0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0. -0. -0.  0.  0.]]\n",
            "absit quilt\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0. -1.  1.]\n",
            " [ 0.  0.  0.  0. -0.]\n",
            " [-0.  0.  0. -0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0. -0. -0.  0.  0.]]\n",
            "amass quilt\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [-0.  0.  0. -0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0. -0. -0.  0.  0.]]\n",
            "amass quilt\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [ 0. -0. -0.  0.  0.]]\n",
            "amass quilt\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0. -0. -0.  0.  0.]]\n",
            "amass quilt\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0. -1.  1.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 97: reward: -15.000, steps: 6\n",
            "melee jowly\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "absit jowly\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "absit jowly\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "absit jowly\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "absit jowly\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "absit jowly\n",
            "[[ 0.  0. -1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.]]\n",
            "Episode 98: reward: -20.000, steps: 6\n",
            "melee bungs\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee bungs\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee bungs\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee bungs\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee bungs\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee bungs\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 99: reward: -25.000, steps: 6\n",
            "melee bunco\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee bunco\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee bunco\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee bunco\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee bunco\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "melee bunco\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Episode 100: reward: -25.000, steps: 6\n",
            "Training for 1000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "  994/10000 [=>............................] - ETA: 37s - reward: 1.3129done, took 4.188 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "test_rewards = []\n",
        "train_rewards = []\n",
        "tst = []\n",
        "trn = []\n",
        "\n",
        "for i in range(10):\n",
        "\n",
        "  test_history = dqn.test(env, nb_episodes=100)\n",
        "  max_test_reward = np.max(test_history.history['episode_reward'])\n",
        "  test_rewards.append(max_test_reward)\n",
        "  if max_test_reward >= 100:\n",
        "    tst.append(test_history)\n",
        "\n",
        "  dqn.fit(env, nb_steps=1000)\n",
        "  max_train_reward = np.max(dqn.history.history['episode_reward'])\n",
        "  train_rewards.append(max_train_reward)\n",
        "  if max_train_reward >= 100:\n",
        "    trn.append(dqn.history)\n",
        "\n",
        "# Plot test rewards\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(test_rewards)\n",
        "plt.title('Test Rewards')\n",
        "plt.xlabel('Turns')\n",
        "plt.ylabel('Max Reward')\n",
        "plt.show()\n",
        "\n",
        "# Plot train rewards\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_rewards)\n",
        "plt.title('Train Rewards')\n",
        "plt.xlabel('Turns')\n",
        "plt.ylabel('Max Reward')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store max reward episodes\n",
        "max_test_episodes = [h for h in tst if np.max(h.history['episode_reward']) >= 100]\n",
        "max_train_episodes = [h for h in trn if np.max(h.history['episode_reward']) >= 100]\n",
        "\n",
        "print(\"Number of max test reward episodes:\", len(max_test_episodes))\n",
        "print(\"Number of max train reward episodes:\", len(max_train_episodes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF9auRstv0Os",
        "outputId": "41b9d837-c7a4-4b4a-c04f-a2a2499e5f75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of max test reward episodes: 3\n",
            "Number of max train reward episodes: 10\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}